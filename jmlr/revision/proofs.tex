\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{xr}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{fullpage}

\externaldocument{local_spectral_clustering_jmlr_revision_2} 

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

% Make a widecheck symbol (thanks, Stack Exchange!)
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
	<5> <6> <7> <8> <9> <10>
	<10.95> <12> <14.4> <17.28> <20.74> <24.88>
	mathx10
}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareFontSubstitution{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}
% widecheck made

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\iid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\cut}{\mathrm{cut}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\dagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}

%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Dgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\dagger}}
\newcommand{\Lap}{L}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Vset}{\mathcal{V}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\dive}{\mathrm{div}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\dx}{\,dx}
\newcommand{\dy}{\,dy}
\newcommand{\dr}{\,dr}
\newcommand{\dxpr}{\,dx'}
\newcommand{\dypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\wb{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{OS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{example}{Example}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{remark}{Remark}[section]


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Proofs for ``Geometry of Local Spectral Clustering''}
\author{Alden Green}
\date{\today}
\maketitle

The proofs of our major theorems all consist of at most three modular parts.
\begin{enumerate}
	\item~\textbf{Fixed graph results.} Results which hold with respect to an arbitrary graph $G$, and are stated with respect to functionals (i.e. normalized cut, conductance, and local spread) of $G$;
	\item~\textbf{Finite-sample bounds.} For the specific choice of $G = G_{n,r}$, we relate the aforementioned functionals to their population analogues. This proves~\textcolor{red}{(?)} 
	\item~\textbf{Population functionals.} (In the case of density clustering only.) When the candidate cluster is a $\lambda$-density cluster, we bound the population functionals by a function of $\lambda$, as well as the other relevant parameters introduced in Section~\textcolor{red}{(?)}. This proves~\textcolor{red}{(?)}
\end{enumerate}
The first three sections of this appendix will correspond to each of these three parts. In Section~\ref{sec:pf_major_theorems}, we will combine these parts to prove the major theorems of our main text, \textcolor{red}{(...)}. Finally, in Section~\ref{sec:appr_misclassification_error} we derive upper bounds for the aPPR vector, and in Section~\ref{sec:experimental_details} we give relevant details regarding our experiments.

\section{Fixed graph results}
In this section, we give all results that hold with respect to an arbitrary graph $G$. For the convenience of the reader, we begin by reviewing some notation from the main text, and also introduce some new notation. 

\paragraph{Notation.}
The graph $G = (V,E)$ is an undirected and connected but otherwise arbitrary graph, defined over vertices $V = \{1,\ldots,n\}$ with $m = |E|$ total edges. The adjacency matrix of $G$ is $A$, the degree matrix is $D$, and the lazy random walk matrix over $G$ is $W = (I + D^{-1}A)/2$. If the lazy random walk originates at a node $v$, the distribution of the lazy random walk $q_v^{(t)} := q(v,t;G)$ after $t$ steps is $q_v^{(t)} := e_v W^t$, with stationary distribution $\pi := \pi(G) := \lim_{t \to \infty} q_v^{(t)}$ with entries $\pi(u) = \deg(u;G)/\vol(u;G)$.

For a starting distribution $s$ (by distribution we mean a vector with non-negative entries), the PPR vector $p_s = p(s,\alpha;G)$ is the solution of
\begin{equation}
\label{eqn:ppr}
p_s = \alpha s + (1 - \alpha) p_s W.
\end{equation}
When $s = e_v$, we write $p_v := p_{e_v}$. It is easy to check that $p_s = \alpha \sum_{t = 0}^{\infty} (1 - \alpha)^t q_s^{(t)}$.  Note that $s$ need not be a probability distribution (i.e. its entries need not sum to $1$) to make sense of~\eqref{eqn:ppr}.

Given a distribution $q$ (for instance, $q = q_v^{(t)}$ for $t \in \mathbb{N}$, $q = p_v$, or $q = \pi$) and $\beta \in (0,1)$, the $\beta$-sweep cut of $q$ is
\begin{equation*}
S_{\beta}(q) = \set{u: \frac{q(u)}{\deg(u;G)} > \beta};
\end{equation*} 
in the special case where $q = p_v$ we write $S_{\beta,v}$ for $S_{\beta}(p_v)$. The argument of $S_{\beta}(\cdot)$ will usually be clear from context, in which case we will drop it and simply write $S_{\beta}$. For $j = 1,\ldots,n$, let $\beta_j$ be the smallest value of $\beta \in (0,1)$ such that the sweep cut $S_{\beta_j}$ contains at least $j$ vertices. For notational ease, we will write $S_j := S_{\beta_j}$, and $S_0 = \emptyset$. 

We now introduce the~\emph{Lovasz-Simonovits curve} $h_q(\cdot): [0,2m] \to [0,1]$ to measure the extent to which a distribution $q$ is mixed. To do so, we first define a piecewise linear function $q[\cdot]: [0,2m] \to [0,1]$. Letting $q(S) := \sum_{u \in S} q(u)$, we take $q[\vol(S_j)] = q(S_j)$ for each sweep cut $S_j$, and then extend $q[\cdot]$ by piecewise linear interpolation to be defined everywhere on its domain. Then the mixedness of $q$ is measured by
\begin{equation*}
h_q(k) := q[k] - \frac{k}{2m}.
\end{equation*}
The Lovasz-Simonovits curve is a non-negative function, with $h_q(0) = h_q(2m) = 0$. The stationary distribution $\pi$ is mixed, i.e. $h_{\pi}(k) = 0$ for all $k \in [0,2m]$. Finally, both $q[\cdot]$ and $h_q(\cdot)$ are concave functions, which will be an important fact later on.  

The conductance of $V$ is abbreviated as $\Psi(G) := \Psi(V;G)$, and likewise for the local spread $s(G) := s(V;G)$. Finally, for convenience we introduce the following functionals:
\begin{equation*}
\begin{aligned}
& d_{\max}(C; G) := \max_{u \in C} \deg(u; G), && d_{\min}(C; G) := \min_{u \in C} \deg(u;G) \\
& d_{\max}(G) := d_{\max}(V;G),~~ && d_{\min}(G) := d_{\min}(V; G)
\end{aligned}
\end{equation*}
We note that $d_{\min}(G)^2 \leq d_{\min}(G) \cdot n \leq \vol(G) \leq n \cdot d_{\max}(G)$, and that for any $S \subseteq V$, $|S| \cdot d_{\min}(G) \leq \vol(S;G)$ (where $|S|$ is the cardinality of $S$.)

\paragraph{Organization.} In the following subsections we establish: (\ref{subsec:pf_lem_zhu}) an upper bound on the misclassification error of PPR in terms of $\alpha$ and $\Phi(C;G)$ (Lemma~\ref{lem:zhu}); (\ref{subsec:ppr_uniform_bounds}) a uniform bound on the perturbations of the PPR vector, to be used later in the proof of Theorem~\ref{thm:density_cluster_consistent_recovery} (consistency of PPR);  (\ref{subsec:lovasz_simonovits_bounds}) upper bounds on the mixedness of $q_v^{(t)}$ (as a function of $t$) and $p_v$ (as a function of $\alpha$), which will be helpful in the proofs of Proposition~\ref{prop:pointwise_mixing_time} and Theorem~\ref{thm:ppr_lb}; (\ref{subsec:pf_prop_pointwise_mixing_time}) an upper bound on $\tau_{\infty}(G)$ in terms of $\Psi(G)$ and $s(G)$ (Proposition~\ref{prop:pointwise_mixing_time}); and (\ref{subsec:ppr_spectral_partitioning}) an upper bound on the normalized cut $\Phi(\wh{C};G)$ in terms of $\Phi(C;G)$, to be used later in the proof of Theorem~\ref{thm:ppr_lb} (negative example). 

\subsection{Proof of Lemma~\ref{lem:zhu}}
\label{subsec:pf_lem_zhu}
For a candidate cluster $C \subseteq V$, we use the tilde-notation $\wt{G} = G[C]$ to refer to the subgraph of $G$ induced by $C$. Similarly we write $\wt{q}_v^{(t)} := q(v,t;\wt{G})$ for the $t$-step distribution of the lazy random walk over $\wt{G}$, $\wt{\pi} = \pi(G[C])$ for the stationary distribution of $\wt{q}_v^{(t)}$ (we will always assume $G[C]$ is connected), and $\wt{p}_v := p(v,\alpha;\wt{G})$ for the PPR vector over $\wt{G}$.

As mentioned in the main text, Lemma~\ref{lem:zhu} is equivalent, up to constants, to Lemma~3.4 in \cite{zhu2013}, and the proof of Lemma~\ref{lem:zhu} proceeds along very similar lines to the proof of that lemma. In fact, we directly use the following three inequalities, derived in that work:
\begin{itemize}
	\item \textbf{(c.f. Lemma 3.2 of \cite{zhu2013})} For any seed node $v \in C$, the PPR vector is lower bounded,
	\begin{equation}
	\label{pf:zhu1}
	\wt{p}_v(u) \geq \frac{3}{4}\bigl(1 - \alpha \cdot \tau_{\infty}(\wt{G})\bigr) \cdot \wt{\pi}(u),~~\textrm{for every $u \in C$.}
	\end{equation}
	\item \textbf{(c.f. Corollary 3.3 of \cite{zhu2013})} For any seed node $v \in C$, there exists a so-called leakage distribution $\ell = \ell(v)$ such that $\mathrm{supp}(\ell) \subseteq C$, $\|\ell\|_1 \leq 2\Phi(C;G)/\alpha$, and 
	\begin{equation}
	\label{pf:zhu2}
	p_v(u) \geq \wt{p}_v(u) - \wt{p}_{\ell}(u),~~\textrm{for every $u \in C$.}
	\end{equation}
	\item \textbf{(c.f. Lemma 3.1 of \cite{zhu2013})} There exists a set $C^g \subset C$ with $\vol(C^g;G) \geq \frac{1}{2}\vol(C;G)$ such that for any seed node $v \in C^g$, the following inequality holds
	\begin{equation}
	\label{pf:zhu3}
	p_v(C^c) \leq 2\frac{\Phi(C;G)}{\alpha}.
	\end{equation}
\end{itemize}
We use~\eqref{pf:zhu1}-\eqref{pf:zhu3} to separately upper bound $\vol(S_{\beta,v} \setminus C;G)$, $\vol(C^{\mathrm{int}} \setminus S_{\beta,v};G)$ and $\vol(C^{\mathrm{bdry}} \setminus S_{\beta,v};G)$; here $C^{\mathrm{int}} \cup C^{\mathrm{bdry}} = C$ is a partition of $C$, with
\begin{equation*}
C^{\mathrm{int}} := \Bigl\{u \in C: \deg(u;\wt{G}) > \bigl(1 - \alpha \cdot \beta \cdot \vol(C;G)\bigr) \deg(u;G) \Bigr\},
\end{equation*}
consisting of those vertices $u \in C$ with sufficient large degree in $\wt{G}$. 

First we upper bound $\vol(S_{\beta,v} \setminus C;G)$. Observe that for any $u \in S_{\beta,v} \setminus C$, $p_v(u) > \beta \cdot \deg(u;G)$. Summing up over all such vertices, from~\eqref{pf:zhu3} we conclude that
\begin{equation}
\label{pf:zhu3.5}
\vol(S_{\beta,v} \setminus C; G) \leq \frac{p_v(C^c)}{\beta} \leq 2\frac{\Phi(C;G)}{\beta \cdot \alpha}.
\end{equation} 

Next we upper bound $\vol(C^{\mathrm{int}} \setminus S_{\beta,v};G)$. From~\eqref{pf:zhu1} and~\eqref{pf:zhu2} we see that 
\begin{equation*}
p_v(u) \geq \frac{3}{4}\bigl(1 - \alpha \cdot \tau_{\infty}(\wt{G})\bigr) \cdot \wt{\pi}(u) - \wt{p}_{\ell}(u)~~\textrm{for all $u \in C$.}
\end{equation*}
If additionally $u \not\in S_{\beta,v}$ then $p_v(u) \leq \beta \deg(u;G)$, and for all such $u \in C \setminus S_{\beta,v}$,
\begin{equation}
\label{pf:zhu4}
\frac{3}{4}\bigl(1 - \alpha \cdot \tau_{\infty}(\wt{G})\bigr) \cdot \wt{\pi}(u) -  \beta\deg(u;G) \leq \wt{p}_{\ell}(u).
\end{equation}
On the other hand, for any $u \in C^{\mathrm{int}}$ it holds that
\begin{equation*}
\wt{\pi}(u) = \frac{\deg(u;\wt{G})}{\vol(\wt{G})} \geq \frac{\deg(u;\wt{G})}{\vol(G)} \geq \frac{(1 - \alpha \beta \vol(C;G))\deg(u;G)}{\vol(C;G)};
\end{equation*}
by plugging this in to~\eqref{pf:zhu4} we obtain
\begin{equation*}
\biggl(\frac{3(1 - \alpha \beta \vol(C;G))\cdot\bigl(1 - \alpha \tau_{\infty}(\wt{G})\bigr)}{4\vol(C;G)} - \beta\biggr) \cdot \deg(u;G) \leq \wt{p}_{\ell}(u),~~\textrm{for all $u \in C^{\mathrm{int}} \setminus S_{\beta,v}$},
\end{equation*}
and summing over all such $u$ gives
\begin{equation*}
\biggl(\frac{3(1 - \alpha \beta \vol(C;G))\cdot\bigl(1 - \alpha \tau_{\infty}(\wt{G})\bigr)}{4\vol(C;G)} - \beta\biggr) \cdot \vol\bigl(C^{\mathrm{int}} \setminus S_{\beta,v}; G\bigr) \leq \wt{p}_{\ell}\bigl(C^{\mathrm{int}} \setminus S_{\beta,v}\bigr) \leq 2\frac{\Phi(C;G)}{\alpha}.
\end{equation*}
The upper bounds in~\eqref{eqn:zhu_condition} imply
\begin{equation*}
\biggl(\frac{3(1 - \alpha \beta \vol(C;G))\cdot\bigl(1 - \alpha \tau_{\infty}(\wt{G})\bigr)}{4\vol(C;G)} - \beta\biggr) \geq \frac{2}{3}\beta,
\end{equation*}
and we conclude that
\begin{equation}
\label{pf:zhu5}
\vol(C^{\mathrm{int}} \setminus S_{\beta,v}; G) \leq \frac{3\Phi(C;G)}{\alpha\beta}.
\end{equation}

Finally, we upper bound $\vol(C^{\mathrm{bdry}} \setminus S_{\beta,v};G)$. Indeed, for any $u \in C^{\mathrm{bdry}}$,
\begin{equation*}
\frac{1}{\vol(C;G)}\sum_{w \not\in C} \1((u,w) \in E) \geq \alpha \cdot \beta \cdot \deg(u;G)
\end{equation*}
and summing over all such vertices yields
\begin{equation}
\label{pf:zhu6}
\vol(C^{\mathrm{bdry}};G) \leq \frac{1}{\alpha \beta \vol(C;G)}\sum_{\substack{u \in C^{\mathrm{bdry}} \\ w \not\in C}} \1((u,w) \in E) \leq \frac{\Phi(C;G)}{\alpha \cdot \beta}.
\end{equation} 
The claim follows upon summing the upper bounds in~\eqref{pf:zhu3.5}, \eqref{pf:zhu5} and~\eqref{pf:zhu6}.

\subsection{Uniform bounds on PPR}
\label{subsec:ppr_uniform_bounds}
As mentioned in our main text, in order to prove Theorem~\ref{thm:density_cluster_consistent_recovery}, we require a uniform bound on the PPR vector. Actually, we require two such bounds: for a candidate cluster $C \subseteq V$ and an alternative cluster $C' \subseteq V$, we require a lower bound on $p_v(u)$ for all $u \in C$, and an upper bound on $p_v(u')$ for all $u' \in C'$. In Lemma~\ref{lem:ppr_uniform_bound} we establish an upper bound that holds for all vertices $u$ in the interior $C_{o}$ of $C$, and a lower bound holds for all vertices $u'$ in the interior of $C_{o}'$ of $C'$; here
\begin{equation*}
C_{o} = \Bigl\{u \in C: \deg(u,\wt{G}) =  \deg(u;G)\Bigr\},~~\textrm{and}~~C_{o}'= \Bigl\{u \in C': \deg(u,G[C']) =  \deg(u;G)\Bigr\},
\end{equation*}
and we remind the reader that $\wt{G} = G[C]$. 
\begin{lemma}
	\label{lem:ppr_uniform_bound}
	Let $C$ and $C'$ be disjoint subsets of $V$, and suppose that
	\begin{equation*}
	\alpha \leq \frac{1}{2\tau_{\infty}(\wt{G})}.
	\end{equation*}
	Then there exists a set $C^g \subseteq C$ with $\vol(C^g;G) \geq \vol(C;G)/2$ such that for any $v \in C^g$,
	\begin{equation}
	\label{eqn:ppr_uniform_bound_C}
	p_v(u) \geq \frac{3}{8}\wt{\pi}(u) - \frac{2 \Phi(C;G)}{d_{\min}(\wt{G})\cdot \alpha}~~\textrm{for all $u \in C_{o}$}
	\end{equation}
	and
	\begin{equation}
	\label{eqn:ppr_uniform_bound_Cprime}
	p_v(u') \leq \frac{2\Phi(C;G)}{d_{\min}(C';G) \cdot \alpha}~~\textrm{for all $u \in C_{o}'$.}
	\end{equation}
\end{lemma}

\paragraph{``Leakage'' and ``soakage'' vectors.} To prove Lemma~\ref{lem:ppr_uniform_bound}, we will make use of the following explicit representation of the \emph{leakage} distribution $\ell$ from~\eqref{pf:zhu3}, as well as an analogously defined \emph{soakage} distribution $s$:
\begin{equation}
\label{eqn:leakage_soakage}
\begin{aligned}
\ell^{(t)} & := e_v(W \wt{I})^t(I - D^{-1}\wt{D}),~~&& \ell = \sum_{t = 0}^{\infty} (1 - \alpha)^t \ell^{(t)} \\
s^{(t)} & := e_v(W \wt{I})^t W (I - \wt{I}),~~&& s = \sum_{t = 0}^{\infty} (1 - \alpha)^t s^{(t)}.
\end{aligned}
\end{equation}
In the above, $\wt{I} \in \Reals^{n \times n}$ is a diagonal matrix with $I_{uu} = 1$ if $u \in C$ and $0$ otherwise, and $\wt{D}$ is the diagonal matrix with $\wt{D}_{uu} = \deg(u;\wt{G})$ if $u \in C$, and $0$ otherwise. 

These quantities admit a natural interpretation in terms of random walks. For $u \in C$, $\ell^{(t)}(u)$ is the probability that a lazy random walk over $G$ originating at $v$ stays within $\wt{G}$ for $t$ steps, arriving at $u$ on the $t$th step, and then ``leaks out'' of $C$ on the $(t + 1)$st step. On the other hand, for $u \not\in C$, $s^{(t)}(u)$ is the probability that a lazy random walk over $G$ originating at $v$ stays within $\wt{G}$ for $t$ steps and is then ``soaked up'' into $u$ on the $(t + 1)$st step. The vectors $\ell$ and $s$ then give the total mass leaked and soaked, respectively, by the PPR vector. 

Three properties of $\ell$ and $s$ are worth pointing out. First, $\mathrm{supp}(\ell) \subseteq C \setminus C_o$ and $\mathrm{supp}(s) \subseteq V \setminus C$. Second, $\|\ell^{(t)}\|_1 = \|s^{(t)}\|_1$ for all $t \in \mathbb{N}$, and so $\|\ell\|_1 = \|s\|_1$. Third, for any $u \in V \setminus C$, $p_v(u) = p_s(u)$. The first two properties are immediate. The third property follows by the law of total probability, which implies that
\begin{equation*}
q_v^{(\tau)}(u) = \sum_{t = 0}^{\tau} q_{s^{(t)}}^{(\tau - t)}(u),~~\textrm{for all $u \in V \setminus C$.}
\end{equation*}
or in terms of the PPR vector,
\begin{equation*}
p_v(u) = \alpha \sum_{\tau = 0}^{\infty} (1 - \alpha)^{\tau} q_v^{(\tau)}(u) = \alpha \sum_{\tau = 0}^{\infty} \sum_{t = 0}^{\tau} (1 - \alpha)^{\tau} q_{s^{(t)}}^{(\tau - t)}(u).
\end{equation*}
Substituting $\Delta = \tau + t$ and rearranging gives the claimed property, as
\begin{equation*}
p_v(u) = \alpha \sum_{\tau = 0}^{\infty} \sum_{t = 0}^{\tau} (1 - \alpha)^{\tau} q_{s^{(t)}}^{(\tau - t)}(u) = \sum_{\Delta = 0}^{\infty} \sum_{t = 0}^{\infty} (1 - \alpha)^{\Delta + t} q_{s^{(t)}}^{(\Delta)}(u) = \alpha \sum_{\Delta = 0}^{\infty} (1 - \alpha)^{\Delta} q_s^{(\Delta)}(u) = p_s(u).
\end{equation*}

\begin{proof}[Proof (of Lemma~\ref{lem:ppr_uniform_bound})]
	We first show~\eqref{eqn:ppr_uniform_bound_C}. From~\eqref{pf:zhu3} and~\eqref{pf:zhu2}, we have that
	\begin{equation*}
	p_v(u) \geq \frac{3}{4}\bigl(1 - \alpha \cdot \tau_{\infty}(\wt{G})\bigr) \cdot \wt{\pi}(u) - \wt{p}_{\ell}(u)~~\textrm{for all $u \in C$,}
	\end{equation*}
	where $\ell$ has support $\mathrm{supp}(\ell) \subseteq C$ with $\|\ell\|_1 \leq 2\Phi(C;G)/\alpha$. Recalling that $u \in C_{o}$ implies that $u \not\in \mathrm{supp}(C_{o})$, as a consequence of~\eqref{pf:interpolator_bound_max_entry}, 
	\begin{equation*}
	\wt{p}_{\ell}(u) \leq \frac{\|\ell\|_1}{d_{\min}(\wt{G})}~~\textrm{for all $u \in C_{o}$,}
	\end{equation*}
	establishing~\eqref{eqn:ppr_uniform_bound_C}. The proof of~\eqref{eqn:ppr_uniform_bound_Cprime} follows similarly:
	\begin{equation*}
	p_v(u) = p_s(u) \overset{(i)}{\leq} \frac{\|s\|_1}{d_{\min}(C';G)} = \frac{\|\ell\|_1}{d_{\min}(C';G)},~~\textrm{for all $u \in C_{o}'$},
	\end{equation*}
	where the presence of $d_{\min}(C';G)$ on the right hand side of $(i)$ can be verified by inspecting~\eqref{pf:interpolator_bound_max_entry_inductive_step}. 	
\end{proof}

\subsection{Mixedness of lazy random walk and PPR vectors}
\label{subsec:lovasz_simonovits_bounds}
In this subsection, we give upper bounds on $h^{(t)} := h_{q_v^{(t)}}$ and $h^{(\alpha)} := h_{p_v}$. Although similar bounds exist in the literature (see in particular Theorem~1.1 of \citep{lovasz1990} and Theorem~3 of~\citep{andersen2006}), we could not find precisely the results we needed, and so for completeness we state and prove these results ourselves. 

\begin{theorem}
	\label{thm:mixing_time_rw}
	For any $k \in [0, 2m]$, $t_0 \in \mathbb{N}$ and $t \geq t_0$,
	\begin{equation}
	\label{eqn:mixing_time_rw_1}
	h^{(t)}(k) \leq \frac{1}{2^{t_0}} + \frac{d_{\max}(G)}{d_{\min}(G)^2} + \frac{m}{d_{\min}(G)^2} \biggl(1 - \frac{\Psi(G)^2}{8}\biggr)^{t - t_0}.
	\end{equation}
\end{theorem}


\begin{theorem}
	\label{thm:mixing_time_PPR}
	Let $\phi$ be any constant in $[0,1]$. Either the following bound holds for any $t \in \mathbb{N}$ and any $k \in [d_{\max}(G),2m - d_{\max}(G)]$:
	\begin{equation*}
	h^{(\alpha)}(k) \leq \alpha t + \frac{2\alpha}{1 + \alpha} + \frac{d_{\max}(G)}{d_{\min}(G)^2} + \frac{m}{d_{\min}(G)^2} \left(1 - \frac{\phi^2}{8}\right)^{t},
	\end{equation*}
	or there exists some sweep cut $S_j$ of $p_v$ such that $\Phi(S_j;G) < \phi$.
\end{theorem}

The proofs of these upper bounds will be similar to each other (in places word-for-word alike), and will follow a similar approach and use similar notation to that of \citep{lovasz1990,andersen2006}. For $h: [0,2m] \to [0,1]$, $0 \leq K_0 \leq m$ and $k \in [K_0,2m - K_0]$, define
\begin{equation*}
L_{K_0}(k;h) = \frac{2m - K_0 - k}{2m - 2K_0}h(K_0) + \frac{k - K_0}{2m - 2K_0}h(2m - K_0)
\end{equation*}
to be the linear interpolant of $h(K_0)$ and $h(2m - K_0)$, and additionally let
\begin{equation*}
C(K_0;h) := \max\set{\frac{h(k) - L_{K_0}(k;h)}{\sqrt{\wb{k}}}: K_0 \leq k \leq  2m - K_0}.
\end{equation*}
where we use the notation $\wb{k} := \min\{k, 2m - k\}$, and treat $0/0$ as equal to $1$. Our first pair of Lemmas upper bound $h^{(t)}$ and $h^{(\alpha)}$ as a function of $L_{K_0}$ and $C(K_0)$. Lemma~\ref{lem:mixing_random_walk} implies that if $t$ is large relative to $\Psi(G)$, then $h^{(t)}(\cdot)$ must be small.

\begin{lemma}[\textbf{c.f. Theorem 1.2 of~\citep{lovasz1990}}]
	\label{lem:mixing_random_walk}
	For any $K_0 \in [0,m]$, $k \in [K_0, 2m - K_0]$, $t_0 \in \mathbb{N}$ and $t \geq t_0$,
	\begin{equation}
	\label{eqn:mixing_random_walk}
	h^{(t)}(k) \leq L_{K_0}(k;h^{(t_0)}) + C(K_0; h^{(t_0)}) \sqrt{\wb{k}} \cdot \Bigl(1 - \frac{\Psi(G)^2}{8}\Bigr)^{t - t_0}
	\end{equation}
\end{lemma}

Lemma~\ref{lem:mixing_time_PPR} implies that if the PPR random walk is not well mixed, then some sweep cut of $p_v$ must have small normalized cut.

\begin{lemma}[\textbf{c.f Theorem~3 of \citep{andersen2006}}]
	\label{lem:mixing_time_PPR}
	Let $\phi \in [0,1]$. Either the following bound holds for any $t \in \mathbb{N}$, any $K_0 \in [0,m]$, and any $k \in [K_0,2m - K_0]$:
	\begin{equation}
	\label{eqn:mixing_time_PPR}
	h^{(\alpha)}(k) \leq \alpha t + L_{K_0}(k; h^{(\alpha)}) + C(K_0;h^{(\alpha)})\sqrt{\wb{k}}\left(1 - \frac{\phi^2}{8}\right)^t
	\end{equation}
	or else there exists some sweep cut $S_{j}$ of $p_v$ such that $\Phi(S_j;G) < \phi$.
\end{lemma}

In order to make use of these Lemmas, we require upper bounds on $L_{K_0}(\cdot,h)$ and $C(K_0;h)$, for each of $h = h^{(t_0)}$ and $h = h^{(\alpha)}$. Of course, trivially $L_{K_0}(k;h) \leq \max\{h(K_0); h(2m - K_0)\}$ for any $k \in [K_0, 2m - K_0]$. As it happens, this observation will lead to sufficient upper bounds on $L_{K_0}(k,h)$ for both $h = h^{(t_0)}$ (Lemma~\ref{lem:interpolator_bound_rw}) and $h = h^{(\alpha)}$ (Lemma~\ref{lem:interpolator_bound_ppr}).  
\begin{lemma}
	\label{lem:interpolator_bound_rw}
	For any $t_0 \in \mathbb{N}$ and $K_0 \in [0,m]$, the following inequalities hold:
	\begin{equation}
	\label{eqn:interpolator_bound_rw}
	h^{(t_0)}\bigl(2m - K_0\bigr) \leq \frac{K_0}{2m}~~\textrm{and}~~h^{(t_0)}\bigl(K_0\bigr) \leq \frac{K_0}{d_{\min}(G)^2} + \frac{1}{2^{t_0}}.
	\end{equation}
	As a result, for any $k \in [K_0, 2m - K_0]$,
	\begin{equation}
	\label{eqn:interpolator_bound_rw_2}
	L_{K_0}(k;h^{(t_0)}) \leq \max\Bigl\{\frac{K_0}{2m}, \frac{K_0}{d_{\min}(G)^2} + \frac{1}{2^{t_0}}\Bigr\} = \frac{K_0}{d_{\min}(G)^2} + \frac{1}{2^{t_0}}.
	\end{equation}
\end{lemma}

\begin{lemma}
	\label{lem:interpolator_bound_ppr}
	For any $\alpha \in [0,1]$ and $K_0 \in [0,m]$, the following inequalities hold:
	\begin{equation}
	\label{eqn:interpolator_bound_ppr}
	h^{(\alpha)}\bigl(2m - K_0\bigr) \leq \frac{K_0}{2m}~~\textrm{and}~~h^{(\alpha)}\bigl(K_0\bigr) \leq \frac{K_0}{d_{\min}(G)^2} + \frac{2\alpha}{1 + \alpha}.
	\end{equation}
	As a result, for any $k \in [K_0, 2m - K_0]$,
	\begin{equation}
	\label{eqn:interpolator_bound_ppr_2}
	L_{K_0}(k;h^{(\alpha)}) \leq \max\Bigl\{\frac{K_0}{2m}, \frac{K_0}{d_{\min}(G)^2} + \frac{2\alpha}{1 + \alpha}\Bigr\} = \frac{K_0}{d_{\min}(G)^2} + \frac{2\alpha}{1 + \alpha}.
	\end{equation}
\end{lemma}

We next establish an upper bound on $C_{K_0}(k;h)$, which rests on the following key observation: since $h(k)$ is concave and $L_{K_0}(K_0;h) = h(K_0)$, it holds that
\begin{equation}
\label{pf:linearization_bound_1}
\frac{h(k) - L_{K_0}(k)}{\sqrt{\wb{k}}} \leq
\begin{cases}
h'(K_0) \sqrt{k},~& k \leq m \\
-h'(2m - K_0) \sqrt{2m - k},~& k > m.
\end{cases}
\end{equation}
(Since $h$ is not differentiable at $k = k_j$, here $h'$ refers to the right derivative of $h$.)  

Lemma~\ref{lem:linearization_bound} gives good estimates for $h'(K_0)$ and $h'(2m - K_0)$, which hold for both $h = h^{(t_0)}$ and $h = h^{(\alpha)}$, and result in an upper bound on $C(K_0;h)$. Both the statement and proof of this Lemma rely on the following explicit representation of the Lovasz-Simonovits curve $h_q(\cdot)$. Order the vertices $q(u_{(1)})/\deg(u_{(1)};G) \geq q(u_{(2)})/\deg(u_{(2)};G) \geq \cdots \geq q(u_{(n)})/\deg(u_{(n)};G)$. Then for each $j = 0,\ldots,n - 1$, and for all $k \in [\vol(S_j),\vol(S_{j + 1}))$,  the function $h_q(k)$ satisfies
\begin{equation}
\label{eqn:lovasz_simonovits}
h_q(k) = \sum_{i = 0}^{j} \left(q(u_{(i)}) - \pi(u_{(i)})\right) + \frac{\bigl(k - \vol(S_j;G)\bigr)}{\deg(u_{(j + 1)};G)} \left(q(u_{(j+1)}) - \pi(u_{(j+1)})\right). 
\end{equation}
\begin{lemma}
	\label{lem:linearization_bound}
	The following statements hold for both $h = h^{(\alpha)}$ and $h = h^{(t_0)}$. 
	\begin{itemize}
		\item Let $K_0 = k_1 = \deg(v;G)$ if $u_{(1)} = v$, and otherwise $K = 0$. Then  
		\begin{equation}
		\label{eqn:right_derivative_1}
		h'\bigl(K_0\bigr) \leq \frac{1}{d_{\min}(G)^2}.
		\end{equation}
		\item For all $K_0 \in [0,m]$,
		\begin{equation}
		\label{eqn:right_derivative_2}
		h'(2m - K_0) \geq -\frac{d_{\max}(G)}{d_{\min}(G)\cdot \vol(G)}.
		\end{equation}
	\end{itemize}
	As a result, letting $K_0 = \deg(v;G)$ if $u_{(1)} = v$, and otherwise letting $K_0 = 0$, we have
	\begin{equation*}
	C(K_0,h) \leq \frac{\sqrt{m}}{d_{\min}(G)^2}.
	\end{equation*}
\end{lemma}
\paragraph{Proof of Theorems~\ref{thm:mixing_time_rw} and~\ref{thm:mixing_time_PPR}.}
\begin{proof}[Proof (of Theorem~\ref{thm:mixing_time_rw})]
	Take $K_0 = 0$ if $u_{(1)} \neq v$, and otherwise take $K_0 = \deg(v;G)$. Combining Lemmas~\ref{lem:mixing_random_walk},~\ref{lem:interpolator_bound_rw} and~\ref{lem:linearization_bound}, we obtain that for any $k \in [K_0,2m - K_0]$, 
	\begin{align*}
	h^{(t)}(k) & \leq \frac{1}{2^{t_0}} + \frac{K_0}{d_{\min}(G)^2}  + \frac{\sqrt{m}}{d_{\min}(G)^2} \sqrt{\wb{k}} \Bigl(1 - \frac{\Psi^2(G)}{8}\Bigr)^{t-t_0} \\
	& \leq \frac{1}{2^{t_0}} + \frac{d_{\max}(G)}{d_{\min}(G)^2}  + \frac{m}{d_{\min}(G)^2} \Bigl(1 - \frac{\Psi^2(G)}{8}\Bigr)^{t-t_0},
	\end{align*}
	where the second inequality follows since we have chosen $K_0 \leq d_{\max}(G)$, and since $\wb{k} \leq m$. If $K_0 = 0$, we are done. 
	
	Otherwise, we must still establish that~\eqref{eqn:mixing_random_walk} is a valid upper bound when $k \in [0, \deg(v;G)) ~\cup~ (2m - \deg(v;G),2m]$. If $k \in [0, \deg(v;G))$ then
	\begin{equation}
	\label{pf:mixing_random_walk_1}
	h^{(t)}(k) \overset{\eqref{pf:mixing_random_walk_inductive_step}}{\leq} h^{(t_0)}(k) \overset{(\textrm{i})}{\leq} h^{(t_0)}(K_0) \overset{\eqref{eqn:interpolator_bound_rw}}{\leq} \frac{K_0}{d_{\min}(G)^2} + \frac{1}{2^{t_0}},
	\end{equation}
	where $(\textrm{i})$ follows since $k \in [0,K_0]$, and $h^{(t_0)}$ is linear over $[0,K_0)$ with $h^{(t_0)}(0) = 0$ and $h^{(t_0)}(K_0) \geq 0$. For similar reasons,
	\begin{equation}
	\label{pf:mixing_random_walk_2}
	h^{(t)}(k) \leq h^{(t_0)}(k) \leq h^{(t_0)}(2m - K_0) \leq \frac{\deg(v;G)}{2m}.
	\end{equation}
	Since the ultimate upper bounds in~\eqref{pf:mixing_random_walk_1} and~\eqref{pf:mixing_random_walk_2} are each no greater than that of~\eqref{eqn:mixing_random_walk}, the claim follows.
\end{proof}

\begin{proof}[Proof (of Theorem~\ref{thm:mixing_time_PPR})]
	The proof of Theorem~\ref{thm:mixing_time_PPR} follows immediately from Lemmas~\ref{lem:mixing_time_PPR},~\ref{lem:interpolator_bound_ppr} and~\ref{lem:linearization_bound}, taking $K_0 = 0$ if $u_{(1)} \neq v$ and otherwise $K_0 = \deg(v;G)$. 
\end{proof}

\paragraph{Proofs of Lemmas.}
In what follows, for a distribution $q$ and vertices $u,w \in V$, we write $q(u,w) := q(u)/d(u) \cdot 1\{(u,w) \in E\}$, and similarly for a collection of dyads $\wt{E} \subseteq V \times V$ we write $q(\wt{E}) := \sum_{(u,w) \in \wt{E}} q(u,w)$. 
\begin{proof}[Proof (of Lemma~\ref{lem:mixing_random_walk})]
	 We will prove Lemma~\ref{lem:mixing_random_walk} by induction on $t$. In the base case $t = t_0$, observe that $C(K_0;h^{(t_0)}) \cdot \sqrt{\wb{k}} \geq h^{(t_0)}(k) - L_{K_0}(k;h^{(t_0)})$ for all $k \in [K_0, 2m - K_0]$, which implies 
	\begin{equation*}
	L_{K_0}(k;h^{(t_0)}) + C(K_0; h^{(t_0)}) \cdot \sqrt{\wb{k}} \geq h^{(t_0)}(k).
	\end{equation*}
	
	Now, we proceed with the inductive step, assuming that the inequality holds for $t_0,t_0 + 1,\ldots,t - 1$, and proving that it thus also holds for $t$. By the definition of $L_{K_0}$, the inequality~\eqref{eqn:mixing_random_walk} holds when $k = K_0$ or $k = 2m - K_0$. We will additionally show that~\eqref{eqn:mixing_random_walk} holds for every $k_j = \vol(S_j), j = 1,2,\ldots,n$ such that $k_j \in [K_0, 2m - K_0]$. This suffices to show that the inequality~\eqref{eqn:mixing_random_walk} holds for all $k \in [K_0,2m - K_0]$, since the right hand side of~\eqref{eqn:mixing_random_walk} is a concave function of $k$.
	
	Now, we claim that for each $k_j$, it holds that
	\begin{equation}
	\label{pf:mixing_random_walk_inductive_step}
	q_v^{(t)}[k_j] \leq \frac{1}{2}\Bigl(q_v^{(t - 1)}[k_j - \wb{k}_j \Psi(G)] + q_v^{(t - 1)}[k_j + \wb{k}_j \Psi(G)]\Bigr).
	\end{equation}
	To establish this claim, we note that for any $u \in V$
	\begin{equation*}
	q_v^{(t)}(u) = \frac{1}{2}q_v^{(t - 1)}(u) + \frac{1}{2}\sum_{w \in V}q_v^{(t - 1)}(w,u) = \frac{1}{2} \sum_{w \in V} \bigl(q_v^{(t - 1)}(u,w) + q_v^{(t - 1)}(w,u)\bigr),
	\end{equation*}
	and consequentially for any $S \subset V$,
	\begin{align*}
	q_v^{(t)}(S) & = \frac{1}{2}\bigl\{q_v^{(t - 1)}(\mathrm{in}(S)) +  q_v^{(t - 1)}(\mathrm{out}(S))\bigr\} \\
	& = \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl(\mathrm{in}(S) \cup \mathrm{out}(S)\bigr) +  q_v^{(t - 1)}\bigl(\mathrm{in}(S) \cap \mathrm{out}(S)\bigr)\bigr\}
	\end{align*}
	where $\mathrm{in}(S) = \{(u,w) \in E: u \in S\}$ and $\mathrm{out}(S) = \{(w,u) \in E: w \in S\}$. We deduce that
	\begin{align*}
	q_v^{(t)}[k_j] = q_v^{(t)}(S_j) & = \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl(\mathrm{in}(S_j) \cup \mathrm{out}(S_j)\bigr) +  q_v^{(t - 1)}\bigl(\mathrm{in}(S_j) \cap \mathrm{out}(S_j)\bigr)\bigr\} \\
	& \leq \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl[|\mathrm{in}(S_j) \cup \mathrm{out}(S_j)|\bigr] +  q_v^{(t - 1)}\bigl[|\mathrm{in}(S_j) \cap \mathrm{out}(S_j)|\bigr]\bigr\} \\
	& = \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl[k_j + \mathrm{cut}(S_j;G)\bigr] +  q_v^{(t - 1)}\bigl[k_j - \mathrm{cut}(S_j;G)\bigr]\bigr\} \\
	& \leq \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl[k_j + \wb{k}_j\Phi(S_j;G)\bigr] +  q_v^{(t - 1)}\bigl[k_j - \wb{k}_j\Phi(S_j;G)\bigr]\bigr\} \\
	& \leq \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl[k_j + \wb{k}_j\Psi(G)\bigr] +  q_v^{(t - 1)}\bigl[k_j - \wb{k}_j\Psi(G)\bigr]\bigr\},
	\end{align*}
	establishing~\eqref{pf:mixing_random_walk_inductive_step}. The final two inequalities both follow from the concavity of $q_v^{(t)}[\cdot]$. 
	
	Subtracting $k_j/2m$ from both sides, we get
	\begin{equation}
	\label{pf:mixing_random_walk_inductive_step_h}
	h^{(t)}(k_j) \leq \frac{1}{2}\bigl\{h^{(t - 1)}\bigl(k_j + \wb{k}_j\Psi(G)\bigr)+  h^{(t - 1)}\bigl(k_j - \wb{k}_j\Psi(G)\bigr)\bigr\}.
	\end{equation}
	At this point, we divide our analysis into cases.
	
	\textbf{Case 1.}
	Assume $k_j - \Psi(G) \wb{k}_j$ and $k_j + 2 \Psi(G) \wb{k}_j$ are both in $[K_0,2m  - K_0]$. We are therefore in a position to apply our inductive hypothesis to both terms on the right hand side of~\eqref{pf:mixing_random_walk_inductive_step_h} and obtain the following:
	\begin{align*}
	h^{(t)}(k_j) & \leq \frac{1}{2}\biggl(L_{K_0}\bigl(k_j - \Psi(G) \wb{k}_j; h^{(t_0)}\bigr) + L_{K_0}\bigl(k_j + \Psi(G) \wb{k}_j; h^{(t_0)}\bigr) \biggr) ~~+ \\
	& \quad~ \frac{1}{2}C\bigl(K_0; h^{(t_0)}\bigr) \cdot \Bigl(\sqrt{\wb{k_j - \Psi(G) \wb{k}_j}} + \sqrt{\wb{k_j + \Psi(G) \wb{k}_j}}\Bigr)\left(1 - \frac{\Psi(G)^2}{8}\right)^{t-t_0 - 1} \\
	& = L_{K_0}(k;h^{(t_0)}) + \frac{1}{2}\biggl(C(K_0;h^{t_0})\bigl(\sqrt{\wb{k_j - \Psi(G) \wb{k}_j}} + \sqrt{\wb{k_j + \Psi(G) \wb{k}_j}}\bigr)\left(1 - \frac{\Psi(G)^2}{8}\right)^{t-t_0 - 1} \biggr) \\
	& \leq L_{K_0}(k;h^{(t_0)}) + \frac{1}{2}\biggl(C(K_0;h^{(t_0)})\bigl(\sqrt{\wb{k}_j - \Psi(G) \wb{k}_j} + \sqrt{\wb{k}_j + \Psi(G) \wb{k}_j}\bigr)\left(1 - \frac{\Psi(G)^2}{8}\right)^{t-t_0 - 1} \biggr).
	\end{align*}
	A Taylor expansion of $\sqrt{1 + \Psi(G)}$ around $\Psi(G) = 0$ yields the following bound:
	\begin{equation*}
	\sqrt{1 + \Psi(G)} + \sqrt{1 - \Psi(G)} \leq 2 - \frac{\Psi(G)^2}{4},
	\end{equation*}
	and therefore
	\begin{align*}
	h^{(t)}(k_j) & \leq L_{K_0}(k;h^{(t_0)}) + \frac{C(K_0;h^{(t_0)})}{2}\cdot \sqrt{\wb{k}_j}\cdot\left(2 - \frac{\Psi(G)^2}{4}\right)\left(1 - \frac{\Psi(G)^2}{8}\right)^{t-1} \\
	&= L_{K_0}(k_j;h^{(t_0)}) + C(K_0;h^{(t_0)})\sqrt{\wb{k}_j}\left(1 - \frac{\Psi(G)^2}{8}\right)^{t - t_0}.
	\end{align*}
	
	\textbf{Case 2.} Otherwise one of $k_j - 2 \Psi(G) \wb{k}_j$ or $k_j + 2 \Psi(G) \wb{k}_j$ is not in $[K_0,2m  - K_0]$. Without loss of generality assume $k_j < m$, so that (i) we have $k_j - 2 \Psi(G) \wb{k}_j < K_0$ and (ii) $k_j + (k_j - K_0) \leq 2m - K_0$. We deduce the following:
	\begin{align*}
	h^{(t)}(k_j) & \overset{(\textrm{i})}{\leq} \frac{1}{2}\Bigl(h^{(t - 1)}(K_0) + h^{(t - 1)}\bigl(k_j + (k_j - K_0)\bigr)\Bigr) \\
	& \overset{(\textrm{ii})}{\leq} \frac{1}{2}\Bigl(h^{(t_0)}(K_0) + h^{(t)}\bigl(k_j + (k_j - K_0)\bigr)\Bigr) \\
	& \overset{(\textrm{iii})}{\leq}\frac{1}{2}\Bigl(L_{K_0}(K_0;h^{(t_0)}) + L_{K_0}(2k_j - K_0; h^{(t_0)}\bigr) + C(K_0;h^{(t_0)})\sqrt{\wb{2k_j - K_0}}\left(1 - \frac{\Psi(G)^2}{8}\right)^{t - t_0 - 1}\Bigr) \\
	& \leq L_{K_0}(k_j; h^{(t_0)}) + C(K_0;h^{(t_0)}) \frac{\sqrt{2\wb{k}_j}}{2} \left(1 - \frac{\Psi(G)^2}{8}\right)^{t - t_0 - 1} \\
	& \leq L_{K_0}(k_j;h^{(t_0)}) + C(K_0;h^{(t_0)}) \sqrt{\wb{k}_j} \cdot \left(1 - \frac{\Psi(G)^2}{8}\right)^{t - t_0}
	\end{align*}
	where $(\textrm{(i)})$ follows from~\eqref{pf:mixing_random_walk_inductive_step_h} and the concavity of $h^{(t - 1)}$,  we deduce $(\textrm{ii})$ from~\eqref{pf:mixing_random_walk_inductive_step_h}, which implies that $h^{(t)}(k) \leq h^{(t_0)}(k)$, and $(\textrm{iii})$ follows from applying the inductive hypothesis to $h^{(t - 1)}(2k_j - K_0)$. 	
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:mixing_time_PPR}).]
	We will show that if $\Phi(S_j; g) \geq \phi$ for each $j = 1,\ldots,n$, then \eqref{eqn:mixing_time_PPR} holds for all $t$ and any $k \in [K_0,2m - K_0]$.
	
	We proceed by induction on $t$. Our base case will be $t = 0$. Observe that $C(K_0;h^{(\alpha)}) \cdot \sqrt{\wb{k}} \geq h^{(\alpha)}(k) - L_{K_0}(k;h^{(\alpha)})$ for all $k \in [K_0,2m - K_0]$, which implies
	\begin{equation*}
	L_{K_0}(k;h^{(\alpha)}) + C(K_0;h^{(\alpha)}) \cdot \sqrt{\wb{k}} \geq h^{(\alpha)}(k).
	\end{equation*}
	
	Now, we proceed with the inductive step. By the definition of $L_{K_0}$, the inequality~\eqref{eqn:mixing_time_PPR} holds when $k = K_0$ or $k = 2m - K_0$. We will additionally show that~\eqref{eqn:mixing_time_PPR} holds for every $k_j = \vol(S_j), j = 1,2,\ldots,n$ such that $k_j \in [K_0, 2m - K_0]$. This suffices to show that the inequality~\eqref{eqn:mixing_time_PPR} holds for all $k \in [K_0,2m - K_0]$, since the right hand side of~\eqref{eqn:mixing_time_PPR} is a concave function of $k$.
	
	By Lemma 5 of \citet{andersen2006}, we have that
	\begin{align}
	p_v[k_j] & \leq \alpha + \frac{1}{2} \bigl(p_v[k_j - \mathrm{cut}(S_j;G)] + p_v[k_j + \mathrm{cut}(S_j;G)] \bigr) \nonumber\\
	& \leq \alpha + \frac{1}{2} \bigl(p_v[k_j - \Phi(S_j;G) \wb{k}_j] + p_v[k_j + \Phi(S_j;G) \wb{k}_j]  \bigr) \nonumber \\
	& \leq \alpha + \frac{1}{2} \bigl(p_v[k_j - \phi \wb{k}_j] + p_v[k_j + \phi \wb{k}_j]\bigr) \nonumber
	\end{align}
	and subtracting $k_j/2m$ from both sides, we get
	\begin{equation}
	\label{eqn:mixing_time_PPR_pf1}
	h^{(\alpha)}(k_j) \leq \alpha + \frac{1}{2} \bigl(h^{(\alpha)}(k_j - \phi \wb{k}_j) + h^{(\alpha)}(k_j +  \phi \wb{k}_j) \bigr)
	\end{equation}
	From this point, we divide our analysis into cases. 
	
	\textbf{Case 1.}
	Assume $k_j - 2 \phi \wb{k}_j$ and $k_j + 2 \phi \wb{k}_j$ are both in $[K_0,2m  - K_0]$. We are therefore in a position to apply our inductive hypothesis to \eqref{eqn:mixing_time_PPR_pf1}, yielding
	\begin{align*}
	h^{(\alpha)}(k_j) & \leq \alpha + \alpha(t-1) \frac{1}{2}\biggl(L_{K_0}(k_j - \phi \wb{k}_j) + L_{K_0}(k_j + \phi \wb{k}_j) + C(K_0;h^{(\alpha)})\bigl(\sqrt{\wb{k_j - \phi \wb{k}_j}} + \sqrt{\wb{k_j + \phi \wb{k}_j}}\bigr)\left(1 - \frac{\phi^2}{8}\right)^{t-1} \biggr) \\
	& \leq \alpha t + L_{K_0}(k;h^{(\alpha)}) + \frac{1}{2}\biggl(C(K_0;h^{(\alpha)})\bigl(\sqrt{\wb{k_j - \phi \wb{k}_j}} + \sqrt{\wb{k_j + \phi \wb{k}_j}}\bigr)\left(1 - \frac{\phi^2}{8}\right)^{t-1} \biggr) \\
	& \leq \alpha t + L_{K_0}(k;h^{(\alpha)}) + \frac{1}{2}\biggl(C(K_0;h^{(\alpha)})\bigl(\sqrt{\wb{k}_j - \phi \wb{k}_j} + \sqrt{\wb{k}_j + \phi \wb{k}_j}\bigr)\left(1 - \frac{\phi^2}{8}\right)^{t-1} \biggr).
	\end{align*}
	and therefore
	\begin{equation*}
	h^{(\alpha)}(k_j) \leq  \alpha t + L_{K_0}(k;h^{(\alpha)}) + \frac{C(K_0;h^{(\alpha)})}{2}\cdot \sqrt{\wb{k}_j}\cdot\left(2 - \frac{\phi^2}{4}\right)\left(1 - \frac{\phi^2}{8}\right)^{t-1} = \alpha t + L_{K_0}(k;h^{(\alpha)}) + C(K_0;h^{(\alpha)})\sqrt{\wb{k}_j}\left(1 - \frac{\phi^2}{8}\right)^{t}.
	\end{equation*}
	
	\textbf{Case 2.} Otherwise one of $k_j - 2 \phi \wb{k}_j$ or $k_j + 2 \phi \wb{k}_j$ is not in $[K_0,2m  - K_0]$. Without loss of generality assume $k_j < m$, so that (i) we have $k_j - 2 \phi \wb{k}_j < K_0$ and (ii) $k_j + (k_j - K_0) \leq 2m - K_0$. By the concavity of $h$, and applying the inductive hypothesis to $h^{(\alpha)}2k_j - K_0)$, we have
	\begin{align*}
	h^{(\alpha)}(k_j) & \leq \alpha + \frac{1}{2}\Bigl(h^{(\alpha)}(K_0) + h\bigl(k_j + (k_j - K_0)\bigr)\Bigr) \\
	& \leq\alpha + \frac{\alpha(t - 1)}{2} + \frac{1}{2}\Bigl(L_{K_0}(K_0;p^{\alpha}) + L_{K_0}(2k_j - K_0\bigr) + C(K_0;h^{(\alpha)})\sqrt{\wb{2k_j - K_0}}\left(1 - \frac{\phi^2}{8}\right)^{t - 1}\Bigr) \\
	& \leq \alpha t + L_{K_0}(k_j) + C(K_0;h^{(\alpha)}) \frac{\sqrt{2\wb{k}_j}}{2} \left(1 - \frac{\phi^2}{8}\right)^{t - 1} \\
	& \leq \alpha t + L_{K_0}(k_j) + C(K_0;h^{(\alpha)}) \sqrt{\wb{k}_j} \cdot \left(1 - \frac{\phi^2}{8}\right)^{t}
	\end{align*}
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:interpolator_bound_rw})]
	We will prove that the inequalities of~\eqref{eqn:interpolator_bound_rw} hold at the knot points of $h^{(t_0)}$, whence they follow for all $K_0 \in [0,m]$. 
	
	We first prove the upper bound on $h^{(t_0)}(2m - K_0)$, when $2m - K_0 = k_j$ for some $j = 0,\ldots,n - 1$. Indeed, the following manipulations show the upper bound holds for $h_q(\cdot)$ regardless of the distribution $q$. Noting that $h_q(2m) = 0$, we have that,
	\begin{equation*}
	h_q(k_j) = h_q(k_j) - h_q(2m) = \sum_{i = j + 1}^{n} q(u_{(i)}) - \pi(u_{(i)}) \leq \sum_{i = j + 1}^{n} \pi(u_{(i)}) = 1 - \frac{k_j}{2m} = \frac{K_0}{2m}.
	\end{equation*}
	
	In contrast, when $K_0 = k_j$ the upper bound on $h^{(t_0)}(\cdot)$ depends on the properties of $q = q_v^{(t_0)}$. In particular, we claim that for any $t \in \mathbb{N}$,
	\begin{equation}
	\label{pf:interpolator_bound_max_entry}
	q_v^{(t)}(u) \leq
	\begin{dcases*}
	\frac{1}{d_{\min}(G)},& ~~\textrm{if $u \neq v$} \\
	\frac{1}{d_{\min}(G)} + \frac{1}{2^t},& ~~\textrm{if $u = v$.}
	\end{dcases*}
	\end{equation}
	This claim follows straightforwardly by induction. In the base case $t = 0$, the claim is obvious. If the claim holds true for a given $t \in \mathbb{N}$, then for $u \neq v$,
	\begin{equation}
	\label{pf:interpolator_bound_max_entry_inductive_step}
	\begin{aligned}
	q_v^{(t + 1)}(u) & = \frac{1}{2}\sum_{w \neq u}q_v^{(t)}(w,u)  + \frac{1}{2}q_v^{(t)}(u) \\
	& \leq \frac{1}{2d_{\min}(G)}\sum_{w \neq u}q_v^{(t)}(w)  + \frac{1}{2d_{\min}(G)} \\
	& \leq \frac{1}{d_{\min}(G)},
	\end{aligned}
	\end{equation}
	where the last inequality holds because $q_v^{(t)}$ is a probability distribution (i.e. the sum of its entries is equal to $1$). Similarly, if $u = v$, then
	\begin{align*}
	q_v^{(t + 1)}(v) & = \frac{1}{2}\sum_{w \neq v}q_v^{(t)}(w,v)  + \frac{1}{2}q_v^{(t)}(v) \\
	& \leq \frac{1}{2d_{\min}(G)}\sum_{w \neq u}q_v^{(t)}(w)  + \frac{1}{2d_{\min(G)}} + \frac{1}{2^{t + 1}} \\
	& \leq \frac{1}{d_{\min}(G)} + \frac{1}{2^{t + 1}},
	\end{align*}
	and the claim~\eqref{pf:interpolator_bound_max_entry} is shown. The upper bound on $h^{(t_0)}(K_0)$ for $K_0 = k_j$ follows straightforwardly:
	\begin{equation*}
	h^{(t_0)}(K_0) \leq \sum_{i = 0}^{j} q_v^{(t_0)}(u_{(j)}) \leq \frac{j}{d_{\min}(G)} + \frac{1}{2^{t_0}} \leq \frac{K_0}{d_{\min}(G)^2} + \frac{1}{2^{t_0}},
	\end{equation*}
	where the last inequality follows since $\vol(S) \geq |S| \cdot d_{\min}(G)$ for any set $S \subseteq V$. 
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:interpolator_bound_ppr})]
	We have already established the first upper bound in~\eqref{eqn:interpolator_bound_ppr}, in the proof of Lemma~\ref{lem:interpolator_bound_rw}. Then, noting that from~\eqref{pf:interpolator_bound_max_entry}, 
	\begin{equation}
	\label{eqn:ppr_max_entry}
	p_v(u) = \alpha \sum_{t = 0}^{\infty} (1 - \alpha)^t q_v^{(t)}(u) \leq
	\begin{dcases*}
	\alpha \sum_{t = 0}^{\infty} (1 - \alpha)^t \Bigl(\frac{1}{d_{\min}(G)} + \frac{1}{2^t}\Bigr) = \frac{1}{d_{\min}(G)} + \frac{2\alpha}{1 - \alpha}& ~~\textrm{if $u = v$} \\
	\alpha \sum_{t = 0}^{\infty} (1 - \alpha)^t \frac{1}{d_{\min}(G)} = \frac{1}{d_{\min}(G)} & ~~\textrm{if $u \neq v$,}
	\end{dcases*}
	\end{equation}
	the second upper bound in~\eqref{eqn:interpolator_bound_ppr} follows similarly to the proof of the equivalent upper bound in~Lemma~\ref{lem:interpolator_bound_rw}.
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:linearization_bound}).]
	The result of the Lemma follows obviously from~\eqref{pf:linearization_bound_1}, once we show \eqref{eqn:right_derivative_1}-\eqref{eqn:right_derivative_2}. We begin by showing~\eqref{eqn:right_derivative_1}. Inspecting the representation~\eqref{eqn:lovasz_simonovits}, we see that for any distribution $q$ and knot point $k_j$, the right derivative of $h_q$ can always be upper bounded,
	\begin{equation*}
	h_{q}'(k_j) \leq \frac{q(u_{(j + 1)})}{\deg(u_{(j + 1)};G)}.
	\end{equation*}
	We have chosen $K_0 = k_j$ so that $v \neq u_{(j + 1)}$, and so~\eqref{pf:interpolator_bound_max_entry} implies that $h_{q}'(k_j) \leq 1/(d_{\min}(G)^2)$, for either $q = q_v^{(t)}$ or $q = p_v$.
	
	On the other hand, the inequality \eqref{eqn:right_derivative_2} follows immediately from the representation \eqref{eqn:lovasz_simonovits}, since for any $K_0 \in [0,m]$, taking $j$ so that $2m - K_0 \in [k_j, k_{j + 1})$, 
	\begin{equation*}
	h'(2m - K_0) \geq -\frac{\pi(u_{(j+1)})}{\mathrm{deg}(u_{(j + 1)};G)} \geq -\frac{d_{\max}(G)}{d_{\min}(G) \cdot \vol(G)}.
	\end{equation*}
\end{proof}

\subsection{Proof of Proposition~\ref{prop:pointwise_mixing_time}}
\label{subsec:pf_prop_pointwise_mixing_time}
To prove Proposition~\ref{prop:pointwise_mixing_time}, we will upgrade from an upper bound on the total variation distance between $q_v^{(t)}$ and $\pi$ to the desired uniform upper bound. The \emph{total variation distance} between distributions $q$ and $p$ is
\begin{equation*}
\mathrm{TV}(q,p) := \frac{1}{2}\sum_{u \in v} \bigl|q(u) - p(u)\bigr|
\end{equation*}
It follows from the representation~\eqref{eqn:lovasz_simonovits} that
\begin{equation*}
\mathrm{TV}(q,\pi) = \max_{S \subseteq V} \Bigl\{q(S) - \pi(S)\Bigr\} = \max_{j = 1,\ldots,n} \Bigl\{q(S_j) - \pi(S_j)\Bigr\} = \max_{k \in [0,2m]} h_q(k),
\end{equation*}
so that Theorem~\ref{thm:mixing_time_rw} gives an upper bound on $\mathrm{TV}(q_v^{(t)},\pi)$. We can then use the following result to upgrade to a uniform upper bound.

\begin{lemma}
	\label{lem:tv_to_pointwise}
	For any $t \in \mathbb{N}$,
	\begin{equation*}
	\max_{u \in V} \Bigl\{\frac{\pi(u) - q_v^{(t + 1)}(u)}{\pi(u)}\Bigr\} \leq \frac{2 \cdot \mathrm{TV}(q_v^{(t)}, \pi)}{s(G)}.
	\end{equation*}
\end{lemma}

The proof of Proposition~\ref{prop:pointwise_mixing_time} is then straightforward.
\begin{proof}[Proof (of Proposition~\ref{prop:pointwise_mixing_time})]
	Put $t_{\ast} = 8/(\Psi(G)^2) \ln(4/s(G)) + 4$. We will use Theorem~\ref{thm:mixing_time_rw} to show that $\mathrm{TV}(q_v^{(t_{\ast})},\pi) \leq 1/4$. This will in turn imply (\cite{montenegro2002} pg. 13) that for $\tau_{\ast} = t_{\ast} \log_2(8/s(G))$,
	\begin{equation*}
	\mathrm{TV}(q_v^{(\tau_{\ast})},\pi) \leq \frac{1}{8}s(G),
	\end{equation*}
	and applying Lemma~\ref{lem:tv_to_pointwise} gives
	\begin{equation*}
	\max_{u \in V} \Bigl\{\frac{\pi(u) - q_v^{(\tau_{\ast} + 1)}(u)}{\pi(u)}\Bigr\} \leq \frac{1}{4}.
	\end{equation*}
	Taking maximum over all $v \in V$, we conclude that $\tau_{\infty}(G) \leq \tau_{\ast} + 1$, which is exactly the claim of Proposition~\ref{prop:pointwise_mixing_time}.
	
	It remains to show that $\mathrm{TV}(q_v^{(t_{\ast})},\pi) \leq 1/4$. Choosing $t_0 = 4$ in the statement of Theorem~\ref{thm:mixing_time_rw}, we have that
	\begin{align*}
	\mathrm{TV}(q_v^{(t_{\ast})},\pi) & \leq \frac{1}{16} + \frac{d_{\max}(G)}{d_{\min}(G)^2} + \frac{1}{2s(G)} \Bigl(1 - \frac{\Psi(G)^2}{8}\Bigr)^{t_{\ast} - 4} \\
	& \leq \frac{1}{8} + \frac{1}{2s(G)} \Bigl(1 - \frac{\Psi(G)^2}{8}\Bigr)^{t_{\ast} - 4} \\
	& \leq \frac{1}{8} + \frac{1}{2s(G)} \exp\Bigl(-\frac{\Psi(G)^2}{8}(t_{\ast} - 4)\Bigr) = \frac{1}{4},
	\end{align*}
	where the middle inequality follows by assumption.
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:tv_to_pointwise})]
	We proceed by induction. In the base case $t = 0$, we have that
	\begin{equation*}
	\max_{u \in V} \Bigl\{\frac{\pi(u) - q_v^{(t + 1)}(u)}{\pi(u)}\Bigr\} \leq 1 \leq 2(1 - \pi(v)) \leq 2\frac{\mathrm{TV}(q_v^{(0)},\pi)}{s(G)},
	\end{equation*},
	where the second inequality follows since $\pi(v) \leq d_{\max}(G)/(2m) \leq d_{\max}(G)/d_{\min}(G)^2 \leq 1/16$. 
	
	To prove the inductive step, the key observation is the following equivalence (see equation (16) of \citep{morris2005}):
	\begin{align}
	\label{pf:tv_to_pointwise_1}
	\frac{\pi(u) - q_v^{(t + 1)}(u)}{\pi(u)} & = \sum_{w \in V} \bigl(\pi(w) - q_v^{(t)}(w) \bigr) \cdot \Bigl(\frac{q_w^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr) \nonumber \\
	& = \sum_{w \neq u} \bigl(\pi(w) - q_v^{(t)}(w) \bigr) \cdot \Bigl(\frac{q_w^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr) + \bigl(\pi(u) - q_v^{(t)}(u) \bigr) \cdot \Bigl(\frac{q_u^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr)
	\end{align}
	We separately upper bound each term on the right hand side of~\eqref{pf:tv_to_pointwise_1}. The sum over all $w \neq u$ can be related to the TV distance between $q_v^{(t)}$ and $\pi$ using H{\"o}lder's inequality,
	\begin{align*}
	\sum_{w \neq u} \bigl(\pi(w) - q_v^{(t)}(w) \bigr) \cdot \Bigl(\frac{q_w^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr) & \leq 2\mathrm{TV}(q_v^{(t)},\pi) \cdot \max_{w \neq u}\Bigl|\frac{q_w^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr| \\
	& \leq 2\mathrm{TV}(q_v^{(t)},\pi) \cdot \max\biggl\{1, \max_{w \neq u} \frac{q_w^{(1)}(u)}{\pi(u)} \biggr\} \\
	& \leq 2\mathrm{TV}(q_v^{(t)},\pi) \cdot \frac{m}{d_{\min}(G)^2} = \frac{\mathrm{TV}(q_v^{(t)},\pi) }{s(G)}.
	\end{align*}
	On the other hand, the second term on the right hand side of~\eqref{pf:tv_to_pointwise_1} satisfies
	\begin{align*}
	\bigl(\pi(u) - q_v^{(t)}(u) \bigr) \cdot \Bigl(\frac{q_u^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr) \leq \bigl(\pi(u) - q_v^{(t)}(u) \bigr) \cdot \Bigl(\frac{1/2 - \pi(u)}{\pi(u)}\Bigr) \leq \frac{\pi(u) - q_v^{(t)}(u)}{2\pi(u)},
	\end{align*}
	so that we obtain the recurrence relation
	\begin{equation*}
	\frac{\pi(u) - q_v^{(t + 1)}(u)}{\pi(u)} \leq \frac{\mathrm{TV}(q_v^{(t)},\pi) }{s(G)} +\frac{\pi(u) - q_v^{(t)}(u)}{2\pi(u)}.
	\end{equation*}
	Then by the inductive hypothesis $\bigl(\pi(u) - q_v^{(t)}(u)\bigr)/\bigl(2\pi(u)\bigr) \leq \mathrm{TV}(q_v^{(t - 1)},\pi)/s(G)$, and consequentially
	\begin{equation*}
	\frac{\pi(u) - q_v^{(t + 1)}(u)}{\pi(u)} \leq \frac{\mathrm{TV}(q_v^{(t)},\pi) }{s(G)} + \frac{\mathrm{TV}(q_v^{(t - 1)},\pi) }{s(G)} \leq 2\frac{\mathrm{TV}(q_v^{(t)},\pi)}{s(G)}.
	\end{equation*}
	This completes the proof of Lemma~\ref{lem:tv_to_pointwise}.
\end{proof}

\subsection{Spectral partitioning properties of PPR}
\label{subsec:ppr_spectral_partitioning}
The following theorem is the main result of~\ref{subsec:ppr_spectral_partitioning}. It relates the normalized cut of the sweep sets $\Phi(S_{\beta};G)$ to the normalized cut of a candidate cluster $C \subseteq V$, when $p_v$ is properly initialized within $C$.

\begin{theorem}[\textbf{c.f. Theorem~6 of \cite{andersen2006}}]
	\label{thm:normalized_cut_ppr}
	Suppose that
	\begin{equation}
	\label{eqn:normalized_cut_ppr_vol}
	d_{\max}(G) \leq \vol(C;G) \leq \max\Bigl\{\frac{2}{3}\vol(G); \vol(G) - d_{\max}(G)\Bigr\}
	\end{equation}
	and
	\begin{equation}
	\label{eqn:normalized_cut_ppr_ncut}
	\max\Bigl\{288\Phi(C;G)\cdot \ln\Bigl(\frac{36}{s(G)}\Bigr),72\Phi(C;G) + \frac{d_{\max}(G)}{d_{\min}(G)^2}\Bigr\} < \frac{1}{18}.
	\end{equation}
	Set $\alpha = 36 \cdot \Phi(C;G)$. The following statement holds: there exists a set $C^g \subseteq C$ of large volume, $\vol(C^g;G) \geq 5/6 \cdot \vol(C;G)$, such that for any $v \in C^g$,  the minimum normalized cut of the sweep sets of $p_v$ satisfies 
	\begin{equation}
	\label{eqn:normalized_cut_ppr}
	\min_{\beta \in (0,1)}\Phi(S_{\beta,v};G) < 72\sqrt{\Phi(C;G) \cdot \ln\Bigl(\frac{36}{s(G)}\Bigr)}.
	\end{equation}
\end{theorem}
A few remarks:
\begin{itemize}
	\item Theorem~\ref{thm:normalized_cut_ppr} is similar to Theorem 6 of \citet{andersen2006}, but crucially the above bound depends on $\log\bigl(1/s(G)\bigr)$ rather than $\log m$. In the case where $d_{\min}(G)^2 \asymp \vol(G)$ and thus $s(G) \asymp 1$, this amounts to replacing a factor of $O(\log m)$ by a factor of ${O}(1)$, and therefore allows us to obtain meaningful results in the limit as $m \to \infty$. 
	\item For simplicity, we have chosen to state Theorem~\ref{thm:normalized_cut_ppr} with respect to a specific choice of $\alpha = 36 \cdot \Phi(C;G)$, but if $\alpha \approx 36 \cdot \Phi(C;G)$ then the Theorem will still hold up to constant factors.
\end{itemize}

It follows from Markov's inequality (see Theorem~4 of \cite{andersen2006}) that there exists a set $C^g \subseteq C$ of volume $\vol(C^g;G) \geq 5/6 \cdot \vol(C;G)$ such that for any $v \in C^g$,
\begin{equation}
\label{eqn:ppr_leakage}
p_v(C) \geq 1 - \frac{6\Phi(C;G)}{\alpha}.
\end{equation}
The claim of Theorem~\ref{thm:normalized_cut_ppr} is a consequence of~\eqref{eqn:ppr_leakage} along with Theorem~\ref{thm:mixing_time_PPR}, as we now demonstrate.
\begin{proof}[Proof (of Theorem~\ref{thm:normalized_cut_ppr})]
	From~\eqref{eqn:ppr_leakage}, the upper bound in~\eqref{eqn:normalized_cut_ppr_vol}, and the choice of $\alpha = 36 \cdot \Phi(C;G)$, 
	\begin{equation}
	\label{pf:normalized_cut_ppr}
	p_v(C) - \pi(C) \geq \frac{1}{3} - \frac{6 \Phi(C;G)}{\alpha} = \frac{1}{6}.
	\end{equation}
	Now, put 
	\begin{equation*}
	t_{\ast} = \frac{1}{648 \Phi(C;G)},~~\phi_{\ast}^2 = \frac{8}{t_{\ast}} \cdot \ln\Bigl(\frac{36}{s(G)}\Bigr),
	\end{equation*}
	and note that by~\eqref{eqn:normalized_cut_ppr_ncut} $\phi_{\ast}^2 \in [0,1]$. It therefore follows from~\eqref{pf:normalized_cut_ppr} and Theorem~\ref{thm:mixing_time_PPR} that either
	\begin{equation}
	\label{pf:normalized_cut_ppr_2}
	\frac{1}{6} \leq p_v(C) - \pi(C) \leq \frac{1}{18} + 72 \Phi(C;G) + \frac{d_{\max}(G)}{d_{\min}(G)^2} + \frac{1}{2s(G)} \cdot \left(1 - \frac{\phi_{\ast}^2}{8}\right)^{t_{\ast}},
	\end{equation}
	or $\min_{\beta \in (0,1)} \Phi(S_{\beta,v};G) \leq \phi_{\ast}^2$. But by~\eqref{eqn:normalized_cut_ppr_ncut}
	\begin{equation*}
	72 \Phi(C;G) + \frac{d_{\max}(G)}{d_{\min}(G)^2} < \frac{1}{18},
	\end{equation*}
	and we have chosen $\phi_{\ast}$ precisely so that
	\begin{equation*}
	\frac{1}{2s(G)} \cdot \left(1 - \frac{\phi_{\ast}^2}{8}\right)^{t_{\ast}} \leq \frac{1}{2s(G)} \exp\Bigl(-\frac{\phi_{\ast}^2 t_{\ast}}{8}\Bigr) \leq \frac{1}{18}.
	\end{equation*}
	Thus the inequality~\eqref{pf:normalized_cut_ppr_2} cannot hold, and so it must be that $\min_{\beta \in (0,1)} \Phi(S_{\beta,v};G) \leq \phi_{\ast}^2$. This is exactly the claim of the theorem.
\end{proof}

\section{Sample-to-population bounds}
\label{sec:sample_to_population}
In this section, we prove Propositions~\ref{prop:sample_to_population_1} and~\ref{prop:sample_to_population_2}, by establishing high-probability finite-sample bounds on various functionals of the random graph $G_{n,r}$: cut, volume, and normalized cut~(\ref{subsec:sample_to_population_ncut}), minimum and maximum degree, and local spread~(\ref{subsec:sample_to_population_local_spread}), and conductance~(\ref{subsec:sample_to_population_conductance}). To establish these results, we will use several different concentration inequalities, and we begin by reviewing these in~(\ref{subsec:concentration}). Throughout, we denote the empirical probability of a set $\mc{S} \subseteq \Rd$ as $\Pbb_n(\mc{S}) = \sum_{i = 1}^{n} \1\{x_i \in \mc{S}\}/n$, and the conditional (on being in $\mc{C}$) empirical probability as $\wt{\Pbb}_n = \sum_{i = 1}^{n} \1\{x_i \in (\mc{S} \cap \mc{C})\}/\wt{n}$, where $\wt{n} = |\mc{C}[X]|$ is the number of sample points that are in $\mc{C}$. For a probability measure $\Qbb$, we also write
\begin{equation}
\label{eqn:population_minmax_degree}
d_{\min}(\Qbb) := \inf_{x \in \mathrm{supp}(\mbb{Q})} \deg_{\Pbb,r}(x),~~\textrm{and}~~d_{\max}(\Qbb) := \sup_{x \in \mathrm{supp}(\mbb{Q})} \deg_{\Pbb,r}(x).
\end{equation}

\subsection{Review: concentration inequalities}
\label{subsec:concentration}
We use Hoeffding's inequality to control the deviations of the empirical measure of $\mc{S}$.
\begin{lemma}[Hoeffding's Inequality.]
	\label{lem:hoeffding_2}
	Fix $\delta \in (0,1)$. For any measurable $\mc{S} \subseteq \Rd$,
	\begin{equation*}
	(1 - \delta) \Pbb(\mathcal{S}) \leq \Pbb_n(\mathcal{S}) \leq (1 + \delta)\Pbb(\mathcal{S})
	\end{equation*}
	with probability at least $1 - 2\exp\set{-2\delta^2(\Pbb(\mathcal{S}))^2n}$. 
\end{lemma}

Many graph functionals are order-2 U-statistics, and we use Hoeffding's inequality to control the deviations of these functionals from their expectations. Recall that $U_n$ is an order-2 U-statistic with kernel $\varphi: \Rd \times \Rd \to \Reals$ if 
\begin{equation*}
U_n = \frac{1}{n(n-1)}\sum_{i = 1}^{n} \sum_{j \neq i} \varphi(x_i,x_j).
\end{equation*}
We write $\norm{\varphi}_{\infty} = \sup_{x,y} |\varphi(x,y)|$. 
\begin{lemma}[Hoeffding's Inequality for Order-2 U-statistics.]
	\label{lem:hoeffding}
	Fix $\delta \in (0,1)$. Assume $\norm{\varphi}_{\infty} \leq 1$. Then,
	\begin{equation*}
	(1 - \delta) \mathbb{E}[U_n] \leq U_n \leq (1 + \delta) \mathbb{E}[U_n]
	\end{equation*}
	with probability at least $1 - 2 \exp\left(-\delta^2 (\mathbb{E}[U_n])^2 n\right)$.
\end{lemma}

Finally, we use Lemma~\ref{lem:bernstein_union}---a combination of Bernstein's inequality and a union bound---to upper and lower bound $d_{\max}(G_{n,r})$ and $d_{\min}(G_{n,r})$. For measurable sets $\mc{S}_1,\ldots,\mc{S}_M$, we denote $p_{\min} := \min_{m = 1,\ldots,M} \Pbb(\mathcal{A}_m)$, and likewise let $p_{\max} := \max_{m = 1,\ldots,M} \Pbb(\mathcal{A}_m)$
\begin{lemma}[Bernstein's inequality + union bound.]
	\label{lem:bernstein_union}
	Fix $\delta \in (0,1)$. For any measurable $\mathcal{S}_1,\ldots,\mathcal{S}_M \subseteq \Rd$, 
	\begin{equation*}
	(1 - \delta) p_{\min} \leq \min_{m = 1,\ldots,M} \Pbb_n(\mathcal{A}_m) \leq \max_{m = 1,\ldots,M}  \Pbb_n(\mathcal{A}_m) \leq (1 + \delta) p_{\max}
	\end{equation*}
	with probability at least $1 - 2 M \exp\left\{-\frac{\frac{1}{3}\delta^2p_{\min}n}{1 + \frac{\delta}{3}}\right\}$.  
\end{lemma}

\subsection{Sample-to-population: normalized cut}
\label{subsec:sample_to_population_ncut}
In this subsection we establish~\eqref{eqn:sample_to_population_normalized_cut}. For a set $\mc{S} \subseteq \Rd$, both $\mathrm{cut}_{n,r}(\mc{S}[X])$ and $\mathrm{vol}_{n,r}(\mc{S}[X])$ are order-$2$ U-statistics:
\begin{align*}
\mathrm{cut}_{n,r}(\mc{S}[X]) & = \sum_{i = 1}^{n} \sum_{j \neq i} \1\{\|x_i - x_j\| \leq r\} \cdot \1\{x_i \in \mc{S}\} \cdot \1\{x_j \not\in \mc{S}\},
\intertext{and}
\mathrm{vol}_{n,r}(\mc{S}[X]) & = \sum_{i = 1}^{n} \sum_{j \neq i} \1\{\|x_i - x_j\| \leq r\} \cdot \1\{x_i \in \mc{S}\}.
\end{align*}
Therefore with probability at least $1 - \exp\{-\delta^2 \cut_{\Pbb,r}(\mc{S})\}$,
\begin{equation*}
\frac{1}{n(n - 1)} \mathrm{cut}_{n,r}(\mc{S}[X]) \leq (1 + \delta) \cut_{\Pbb,r}(\mc{S}),
\end{equation*}
and likewise with probability at least $1 - \exp\{-\delta^2 \vol_{\Pbb,r}(\mc{S})\}$,
\begin{equation*}
(1 - \delta) \vol_{\Pbb,r}(\mc{S}) \leq \frac{1}{n(n - 1)} \vol_{n,r}(\mc{S}[X]).
\end{equation*}
Consequently, for any $\delta \in (0,1/3)$,  
\begin{equation*}
\Phi_{n,r}(\mc{C}[X]) \leq \frac{1 + \delta}{1 - \delta} \frac{\cut_{\Pbb,r}(\mc{C})}{\min\{\vol_{\Pbb,r}(\mc{C}), \vol_{\Pbb,r}(\mc{C}^c)\}} = \frac{1 + \delta}{1 - \delta} \Phi_{\Pbb,r}(\mc{C}) \leq (1 + 3\delta) \cdot \Phi_{\Pbb,r}(\mc{C})  
\end{equation*}
with probability at least $1 - 3\exp\{-\delta^2 \cut_{\Pbb,r}(\mc{C})\}$. This establishes~\eqref{eqn:sample_to_population_normalized_cut} upon taking $b_1 = \cut_{\Pbb,r}(\mc{C})$ and $C_1 = 3$.


\subsection{Sample-to-population: local spread}
\label{subsec:sample_to_population_local_spread}
In this subsection we establish~\eqref{eqn:sample_to_population_local_spread}. To ease the notational burden, let $\wt{G}_{n,r} := G_{n,r}\bigl[\mc{C}[X]\bigr]$. By Lemma~\ref{lem:hoeffding}, with probability at least $1 - \exp\{-2\delta^2 \Pbb(\mc{C})^2n\}$
\begin{equation}
\label{pf:sample_to_population_local_spread_1}
\wt{n} \geq (1 - \delta) \cdot n \cdot \Pbb(\mc{C}).
\end{equation}
Conditional on~\eqref{pf:sample_to_population_local_spread_1}, it follows from Lemma~\ref{lem:bernstein_union} that with probability at least $1 -n\exp\{-2(1 - \delta)\delta^2d_{\min}(\wt{\Pbb})^2 \Pbb(\mc{C}) n\}$,
\begin{equation}
\label{pf:sample_to_population_local_spread_2}
\frac{1}{\wt{n}} d_{\min}(\wt{G}_{n,r}) \geq (1 - \delta) \cdot d_{\min}(\wt{\Pbb}),
\end{equation}
and likewise it follows from Lemma~\ref{lem:hoeffding_2} that with probability at least $1 - \exp\{-2\delta^2(1 - \delta) \vol_{\wt{\Pbb},r}(\mc{C})^2 \Pbb(\mc{C})n \}$,
\begin{equation*}
\frac{1}{\wt{n}(\wt{n} - 1)} \vol(\wt{G}_{n,r}) \geq (1 - \delta) \cdot \vol_{\wt{\Pbb},r}(\mc{C}).
\end{equation*}
Consequently, for any $\delta \in (0,1/3)$,
\begin{equation*}
s_{n,r}(\mc{C}[X]) = \frac{d_{\min}(\wt{G}_{n,r})^2}{\vol(\wt{G}_{n,r})} \geq \frac{\frac{1}{\wt{n}^2}d_{\min}(\wt{G}_{n,r})^2}{\frac{1}{\wt{n}(\wt{n} - 1)}\vol(\wt{G}_{n,r})} \geq \frac{(1 - \delta)^2}{(1 + \delta)} \frac{d_{\min}(\wt{\Pbb})^2}{\vol_{\wt{\Pbb},r}(\mc{C})} \geq (1 - 3\delta) \cdot s_{\Pbb,r}(\mc{C}).
\end{equation*}
with probability at least $1 - 2\exp\bigl\{-\delta^2\bigl(\vol_{\wt{\Pbb},r}(\mc{C})\bigr)^2 \Pbb(\mc{C})n\bigr\} - n\exp\bigl\{-\delta^2\bigl(d_{\min}(\wt{\Pbb})\bigr)^2 \Pbb(\mc{C}) n\bigr\}$. This establishes~\eqref{eqn:sample_to_population_local_spread} upon taking $b_2 = \bigl(\vol_{\wt{\Pbb},r}(\mc{C})\bigr)^2 \Pbb(\mc{C})$, $b_3 = \bigl(d_{\min}(\wt{\Pbb})\bigr)^2 \Pbb(\mc{C})$, and $C_2 = 3$. 

\subsection{Sample-to-population: conductance}
\label{subsec:sample_to_population_conductance}

In this subsection we establish~\eqref{eqn:sample_to_population_conductance}. As mentioned in our main text, the proof of~\eqref{eqn:sample_to_population_conductance} relies on a high-probability upper bound of the $\infty$-transportation distance between $\mbb{P}$ and $\mbb{P}_n$, from~\citep{garciatrillos16b}. We begin by reviewing this upper bound, in Theorem~\ref{thm:garciatrillos16}. Subsequently in Proposition~\ref{prop:conductance_lb_transportation_distance}, we relate the $\infty$-transportation distance between two measures $\mbb{Q}_1$ and $\mbb{Q}_2$ to the difference of their conductances. Together these results will imply~\eqref{eqn:sample_to_population_conductance}.

\paragraph{Review: $\infty$-transportation distance and transportation maps.}
We give a brief review of some of the main ideas regarding $\infty$-transportation distance, and transportation maps. This discussion is largely taken from~\citep{garciatrillos16b,garciatrillos16}, and the reader should consult these works for more detail. 

For two measures $\mbb{Q}_1$ and $\mbb{Q}_2$ on a domain $D$, the \emph{$\infty$-transportation distance} $\Delta_{\infty}(\mbb{Q}_1,\mbb{Q}_2)$ is
\begin{equation*}
\Delta_{\infty}(\mbb{Q}_1,\mbb{Q}_2) := \inf_{\gamma}\Bigl\{\mathrm{esssup}_{\gamma} \bigl\{|x - y|: (x,y) \in D \times D\bigr\}: \gamma \in \Gamma(\mbb{Q}_1,\mbb{Q}_2) \Bigr\}
\end{equation*}
where $\Gamma(\mbb{Q}_1,\mbb{Q}_2)$ is the set of all couplings of $\mbb{Q}_1$ and $\mbb{Q}_2$, that is the set of all probability measures on $D \times D$ for which the marginal distribution in the first variable is $\mbb{Q}_1$, and the marginal distribution in the second variable is $\mbb{Q}_2$. 

Suppose $\mbb{Q}_1$ is absolutely continuous with respect to the Lebesgue measure. Then $\Delta_{\infty}(\mbb{Q}_1,\mbb{Q}_2)$ can be more simply defined in terms of push-forward measures and transportation maps. For a Borel map $T: D \to D$, the \emph{push-forward} of $\mbb{Q}_1$ by $T$ is $T_{\sharp}\mbb{Q}_1$, defined for Borel sets $U$ as 
\begin{equation*}
T_{\sharp}\mbb{Q}_1(U) = \mbb{Q}_1(T^{-1}(U)).
\end{equation*}
A \emph{transportation map} from $\mbb{Q}_1$ to $\mbb{Q}_2$ is a Borel map $T$ for which $T_{\sharp}\mbb{Q}_1 = \mbb{Q}_2$. Transportation maps satisfy two important properties. First, the transportation distance can be formulated in terms of transportation maps:
\begin{equation*}
\Delta_{\infty}(\mbb{Q}_1,\mbb{Q}_2) = \inf_{T} \|\mathrm{Id} - T\|_{\Leb^{\infty}(\mbb{Q}_1)}
\end{equation*}
where $\mathrm{Id}: D \to D$ is the identity mapping, and the infimum is over transportation maps $T$ from $\mbb{Q}_1$ to $\mbb{Q}_2$. Second, they result in the following change of variables formula; if $T_{\sharp}\mbb{Q}_1 = \mbb{Q}_2$, then for any $g \in L^1(\mbb{Q}_2)$,
\begin{equation}
\label{eqn:transportation_map_change_of_variable}
\int g(y) \,d\mbb{Q}_2(y) = \int g(T(x)) \,d\mbb{Q}_1(x).
\end{equation} 

\paragraph{$\infty$-transportation distance between empirical and population measures.}
Under mild regularity conditions, \cite{garciatrillos16b} upper bound the transportation distance $\Delta_{\infty}(\mbb{P},\mbb{P}_n)$. 
\begin{enumerate}[label=(A\arabic*)]
	\item 
	\label{asmp:domain} 
	The distribution $\Pbb$ is defined on a domain $D \subseteq \Rd$, which is a bounded, connected, open set with Lipschitz boundary. 
	\item 
	\label{asmp:bounded_density} 
	The distribution $\Pbb$ has density $g: D \to (0,\infty)$ such that there exist $g_{\min} \leq 1 \leq g_{\max}$ for which
	\begin{equation*}
	(\forall x \in D)~~ g_{\min} \leq g(x) \leq g_{\max}
	\end{equation*}
\end{enumerate}
\begin{theorem}[Theorem~1.1 of \cite{garciatrillos16}]
	\label{thm:garciatrillos16}
	Suppose $\Pbb$ satisfies~\ref{asmp:domain} and~\ref{asmp:bounded_density}. Then, there exists positive constants $B_{2}$ and $B_3$ that do not depend on $n$, such that with probability at least $1 - B_2/n$:
	\begin{equation*}
	\Delta_{\infty}(\mbb{P},\mbb{P}_n) \leq B_{3} \cdot 
	\begin{dcases*}
	\frac{\ln(n)^{3/4}}{n^{1/2}},&~~\textrm{if $d = 2$,} \\
	\frac{\ln(n)^{1/d}}{n^{1/d}},&~~\textrm{if $d \geq 3$.}
	\end{dcases*}
	\end{equation*}
\end{theorem}
Assuming the candidate cluster $\mc{C}$ and conditional distribution $\wt{\Pbb}$ satisfy~\ref{asmp:domain} and~\ref{asmp:bounded_density}, then Theorem~\ref{thm:garciatrillos16} applies to $\Delta_{\infty}(\wt{\Pbb},\wt{\Pbb}_n)$; we will use this upper bound on $\Delta_{\infty}(\wt{\Pbb},\wt{\Pbb}_n)$ to show~\eqref{eqn:sample_to_population_conductance}.  

\paragraph{Lower bound on conductance using transportation maps.}
Let $\mbb{Q}_1$ and $\mbb{Q}_2$ be probability measures, with $\mbb{Q}_1$ absolutely continuous with respect to Lebesgue measure, and let $T$ be a transportation map from $\mbb{Q}_1$ to $\mbb{Q}_2$. We write $\Delta_T(\mbb{Q}_1,\mbb{Q}_2) := \|\mathrm{Id} - T\|_{\Leb^{\infty}(\mbb{Q}_1)}$. To facilitate easy comparison between the conductances of two arbitrary distributions, let $\Psi_r(\mbb{Q}) := \Psi_{\mbb{Q},r}(\mathrm{supp}(\mbb{Q}))$ for a distribution $\mbb{Q}$. In the following Proposition, we lower bound $\Psi_r(\mbb{Q}_2)$ by $\Psi_r(\mbb{Q}_1)$, plus an error term that depends on $\Delta(\mbb{Q}_1,\mbb{Q}_2)$.
\begin{proposition}
	\label{prop:conductance_lb_transportation_distance}
	Let $\mbb{Q}_1$ be a probability measure that admits a density $g$ with respect to $\nu(\cdot)$, let $\mbb{Q}_2$ be an arbitrary probability measure, and let $T$ be a transportation map from $\mbb{Q}_1$ to $\mbb{Q}_2$. Suppose $\Delta_T(\mbb{Q}_1,\mbb{Q}_2) \leq r/(4(d - 1))$. It follows that
	\begin{equation}
	\label{eqn:conductance_lb_transportation_distance}
	\Psi_r(\mbb{Q}_2) \geq \Psi_r(\mbb{Q}_1) \cdot \biggl(1 - \frac{2b_4\Delta_T(\mbb{Q}_1,\mbb{Q}_2)}{\bigl(1 - \Psi_r(\mbb{Q}_1)\bigr) \cdot \bigl(d_{\min}(\mbb{Q}_2)\bigr)^2}\biggr) - \frac{b_4 \Delta_T(\mbb{Q}_1,\mbb{Q}_2)}{\bigl(1 - \Psi_r(\mbb{Q}_1)\bigr) \cdot \bigl(d_{\min}(\mbb{Q}_2)\bigr)^2},
	\end{equation}
	where $b_4 := 2d \nu_d r^{d - 1} \cdot \max_{x \in \Rd}\{g(x)\}$ is a positive constant that does not depend on $\mbb{Q}_2$. 
\end{proposition}
We note that the lower bound can also be stated with respect to the $\infty$-optimal transport distance $\Delta_{\infty}(\mbb{Q}_1,\mbb{Q}_2)$. 
\begin{proof}[Proof (of Proposition~\ref{prop:conductance_lb_transportation_distance})]
	Throughout this proof, we will write $\Delta_{12} = \Delta_T(\mbb{Q}_1,\mbb{Q}_2)$, and $\wb{\vol}_{\mbb{Q},r}(\mc{R}) = \min\bigl\{\vol_{\mbb{Q},r}\bigl(\mc{R}\bigr),\vol_{\mbb{Q},r}\bigl(\mc{R}^c\bigr)\bigr\}$ for conciseness. Naturally, the proof of Proposition~\ref{prop:conductance_lb_transportation_distance} involves using the transportation map $T$ to relate $\cut_{\mbb{Q}_2,r}(\cdot)$ to $\cut_{\mbb{Q}_1,r}(\cdot)$, and likewise $\vol_{\mbb{Q}_2,r}(\cdot)$ to $\vol_{\mbb{Q}_1,r}(\cdot)$. Define the remainder term $R_{\epsilon,\mbb{Q}_1}^{(\Delta)}(x) = \int \1\{\epsilon \leq \|x - y\| \leq \epsilon + \Delta\} \,d\mbb{Q}_1(y)$ for any $\epsilon, \Delta > 0$. Then for any set $\mc{S} \subseteq \mathrm{supp}(\mbb{Q}_2)$, we have that
	\begin{align}
	\cut_{\mbb{Q}_2,r}(\mc{S}) & = \iint \1\{\|x - y\|\leq r\} \cdot \1\{x \in \mc{S} \} \cdot \1\{y \in \mc{S}^c \} \,d\mbb{Q}_2(y) \,d\mbb{Q}_2(x) \nonumber \\ 
	& \overset{\mathrm{(i)}}{=} \iint \1\{\|T(x) - T(y)\|\leq r\} \cdot \1\{x \in T^{-1}(\mc{S}) \} \cdot \1\{y \in T^{-1}(\mc{S})^c \} \,d\mbb{Q}_1(y) \,d\mbb{Q}_1(x) \nonumber \\
	& \overset{\mathrm{(ii)}}{\geq} \iint \1\{\|x - y\|\leq r - 2\Delta_{12}\} \cdot \1\{x \in T^{-1}(\mc{S}) \} \cdot \1\{y \in T^{-1}(\mc{S}^c) \} \,d\mbb{Q}_1(y) \,d\mbb{Q}_1(x) \nonumber \\
	& = \cut_{\mbb{Q}_1,r}\bigl(T^{-1}(\mc{S})\bigr) - \int R_{r - 2\Delta_{12} ,\mbb{Q}_1}^{(2\Delta_{12})}(x) \,d\mbb{Q}_1(x) \label{pf:conductance_lb_transportation_distance_1}
	\end{align} 
	where $\mathrm{(i)}$ follows from the change of variables formula~\eqref{eqn:transportation_map_change_of_variable}, and $\mathrm{(ii)}$ follows from the triangle inequality. Similar reasoning implies that
	\begin{equation}
	\label{pf:conductance_lb_transportation_distance_2}
	\vol_{\mbb{Q}_2,r}(\mc{S}) \leq \vol_{\mbb{Q}_1,r}\bigl(T^{-1}(\mc{S})\bigr) + \int R_{r,\mbb{Q}_1}^{(2\Delta_{12})}(x)  \,d\mbb{Q}_1(x).
	\end{equation}
	For any $x \in \Rd$, the remainder terms can be upper bounded: since $\Delta_{12} \geq 0$,
	\begin{equation*}
	R_{r - 2\Delta_{12} ,\mbb{Q}_1}^{(2\Delta_{12})}(x) \leq \nu_dr^d\Bigl\{1 - \Bigl(1 - \frac{2\Delta_{12}}{r}\Bigr)^d \Bigr\} \cdot \max_{x \in \Rd}\{g(x)\} \leq \underbrace{2 d \nu_d r^{d - 1} \cdot \max_{x \in \Rd}\{g(x)\}}_{= b_4} \cdot \Delta_{12},
	\end{equation*}
	if $0 \leq \Delta_{12} \leq r/(4(d - 1))$,
	\begin{equation*}
	R_{r,\mbb{Q}_1}^{(2\Delta_{12})}(x) \leq \nu_dr^d\Bigl\{\Bigl(1 + \frac{2\Delta_{12}}{r}\Bigr)^d - 1\Bigr\} \cdot \max_{x \in \Rd}\{g(x)\} \leq 2 b_4 \cdot \Delta_{12}.
	\end{equation*} 
	Plugging these bounds on the remainder terms back into~\eqref{pf:conductance_lb_transportation_distance_1} and~\eqref{pf:conductance_lb_transportation_distance_2} respectively, we see that
	\begin{align*}
	\Phi_{\mbb{Q}_2,r}(\mc{S}) & \geq \frac{\cut_{\mbb{Q}_1,r}\bigl(T^{-1}(\mc{S})\bigr) - b_4 \Delta_{12}}{\wb{\vol}_{\mbb{Q}_1,r}(T^{-1}(\mc{S})) + 2b_4 \Delta_{12}} \\ & = \Phi_{\mbb{Q}_1,r}(T^{-1}(\mc{S})) \cdot \biggl(\frac{\wb{\vol}_{\mbb{Q}_1,r}(T^{-1}(\mc{S}))}{\wb{\vol}_{\mbb{Q}_1,r}(T^{-1}(\mc{S})) + 2b_4\Delta_{12}}\biggr) - \frac{b_4\Delta_{12}}{\wb{\vol}_{\mbb{Q}_1,r}(T^{-1}(\mc{S})) + 2b_4\Delta_{12}} \\
	& \overset{\eqref{pf:conductance_lb_transportation_distance_2}}{\geq} \Phi_{\mbb{Q}_1,r}(T^{-1}(\mc{S})) \cdot \biggl(\frac{\wb{\vol}_{\mbb{Q}_2,r}(\mc{S}) - 2b_4\Delta_{12}}{\wb{\vol}_{\mbb{Q}_2,r}(\mc{S})}\biggr) - \frac{b_4\Delta_{12}}{\wb{\vol}_{\mbb{Q}_2,r}(\mc{S})}.
	\end{align*}
	We would like to conclude by taking an infimum over $\mc{S}$ on both sides, but in order to ensure that the remainder term is small we must specially handle the case where $\wb{\vol}_{\mbb{Q}_2,r}(\mc{S})$ is small. Let
	\begin{equation*}
	\mathfrak{L}_{r}(\mbb{Q}_1,\mbb{Q}_2) = \bigl\{\mc{S} \subseteq \mathrm{supp}(\mbb{Q}_2): \wb{\vol}_{\mbb{Q}_2,r}(\mc{S}) \geq (1 - \Psi_r(\mbb{Q}_1)) \cdot d_{\min}(\mbb{Q}_2)^2 \bigr\}.
	\end{equation*}
	On the one hand, taking an infimum over all sets $\mc{S} \in \mathfrak{L}_{r}(\mbb{Q}_1,\mbb{Q}_2)$, we have that
	\begin{equation*}
	\inf_{\mc{S}: \mc{S} \in \mathfrak{L}_{r}(\mbb{Q}_1,\mbb{Q}_2)} \Phi_{\mbb{Q}_2,r}(\mc{S}) \geq \Psi_{r}(\mbb{Q}_1) \cdot \biggl(1 - \frac{2 b_4 \Delta_{12}}{(1 - \Psi_r(\mbb{Q}_1)) \cdot d_{\min}(\mbb{Q}_2)^2}\biggr) - \frac{b_4\Delta_{12}}{(1 - \Psi_r(\mbb{Q}_1)) \cdot d_{\min}(\mbb{Q}_2)^2}
	\end{equation*}
	On the other hand, we claim that 
	\begin{equation}
	\label{pf:conductance_lb_transportation_distance_3}
	\Phi_{r,\mbb{Q}_2}(\mc{R}) \geq \Psi_{r}(\mbb{Q}_1),~~\textrm{for any $\mc{R} \not\in \mathfrak{L}(\mbb{Q}_1,\mbb{Q}_2)$}.
	\end{equation}
	To derive~\eqref{pf:conductance_lb_transportation_distance_3}, suppose that $\mc{R} \subseteq \mathrm{supp}(\mbb{Q}_2)$ and $\mc{R} \not\in \mathfrak{L}(\mbb{Q}_1,\mbb{Q}_2)$. Without loss of generality, we shall assume that $\vol_{\mbb{Q}_2,r}(\mc{R}) \leq (1 - \Psi_r(\mbb{Q}_1)) \cdot d_{\min}(\mbb{Q}_2)^2$ (otherwise we can work with respect to $\mc{R}^c$.)  Then, for all $x \in \mc{R}$,
	\begin{equation*}
	\int \1\{\|x - y\| \leq r\}\cdot \1\{y \in \mc{R}^c\} \,d\mbb{Q}_2(y) \geq \deg_{\Qbb_2,r}(x) -  \mbb{Q}_2(\mc{R}) \geq \deg_{\Qbb_2,r}(x) - \frac{\vol_{\mbb{Q}_2,r}(\mc{R})}{d_{\min}(\mbb{Q}_2)} \geq d_{\min}(\mbb{Q}_2) \cdot \Psi_{r}(\mbb{Q}_2),
	\end{equation*}
	whence integrating over all $x \in \mc{R}$ and dividing by~$\vol_{\mbb{Q}_2,r}(\mc{R})$ yields~\eqref{pf:conductance_lb_transportation_distance_3}. This completes the proof of Proposition~\ref{prop:conductance_lb_transportation_distance}. 
\end{proof}

\paragraph{Putting the pieces together.} 
First, we note that
\begin{equation*}
\Psi_r(\wt{\mbb{P}}) = \Psi_{\wt{\mbb{P}},r}(\mathrm{supp}(\wt{\mbb{P}})) = \Psi_{\Pbb,r}(\mc{C}), ~~\textrm{and}~~\Psi_{r}(\wt{\mbb{P}}_n) = \Psi_{\wt{\mbb{P}}_n, r}(\mathrm{supp}(\wt{\mbb{P}}_n)) = \Psi_{n,r}(\mc{C}[X]),
\end{equation*}
so that we may apply Proposition~\ref{prop:conductance_lb_transportation_distance} to get a lower bound on $\Psi_{n,r}(\mc{C}[X])$ in terms of $\Psi_{\Pbb,r}(\mc{C})$, $\Delta_{\infty}(\wt{\Pbb}, \wt{\Pbb}_n)$, and $d_{\min}(\wt{\Pbb}_{n})$. Next, as we have already derived in~\eqref{pf:sample_to_population_local_spread_2}, with probability at least $1 - \exp\{-\mbb{P}(\mc{C})^2n\} - \exp\{-d_{\min}(\wt{\Pbb})^2 \Pbb(\mc{C}) n/\sqrt{2}\}$,
\begin{equation*}
d_{\min}(\wt{\Pbb}_n) = \frac{1}{\wt{n}} d_{\min}(\wt{G}_{n,r}) \geq \frac{1}{\sqrt{2}} d_{\min}(\wt{\Pbb}).
\end{equation*}
Finally, taking
\begin{equation*}
b_5 := \frac{1}{2b_4}\Psi_r(\wt{\Pbb}) \cdot (1 - \Psi_r(\wt{\Pbb})) \cdot d_{\min}(\wt{\Pbb})^2,~~\textrm{and}~~ B_1 := \frac{1}{B_4}\min\Bigl\{\frac{b_5}{2\Psi_r(\wt{\Pbb})}, b_5, \frac{r}{4(d - 1)} \Bigr\} 
\end{equation*}
by Theorem~\ref{thm:garciatrillos16} and~\eqref{eqn:sample_to_population_conductance_sample_complexity} we have that
\begin{equation*}
\Delta_{\infty}(\wt{\Pbb},\wt{\Pbb}_n) \leq B_3 \frac{\log(n)^{p_d}}{n^{1/d}} \leq \min\Bigl\{\frac{b_5}{2\Psi_r(\wt{\Pbb})}, b_5, \frac{r}{4(d - 1)} \Bigr\} \cdot \delta
\end{equation*}
with probability at least $1 - B_2/n$. Therefore by Proposition~\ref{prop:conductance_lb_transportation_distance},
\begin{equation*}
\Psi_r(\wt{\Pbb}_n) \geq \Psi_r(\wt{\Pbb}) \cdot \biggl(1 - \frac{2b_4\Delta_{\infty}(\wt{\Pbb}_n,\wt{\Pbb})}{\bigl(1 - \Psi_r(\wt{\Pbb})\bigr) \cdot \bigl(d_{\min}(\wt{\Pbb}_n)\bigr)^2}\biggr) - \frac{b_4 \Delta_{\infty}(\wt{\Pbb}_n,\wt{\Pbb}_N)}{\bigl(1 - \Psi_r(\wt{\Pbb})\bigr) \cdot \bigl(d_{\min}(\wt{\Pbb}_n)\bigr)^2} \geq \Psi_r(\wt{\Pbb}) (1 - 2\delta)
\end{equation*}
with probability at least $1 - B_2/n - (n + 2)\exp\{-d_{\min}(\wt{\Pbb})^2 \Pbb(\mc{C}) n/\sqrt{2}\}$, establishing~\eqref{eqn:sample_to_population_conductance}.

\section{Population functionals for density clusters}
\label{sec:density_cluster_population_functionals}
In this section, we prove Lemma~\ref{lem:density_cluster_local_spread} (in Section~\ref{subsec:density_cluster_local_spread}), Proposition~\ref{prop:density_cluster_normalized_cut} (in Section~\ref{subsec:density_cluster_ncut}), and Proposition~\ref{prop:density_cluster_conductance} (in Section~\ref{subsec:density_cluster_conductance}), by establishing bounds on the population-level local spread, normalized cut, and conductance of a thickened density cluster $\mc{C}_{\lambda,\sigma}$. In these proofs, we make use of some estimates on the volume of spherical caps, and we detail these in Section~\ref{subsec:spherical caps}; we also use some isoperimetric inequalities, which we detail in Section~\ref{subsec:isoperimetric_inequalities}. Throughout, we write $\nu_d := \nu(B(0,1))$ for the Lebesgue measure of a $d$-dimensional unit ball.

\subsection{Balls, Spherical Caps, and Associated Estimates}
\label{subsec:spherical caps}
In this section, we derive lower bounds on the volume of the intersection between two balls in $\Rd$, and the volume of a spherical cap. Results of this type are well-known, but since we could not find exactly the statements we desire, for completeness we also supply proofs. We use the notation $B(x,r)$ for a ball of radius $r$ centered at $x \in \Rd$, and $\mathrm{cap}_{r}(h)$ for a spherical cap of height $r$ and radius $r$. Recall that the Lebesgue measure of a spherical cap is
\begin{equation*}
\nu\bigl(\mathrm{cap}_r(h)\bigr) = \frac{1}{2} \nu_d r^d I_{1 - a}\left(\frac{d + 1}{2}; \frac{1}{2}\right)
\end{equation*}
where $a = (r - h)^2/r^2$, and
\begin{equation*}
I_{1 - a}(z,w) = \frac{\Gamma(z + w)}{\Gamma(z) \Gamma(w)} \int_{0}^{1 - a} u^{z - 1} (1 - u)^{w - 1} du.
\end{equation*}
is the cumulative distribution function of a $\mathrm{Beta}(z,w)$ distribution, evaluated at $1 - a$. (Here $\Gamma(\cdot)$ is the gamma function).
\begin{lemma}
	\label{lem:overlap_balls}
	For any $x,y \in \Rd$ and $r > 0$, it holds that
	\begin{equation}
	\label{eqn:overlap_balls_1}
	\nu\bigl(B(x,r) \cap B(y,r)\bigr) \geq \nu_d r^d\biggl(1 - \frac{\|x - y\|}{r} \sqrt{\frac{d + 2}{2\pi}}\biggr).
	\end{equation}
	For any $x,y \in \Rd$ and $r,\sigma > 0$ such that $\|x - y\| \leq \sigma$, it holds that,
	\begin{equation}
	\label{eqn:overlap_balls_2}
	\nu\bigl(B(x,r) \cap B(y,\sigma)\bigr) \geq \frac{1}{2} \nu_d r^d\biggl(1 - \frac{r}{\sigma}\sqrt{\frac{d + 2}{2\pi}}\biggr).
	\end{equation}
\end{lemma}
\begin{lemma}
	\label{lem:volume_of_spherical_cap}
	For any $0 < h \leq r$, and $a = 1 - (2 r h - h^2)/r^2$,
	\begin{equation*}
	\nu\bigl(\mathrm{cap}_r(h)\bigr) \geq \frac{1}{2}\nu_dr^d\bigl(1 - 2\sqrt{a} \cdot \sqrt{\frac{d + 2}{2\pi}}\bigr)
	\end{equation*}
\end{lemma}
An immediate implication of~\eqref{eqn:overlap_balls_2} is that for any $x \in \mc{C}_{\lambda,\sigma}$,
\begin{equation}
\label{eqn:uniform_local_conductance}
\nu\bigl(B(x,r) \cap \mc{C}_{\lambda,\sigma}\bigr) \geq \frac{1}{2} \nu_d r^d\biggl(1 - \frac{r}{\sigma}\sqrt{\frac{d + 2}{2\pi}}\biggr).
\end{equation}  
\begin{proof}[Proof (of Lemma~\ref{lem:overlap_balls})]
	First, we prove~\eqref{eqn:overlap_balls_1}. The intersection $B(x,r) \cap B(y,r)$ consists of two symmetric spherical caps, each with height $h = r - \frac{\|x - y\|}{2}$. 
	As a result, by Lemma~\ref{lem:volume_of_spherical_cap} we have
	\begin{equation*}
	\nu\bigl(B(x,r) \cap B(y,r)\bigr) \geq \nu_d r^d \bigl(1 - 2\sqrt{a} \cdot \sqrt{\frac{d + 2}{2\pi}}\bigr)
	\end{equation*}
	where $a = \|x - y\|^2/(4r^2)$, and the claim follows.
	
	Next we prove~\eqref{eqn:overlap_balls_2}. Assume that $\|x - y\| = \sigma$, as otherwise if $0 \leq \|x - y\| < \sigma$ the volume of the overlap will only be larger. Then $B(x,r) \cap B(y,\sigma)$ contains a spherical cap of radius $r$ and height $h = r - \frac{r^2}{2\sigma}$, from Lemma~\ref{lem:volume_of_spherical_cap} we deduce
	\begin{equation*}
	\nu\bigl(B(x,r) \cap B(y,\sigma)\bigr) \geq \frac{1}{2}\nu_dr^d\biggl(1 - 2\sqrt{a}\cdot\sqrt{\frac{d + 2}{2\pi}}\biggr)
	\end{equation*}
	for $a = (r - h)^2/r^2 = r^2/(4\sigma^2)$, and the claim follows.
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:volume_of_spherical_cap})]
	For any $0 \leq a \leq 1$, we have that
	\begin{equation*}
	\int_{0}^{1 - a}u^{(d-1)/2}(1 - u)^{-1/2}du = \int_{0}^{1}u^{(d-1)/2}(1 - u)^{-1/2}du - \int_{1 - a}^{1}u^{(d-1)/2}(1 - u)^{-1/2}du. 
	\end{equation*}
	The first integral is simply
	\begin{equation*}
	\int_{0}^{1}u^{(d-1)/2}(1 - u)^{-1/2}du = \frac{\Gamma\bigl(\frac{d + 1}{2}\bigr)\Gamma\bigl(\frac{1}{2}\bigr)}{ \Gamma\bigl(\frac{d}{2}+ 1\bigr)},
	\end{equation*}
	whereas for all $u \in [0,1]$ and $d \geq 1$, the second integral can be upper bounded as follows:
	\begin{equation*}
	\int_{1 - a}^{1}u^{(d-1)/2}(1 - u)^{-1/2}du \leq \int_{1 - a}^{1}(1 - u)^{-1/2}du = \int_{0}^{a} u^{-1/2}du = 2\sqrt{a}.
	\end{equation*}
	As a result, 
	\begin{equation*}
	\nu\bigl(\mathrm{cap}_r(h)\bigr) \geq \frac{1}{2}\nu_dr^d \biggl(1 - 2\sqrt{a}\frac{\Gamma(\frac{d}{2} + 1)}{\Gamma(\frac{d + 1}{2})\Gamma(\frac{1}{2})}\biggr)  \geq \frac{1}{2}\nu_dr^d \biggl(1 - 2\sqrt{a} \cdot \sqrt{\frac{d + 2}{2\pi}}\biggr).
	\end{equation*}
\end{proof}

\subsection{\textcolor{red}{Isoperimetric inequalities}}
\label{subsec:isoperimetric_inequalities}

For any set $\mc{C} \subseteq \Rd$ and $\sigma > 0$, recall that $\mc{C}_{\sigma} := \{x: \mathrm{dist}(x,\mc{C}) \leq \sigma\}$. We begin with an upper bound on the volume of $\mc{C}_{\sigma + \delta}$ as compared to $\mc{C}_{\sigma}$. 
\begin{lemma}
	\label{lem:external_isoperimetric_inequality}
	For any bounded set $\mc{C} \subseteq \Rd$ and  $\sigma, \delta > 0$, it holds that
	\begin{equation}
	\label{eqn:external_isoperimetric_inequality}
	\nu(\mc{C}_{\sigma + \delta}) \leq \nu(\mc{C}_{\sigma}) \cdot \Bigl(1 + \frac{\delta}{\sigma}\Bigr)^d.
	\end{equation}
\end{lemma}
To prove Lemma~\ref{lem:external_isoperimetric_inequality} for a general bounded set $\mc{C}$, we first construct a particular covering of $\mc{C}_{\sigma + \delta}$, then apply~\eqref{eqn:external_isoperimetric_inequality_1} to each set in the covering.

For $\delta \leq \sigma/d$ we have that $\bigl(1 + \delta/\sigma\bigr)^d \leq 1 + d \cdot \delta/(\sigma - d\delta)$, and we deduce from~\eqref{eqn:external_isoperimetric_inequality} that
\begin{equation}
\label{eqn:external_isoperimetric_inequality_2}
\nu(\mc{C}_{\sigma + \delta} \setminus \mc{C}_{\sigma}) = \nu(\mc{C}_{\sigma + \delta}) - \nu(\mc{C}_{\sigma}) \leq d \cdot \frac{\delta}{\sigma - d\delta} \cdot \nu(\mc{C}_{\sigma}).
\end{equation}
We use~\eqref{eqn:external_isoperimetric_inequality_2} along with Assumption~\ref{asmp:low_noise_density} to derive a density-weighted isoperimetric inequality. 
\begin{lemma}
	\label{lem:external_isoperimetric_inequality_density_weighted}
	Let $\mc{C}_{\lambda,\sigma}$ satisfy Assumption~\ref{asmp:bounded_density} and~\ref{asmp:low_noise_density} for some $\theta, \gamma$ and $\lambda_{\sigma}$. Then for any $0 < r \leq \sigma/d$, it holds that
	\begin{equation}
	\label{eqn:external_isoperimetric_inequality_density_weighted}
	\Pbb\bigl(\mc{C}_{\lambda,\sigma + r} \setminus \mc{C}_{\lambda,\sigma}\bigr) \leq \Bigl(1 + \frac{dr}{\sigma - dr}\Bigr) \cdot \frac{dr}{\sigma} \cdot \left(\lambda_{\sigma} - \theta\frac{r^{\gamma}}{\gamma + 1}\right) \cdot \nu(\mc{C}_{\lambda,\sigma}).
	\end{equation}
\end{lemma}

Before we prove Lemma~\ref{lem:external_isoperimetric_inequality}, we observe that in the special case where $\mc{C}$ is a single point---say $\mc{C} = \{0\}$--- the inequality~\eqref{eqn:external_isoperimetric_inequality} holds with equality. In this case $\mc{C}_{\sigma + \delta} = B(0,\sigma + \delta)$ and $\mc{C}_{\sigma} = B(0,\sigma)$, and
\begin{equation}
\label{eqn:external_isoperimetric_inequality_1}
\nu(\mc{C}_{\sigma + \delta}) = \nu_d (\sigma + \delta)^d = \nu_d \sigma^d \Bigl(1 + \frac{\delta}{\sigma}\Bigr)^d = \nu(\mc{C}_{\sigma}) \cdot \Bigl(1 + \frac{\delta}{\sigma}\Bigr)^d.
\end{equation}
\begin{proof}[Proof (of Lemma~\ref{lem:external_isoperimetric_inequality})] 
	Fix $\delta' > 0$, and take $\epsilon = \delta + \delta'$. We will show that
	\begin{equation*}
	\label{pf:external_isoperimetric_inequality}
	\nu(\mc{C}_{\sigma + \epsilon}) \leq \nu(\mc{C}_{\sigma}) \cdot \Bigl(1 + \frac{\epsilon}{\sigma}\Bigr)^d,
	\end{equation*}
	whence taking a limit as $\delta' \to 0$ yields the claim.
	
	To show~\eqref{pf:external_isoperimetric_inequality} for general closed and bounded $\mc{C}$, we need to construct a particular disjoint covering $\mc{A}_1(\sigma + \epsilon), \ldots, \mc{A}_N(\sigma + \epsilon)$ of $\mc{C}_{\sigma + \delta}$. To do so, we first take a finite set of points $x_1,\ldots,x_N$ such that the net $B(x_1,\sigma + \epsilon),\ldots,B(x_N,\sigma + \epsilon)$ covers $\mc{C}_{\sigma + \delta}$.  Note that such a covering exists for some finite $N = N(\epsilon)$ because $\mc{C}_{\sigma + \delta}$ is bounded, and the closure of $\mc{C}_{\sigma + \delta}$ is thus a compact subset of $\cup_{x \in \mc{C}} B(x,\sigma + \epsilon)$. Defining $\mc{A}_1(s), \ldots, \mc{A}_N(s)$ for a given $s > 0$ to be
	\begin{equation*}
	\mc{A}_1(s) := B(x_1,s),~~\textrm{and}~~ \mc{A}_{j + 1}(s) := B(x_{j + 1},s) \setminus \bigcup_{i = 1}^{j} B(x_i,s) ~~\textrm{for $j = 1,\ldots,N - 1$},
	\end{equation*}
	we have that $\mc{A}_1(\sigma + \epsilon),\ldots,\mc{A}_N(\sigma + \epsilon)$ is a disjoint covering of $\mc{C}_{\sigma + \delta}$, and so $\nu(\mc{C}_{\sigma + \delta}) \leq \sum_{j = 1}^{N} \nu(\mc{A}_j(\sigma + \epsilon))$. 
	
	We claim that for all $j = 1,\ldots,N$, the function $s \mapsto \nu\bigl(\mc{A}_j(s)\bigr)/\nu\bigl(B(x_j,s)\bigr)$ is monotonically non-increasing in $s$. Once this claim is verified, it follows that
	\begin{equation*}
	\nu(\mc{A}_j(\sigma + \epsilon)) = \nu\bigl(B(x_j,\sigma + \epsilon)\bigr) \cdot \frac{\nu\bigl(\mc{A}_j(\sigma + \epsilon)\bigr)}{\nu\bigl(B(x_j,\sigma + \epsilon)\bigr)} \leq \Bigl(1 + \frac{\epsilon}{\sigma}\Bigr)^d \cdot  \nu\bigl(B(x_j,\sigma)\bigr) \cdot \frac{\nu\bigl(\mc{A}_j(\sigma)\bigr)}{\nu\bigl(B(x_j,\sigma)\bigr)} = \Bigl(1 + \frac{\epsilon}{\sigma}\Bigr)^d \cdot \nu\bigl(\mc{A}_j(\sigma)
	\end{equation*}
	and summing over $j$, we see that
	\begin{equation*}
	\nu(\mc{C}_{\sigma + \delta}) \leq \sum_{j = 1}^{N} \nu(\mc{A}_j(\sigma + \epsilon)) \leq \Bigl(1 + \frac{\epsilon}{\sigma}\Bigr)^d \sum_{j = 1}^{N} \nu\bigl(\mc{A}_j(\sigma) \leq \Bigl(1 + \frac{\epsilon}{\sigma}\Bigr)^d \mc{C}_{\sigma}
	\end{equation*}
	with the last inequality following since $\mc{A}_1(\sigma),\ldots,\mc{A}_N(\sigma)$ are disjoint subsets of $\mc{C}_{\sigma}$.
	
	It remains to verify that $x \mapsto \nu\bigl(\mc{A}_j(s)\bigr)/\nu\bigl(B(x_j,s)\bigr)$ is monotonically non-increasing. For any $0 < s < t$ and $j = 1,\ldots,N$, suppose $x \in \mc{A}_j(T) - \{x_j\}$, meaning $x \in B(0,t)$ and $x \not\in B(x_i - x_j,t)$ for any $i = 1,\ldots,j - 1$. Thus $(s/t)x \in B(0,s)$, and
	\begin{equation*}
	\|(s/t)x - (x_i - x_j)\| \geq \|x - (x_i - x_j)\| - \|x - (s/t)x\| > t - (1 - s/t)\|x\| \geq t - (1 - s/t)t = s,
	\end{equation*}
	or in other words $(s/t)x \not\in B(x_i - x_j,s)$ for any $i = 1,\ldots,j - 1$. Consequently,
	\begin{equation*}
	\Bigl(\mc{A}_j(t) - \{x_j\}\Bigr) \subset \frac{t}{s} \cdot \Bigl(\mc{A}_{j}(s)- \{x_j\}\Bigr),
	\end{equation*}
	and applying $\nu(\cdot)$ to both sides yields the claim.
\end{proof}
	
\begin{proof}[Proof (of Lemma~\ref{lem:external_isoperimetric_inequality_density_weighted})]
	Fix $k \in \mathbb{N}$. To establish~\eqref{eqn:external_isoperimetric_inequality_density_weighted}, we partition $\mc{C}_{\lambda,\sigma + r} \setminus \mc{C}_{\lambda,\sigma}$ into thin tubes $\mc{T}_{1},\ldots,\mc{T}_{k}$, with the $j$th tube $\mc{T}_j$ defined as $\mc{T}_j := \mc{C}_{\lambda,\sigma + jr/k} \setminus \mc{C}_{\lambda,\sigma + (j - 1)r/k}.$ We upper bound the Lebesgue measure of each tube $\mc{T}_j$ using~\eqref{eqn:external_isoperimetric_inequality_2}:\footnote{Note that $\mc{C}$ must be bounded, since the density $f(x) \geq \lambda_{\sigma}$ for all $x \in \mc{C}$.}
	\begin{equation*}
	\nu(\mc{T}_j) \leq \frac{dr/k}{\sigma - dr/k} \nu(\mc{C}_{\lambda,\sigma + (j - 1)r/k}) \leq \frac{dr/k}{\sigma - dr/k} \nu(\mc{C}_{\lambda,\sigma + r}) \leq \Bigl(1 + \frac{dr}{\sigma - dr}\Bigr) \cdot \frac{dr/k}{\sigma - dr/k}  \cdot \nu(\mc{C}_{\lambda,\sigma}),
	\end{equation*}
	and the maximum density within each tube using~\ref{asmp:low_noise_density}:
	\begin{equation*}
	\max_{x \in \mc{T}_j} f(x) \leq \lambda_{\sigma} - \theta \Bigl(\frac{j - 1}{k}r\Bigr)^{\gamma};
	\end{equation*}
	combining these upper bounds, we see that
	\begin{equation}
	\label{pf:external_isoperimetric_inequality_density_weighted_1}
	\Pbb\bigl(\mc{C}_{\lambda,\sigma + r} \setminus \mc{C}_{\lambda,\sigma}\bigr) = \sum_{j = 1}^{k} \Pbb(\mc{T}_j) \leq \Bigl(1 + \frac{dr}{\sigma - dr}\Bigr) \cdot  \frac{dr/k}{\sigma - dr/k} \cdot \nu(\mc{C}_{\lambda,\sigma}) \cdot \biggl(\sum_{j = 0}^{k - 1}\lambda_{\sigma} -  \theta r^{\gamma} \Bigl(\frac{j}{k}\Bigr)^{\gamma} \biggr).
	\end{equation}
	Treating the sum in the previous expression as a Riemann sum of a non-increasing function evaluated at $0,\ldots,k -1$ gives the upper bound
	\begin{equation*}
	\sum_{j = 0}^{k - 1}\lambda_{\sigma} -  \theta r^{\gamma} \Bigl(\frac{j}{k}\Bigr)^{\gamma } \leq \lambda_{\sigma} + \int_{0}^{k - 1} \Bigl(\lambda_{\sigma} -  \theta r^{\gamma} \Bigl(\frac{x}{k}\Bigr)^{\gamma}\Bigr) \,dx \leq k\lambda_{\sigma} + (k - 1)\frac{\theta r^{\gamma}}{\gamma + 1} \Bigl(\frac{k - 1}{k}\Bigr)^{\gamma},
	\end{equation*}
	and plugging back in to~\eqref{pf:external_isoperimetric_inequality_density_weighted_1}, we obtain
	\begin{equation*}
	\Pbb\bigl(\mc{C}_{\lambda,\sigma + r} \setminus \mc{C}_{\lambda,\sigma}\bigr) \leq  \Bigl(1 + \frac{dr}{\sigma - dr}\Bigr) \cdot \frac{dr}{\sigma - dr/k} \nu(\mc{C}_{\lambda,\sigma}) \cdot \biggl(\lambda - \frac{\theta r^{\gamma}}{\gamma + 1} \cdot \Bigl(\frac{k - 1}{k}\Bigr)^{\gamma + 1} \biggl).
	\end{equation*}
	The above inequality holds for any $k \in \mathbb{N}$, and taking the limit of the right hand side as $k \to \infty$ yields the claim.
\end{proof}


\subsection{Proof of Lemma~\ref{lem:density_cluster_local_spread}}
\label{subsec:density_cluster_local_spread}
The population-level local spread of $\mc{C}_{\lambda,\sigma}$ is
\begin{equation*}
s_{\Pbb,r}(\mc{C}_{\lambda,\sigma}) = \frac{\bigl(d_{\min}(\wt{\Pbb})\bigr)^2}{\vol_{\wt{\Pbb},r}(\mc{C}_{\lambda},\sigma)}
\end{equation*}
where we recall that $\wt{\Pbb}(\mc{S}) = \frac{\Pbb(\mc{S} \cap \mc{C}_{\lambda,\sigma})}{\Pbb(\mc{C}_{\lambda,\sigma})}$ for Borel sets $\mc{S}$, and $d_{\min}(\wt{\Pbb}) := \min_{x \in \mc{C}_{\lambda,\sigma}}\{\deg_{\wt{\Pbb},r}(x)\}^2$. To lower bound $s_{\Pbb,r}(\mc{C}_{\lambda,\sigma})$, we first lower bound $d_{\min}(\wt{\Pbb})$, and then upper bound $\vol_{\wt{\Pbb},r}(\mc{C}_{\lambda},\sigma)$. Using the lower bound $f(x) \geq \lambda_{\sigma}$ for all $x \in \mc{C}_{\lambda,\sigma}$ stipulated in~\ref{asmp:lambda_bounded_density}, we deduce that
\begin{align*}
d_{\min}(\wt{\Pbb}) & = \min_{x \in \mc{C}_{\lambda,\sigma}} \Bigl\{ \int \1\{\|x - y\| \leq r\} \,d\wt{\Pbb}(y) \Bigr\} \\
& \geq \frac{\lambda_{\sigma}}{\Pbb(\mc{C}_{\lambda,\sigma})} \cdot \min_{x \in \mc{C}_{\lambda,\sigma}} \Bigl\{ \int_{\mc{C}_{\lambda,\sigma}} \1\{\|x - y\| \leq r\} \,dy \Bigr\} \\
& \geq \frac{\lambda_{\sigma}}{\Pbb(\mc{C}_{\lambda,\sigma})} \cdot \frac{1}{2}\nu_dr^d \cdot \Bigl(1 - \frac{r}{\sigma} \sqrt{\frac{d + 2}{2\pi}}\Bigr),
\end{align*}
where the final inequality follows from Lemma~\ref{lem:overlap_balls}. 

On the other hand, using the upper bound $f(x) \leq \Lambda_{\sigma}$ for all $x \in \mc{C}_{\lambda,\sigma}$, we deduce that
\begin{align*}
\vol_{\wt{\Pbb},r}(\mc{C}_{\lambda,\sigma}) & = \iint \1\{\|x - y\|\leq r\} \,d\wt{\Pbb}(y) \,d\wt{\Pbb}(x) \\
& \leq \frac{\Lambda_{\sigma}^2}{\Pbb(\mc{C}_{\lambda,\sigma})^2} \cdot \int_{\mc{C}_{\lambda,\sigma}} \int_{\mc{C}_{\lambda,\sigma}} \1\{\|x - y\|\leq r\} \,dy \,dx \\
& \leq \frac{\Lambda_{\sigma}^2}{\Pbb(\mc{C}_{\lambda,\sigma})^2} \cdot \nu_d r^d \cdot \nu(\mc{C}_{\lambda,\sigma}) \\
& \leq \frac{\Lambda_{\sigma}^2}{\Pbb(\mc{C}_{\lambda,\sigma})^2} \cdot \nu_d^2 r^d  \cdot \Bigl(\frac{\rho}{2}\Bigr)^d;
\end{align*}
the final inequality follows from~\ref{asmp:embedding}, which implies that $\nu(\mc{C}_{\lambda,\sigma}) = \nu(\mc{K}) \leq \nu_d(\rho/2)^d$. The claim of Lemma~\ref{lem:density_cluster_local_spread} follows.

\subsection{Proof of Proposition~\ref{prop:density_cluster_normalized_cut}}
\label{subsec:density_cluster_ncut}
By Assumption~\ref{asmp:bounded_volume}, we have that $\Phi_{\Pbb,r}(\mc{C}_{\lambda,\sigma}) = \cut_{\Pbb,r}(\mc{C}_{\lambda,\sigma})/\vol_{\Pbb,r}(\mc{C}_{\lambda,\sigma})$, and to prove Proposition~\ref{prop:density_cluster_normalized_cut} we must therefore upper bound $\cut_{\Pbb,r}(\mc{C}_{\lambda,\sigma})$ and lower bound $\vol_{\Pbb,r}(\mc{C}_{\lambda,\sigma})$. 

Let $\mc{C}_{\lambda,\sigma + r} = \{x: \mathrm{dist}(x,\mc{C}_{\lambda}) \leq \sigma + r\}$. We upper bound $\cut_{\Pbb,r}(\mc{C}_{\lambda,\sigma})$ in terms of the probability mass of $\mc{C}_{\lambda,\sigma + r} \setminus \mc{C}_{\lambda,\sigma}$:
\begin{align*}
\cut_{\Pbb,r}(\mc{C}_{\lambda,\sigma}) & = \iint \1\{\|x - y\| \leq r \} \cdot \1\{x \in \mc{C}_{\lambda,\sigma}\} \cdot \1\{y \not\in \mc{C}_{\lambda,\sigma} \} \,d\Pbb(y) \,d\Pbb(x) \\
& \leq  \iint \1\{\|x - y\| \leq r \} \cdot \1\{x \in \mc{C}_{\lambda,\sigma}\} \cdot \1\{y \in \mc{C}_{\lambda,\sigma + r} \setminus \mc{C}_{\lambda,\sigma}  \} \,d\Pbb(y) \,d\Pbb(x) \\
& \leq \lambda \nu_d r^d \cdot \Pbb\bigl(\mc{C}_{\lambda,\sigma + r} \setminus \mc{C}_{\lambda,\sigma}\bigr).
\end{align*}
On the other hand, using the lower bound $f(x) \geq \lambda_{\sigma} $ for all $x \in \mc{C}_{\lambda,\sigma}$, we lower bound $\cut_{\Pbb,r}(\mc{C}_{\lambda,\sigma})$ in terms of the Lebesgue measure of $\mc{C}_{\lambda,\sigma}$:
\begin{align*}
\vol_{\Pbb,r}(\mc{C}_{\lambda,\sigma}) & = \iint \1\{\|x - y\| \leq r \} \cdot \1\{x \in \mc{C}_{\lambda,\sigma}\} \,d\Pbb(y) \,d\Pbb(x) \\
& \geq \lambda_{\sigma}^2 \cdot \iint \1\{\|x - y\| \leq r \} \cdot \1\{x,y \in \mc{C}_{\lambda,\sigma}\} \,dy \,dy \\
& \geq \lambda_{\sigma}^2 \cdot \frac{1}{2} \nu_d r^d \cdot \Bigl(1 - \frac{r}{\sigma} \sqrt{\frac{d + 2}{2\pi}}\Bigr) \cdot \nu(\mc{C}_{\lambda,\sigma}).
\end{align*}
The claim of Proposition~\ref{prop:density_cluster_normalized_cut} follows upon using Lemma~\ref{lem:external_isoperimetric_inequality_density_weighted} to upper bound $\Pbb\bigl(\mc{C}_{\lambda,\sigma + r} \setminus \mc{C}_{\lambda,\sigma}\bigr)$.

\subsection{Proof of Proposition~\ref{prop:density_cluster_conductance}}
\label{subsec:density_cluster_conductance}
\textcolor{red}{(TODO)}

\section{Proof of Major Theorems}
\label{sec:pf_major_theorems}

\section{Additional results: aPPR and Consistency of PPR}
\label{sec:appr_misclassification_error}

\subsection{Consistency}
\label{subsec:consistent_recovery_density_clusters}

The symmetric set difference does not measure whether \smash{$\wh{C}$}
can (perfectly) distinguish any two distinct clusters \smash{$\mc{C}_{\lambda},\mc{C}_{\lambda}' \in 
	\mathbb{C}_f(\lambda)$}. We therefore also study a second notion of cluster 
estimation, first introduced by \citet{hartigan1981}, and defined
asymptotically.  

\begin{definition}
	\label{def:density_cluster_consistency}
	For an estimator \smash{$\wh{C} \subseteq X$} and cluster \smash{$\mc{C}_{\lambda} \in \mathbb{C}_f(\lambda)$}, we call \smash{$\wh{C}$} \emph{consistent} for
	\smash{$\mc{C}_{\lambda}$} if for all \smash{$\mc{C}_{\lambda}' \in \mathbb{C}_f(\lambda)$} with  
	\smash{$\mc{C} \not= \mc{C}'$}, the following holds with probability tending to 1 as $n \to \infty$: 
	\begin{equation}
	\label{eqn:density_cluster_consistency}
	\mc{C}[X] \subseteq \wh{C} \quad \text{and} \quad
	\wh{C} \cap \mc{C}'[X] = \emptyset.
	\end{equation}
\end{definition}

Consistent cluster recovery roughly ensures that, for a given 
threshold $\lambda>0$, the estimated cluster \smash{$\wh{C}$} contains all points
in a true density cluster $\mc{C}_{\lambda} \in \mathbb{C}_f(\lambda)$, and simultaneously does 
not contain any points in any other density cluster $\mc{C}_{\lambda}' \in
\mathbb{C}_f(\lambda)$. 

The bound on symmetric set difference \eqref{eqn:density_cluster_volume_ssd_ub} does not imply consistent density cluster estimation in the sense of \eqref{eqn:density_cluster_consistency}. This notion of consistency requires a
uniform bound over the PPR vector $p_v$: as an example, suppose that we were
able to show that for all \smash{$\mc{C}' \in \mathbb{C}_f(\lambda), \mc{C}' \neq \mc{C}$}, and each $u \in \mc{C}, w \in \mc{C}'$,  
\begin{equation}
\label{eqn:ppr_gap}
\frac{p_v(w)}{D_{ww}} \leq \frac{1}{100 {n \choose 2} \vol_{\Pbb,r}(\mc{C}_{\lambda,\sigma})} <
\frac{1}{10 {n \choose 2} \vol_{\Pbb,r}(\mc{C}_{\lambda,\sigma})} \leq \frac{p_v(u)}{D_{uu}}. 
\end{equation}
Then, any $(L,U)$ satisfying~\eqref{eqn:initialization} and any sweep cut
$S_{\beta}$ for $\beta \in (L,U)$ would fulfill both conditions laid out in
\eqref{eqn:density_cluster_consistency}. In Theorem 
\ref{thm:density_cluster_consistent_recovery}, we show that a sufficiently 
small upper bound on $\Delta(\wh{C},\mc{C}_{\lambda,\sigma}[X])$ ensures that with high probability the uniform bound~\eqref{eqn:ppr_gap} is satisfied, and hence implies
\smash{$\wh{C}$} will be a consistent estimator.
\begin{theorem}
	\label{thm:density_cluster_consistent_recovery}
	For any $\delta \in (0,1)$, the following statement holds with probability at least $1 - 3\exp\{-b_1\delta^2n\} - B_2n\exp\{-b_2\delta^2n\}$: there exists a constant $c_1$ and a set $\mc{C}_{\lambda,\sigma}[X]^g \subseteq \mc{C}_{\lambda,\sigma}[X]$ of large volume,  \smash{$\vol_{n,r}(\mc{C}_{\lambda,\sigma}[X]^g) \geq\vol_{n,r}(\mc{C}_{\lambda,\sigma}[X])/2$}, such that if Algorithm~\ref{alg:ppr} is $\delta$-well-initialized and run with any seed node $v \in \mc{C}_{\lambda,\sigma}[X]^g$, and moreover
	\begin{equation}
	\label{eqn:density_cluster_consistent_recovery_condition}
	\kappa_{\Pbb,r}(\mc{C}_{\lambda,\sigma},\delta) \leq c_1 \cdot \min_{x: f(x) \geq \lambda} \Bigl\{\deg_{\Pbb,r}(x)\Bigr\},
	\end{equation}
	then the PPR estimated cluster \smash{$\wh{C}$} satisfies \eqref{eqn:density_cluster_consistency}.
\end{theorem}
\textcolor{red}{(In one sense, this is a strong result: if the density cluster $\mc{C}_{\lambda}$ satisfies the requirement~\eqref{eqn:density_cluster_consistent_recovery_condition}, and we are willing to ignore the behavior of the algorithm in low-density regions, Corollary~\ref{cor:density_cluster_volume_ssd_ub} guarantees that PPR will \emph{perfectly distinguish} the candidate cluster $\mc{C}_{\lambda}$ from all other connected components $\mc{C}_{\lambda}' \in \mathbb{C}_f(\lambda), \mc{C}_\lambda' \not= \mc{C}_{\lambda}$.)}\vspace{1 mm}

On the other hand, unfortunately the requirement~\eqref{eqn:density_cluster_consistent_recovery_condition} is rather restrictive. Specifically, from the following chain of inequalities,
\begin{equation*}
\frac{\Delta(\wh{C}, \mc{C}_{\lambda,\sigma}[X])}{\vol_{n,r}(\mc{C}_{\lambda,\sigma}[X])} \overset{(\textrm{Thm.~\ref{thm:volume_ssd_ub}})}{\leq} \kappa_{\Pbb,r}(\mc{C},\delta) \overset{\eqref{eqn:density_cluster_consistent_recovery_condition}}{\leq} c_1 \cdot \min_{x: f(x) \geq \lambda} \bigl\{\deg_{\Pbb,r}(x)\bigr\} \overset{\ref{asmp:lambda_bounded_density}}{\leq} c_1 \cdot \Lambda_{\sigma} \nu_d r^d,
\end{equation*}
we see that in order for~\eqref{eqn:density_cluster_consistent_recovery_condition} to be met, it is necessary that \smash{$\Delta(\wh{C}, \mc{C}_{\lambda,\sigma}[X])/\vol_{n,r}(\mc{C}_{\lambda,\sigma}[X])$} be at most $c_1 \cdot \Lambda_{\sigma} \nu_d r^d$. In plain terms, we 
are able to recover a density cluster $\mc{C}_{\lambda}$ in the strong sense of
\eqref{eqn:density_cluster_consistency} only when we can guarantee the volume of the symmetric set difference will be very small. This strong condition is
the price we pay in order to obtain the uniform bound in~\eqref{eqn:ppr_gap}.\vspace{1 mm}

\textcolor{red}{Originally, we had only (something like) the preceding paragraph, which in my opinion is very negative and invites the reader to discount the preceding result. I added the paragraph in red above it, to make it clear that the Corollary is interesting even if the required conditions are strong.}


\section{Experimental Details}
\label{sec:experimental_details}

\clearpage
\bibliographystyle{plainnat}
\bibliography{../../local_spectral_bibliography} 

\end{document}