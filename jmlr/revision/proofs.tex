\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{xr}
\usepackage[colorlinks]{hyperref}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{fullpage}

\externaldocument{local_spectral_clustering_jmlr_revision_2} 

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\seminorm}[1]{\lvert}{\rvert}{#1}

% Make a widecheck symbol (thanks, Stack Exchange!)
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
	<5> <6> <7> <8> <9> <10>
	<10.95> <12> <14.4> <17.28> <20.74> <24.88>
	mathx10
}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareFontSubstitution{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}
% widecheck made

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\iid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\dagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}
\newcommand{\lambdavec}{\boldsymbol{\lambda}}

%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Dgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\dagger}}
\newcommand{\Lap}{L}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}
\newcommand{\Id}{I}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Vset}{\mathcal{V}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\dive}{\mathrm{div}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\dx}{\,dx}
\newcommand{\dy}{\,dy}
\newcommand{\dr}{\,dr}
\newcommand{\dxpr}{\,dx'}
\newcommand{\dypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\SM}{\mathrm{SM}}
\newcommand{\OS}{\mathrm{OS}}
\newcommand{\PLS}{\mathrm{PLS}}

%%% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{example}{Example}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{remark}{Remark}[section]


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
\title{Proofs for ``Geometry of Local Spectral Clustering''}
\author{Alden Green}
\date{\today}
\maketitle

The proofs of our major theorems all consist of at most three modular parts.
\begin{enumerate}
	\item~\textbf{Fixed graph results.} Results which hold with respect to an arbitrary graph $G$, and are stated with respect to functionals (i.e. normalized cut, conductance, and local spread) of $G$;
	\item~\textbf{Finite-sample bounds.} For the specific choice of $G = G_{n,r}$, we relate the aforementioned functionals to their population analogues. This proves~\textcolor{red}{(?)} 
	\item~\textbf{Population functionals.} (In the case of density clustering only.) When the candidate cluster is a $\lambda$-density cluster, we bound the population functionals by a function of $\lambda$, as well as the other relevant parameters introduced in Section~\textcolor{red}{(?)}. This proves~\textcolor{red}{(?)}
\end{enumerate}
The first three sections of this appendix will correspond to each of these three parts. In Section~\ref{sec:pf_major_theorems}, we will combine these parts to prove the major theorems of our main text, \textcolor{red}{(...)}. Finally, in Section~\ref{sec:appr_misclassification_error} we derive upper bounds for the aPPR vector, and in Section~\ref{sec:experimental_details} we give relevant details regarding our experiments.

\section{Fixed graph results}
In this section, we give all results that hold with respect to an arbitrary graph $G$. For the convenience of the reader, we begin by reviewing some notation from the main text, and also introduce some new notation. 

\paragraph{Notation.}
The graph $G = (V,E)$ is an undirected and connected but otherwise arbitrary graph, defined over vertices $V = \{1,\ldots,n\}$ with $m = |E|$ total edges. The adjacency matrix of $G$ is $A$, the degree matrix is $D$, and the lazy random walk matrix over $G$ is $W = (I + D^{-1}A)/2$. If the lazy random walk originates at a node $v$, the distribution of the lazy random walk $q_v^{(t)} := q(v,t;G)$ after $t$ steps is $q_v^{(t)} := e_v W^t$, with stationary distribution $\pi := \pi(G) := \lim_{t \to \infty} q_v^{(t)}$ with entries $\pi(u) = \deg(u;G)/\vol(u;G)$.

For a starting distribution $s$ (by distribution we mean a vector with non-negative entries), the PPR vector $p_s = p(s,\alpha;G)$ is the solution of
\begin{equation}
\label{eqn:ppr}
p_s = \alpha s + (1 - \alpha) p_s W.
\end{equation}
When $s = e_v$, we write $p_v := p_{e_v}$. It is easy to check that $p_s = \alpha \sum_{t = 0}^{\infty} (1 - \alpha)^t q_s^{(t)}$.  Note that $s$ need not be a probability distribution (i.e. its entries need not sum to $1$) to make sense of~\eqref{eqn:ppr}.

Given a distribution $q$ (for instance, $q = q_v^{(t)}$ for $t \in \mathbb{N}$, $q = p_v$, or $q = \pi$) and $\beta \in (0,1)$, the $\beta$-sweep cut of $q$ is
\begin{equation*}
S_{\beta}(q) = \set{u: \frac{q(u)}{\deg(u;G)} > \beta};
\end{equation*} 
in the special case where $q = p_v$ we write $S_{\beta,v}$ for $S_{\beta}(p_v)$. The argument of $S_{\beta}(\cdot)$ will usually be clear from context, in which case we will drop it and simply write $S_{\beta}$. For $j = 1,\ldots,n$, let $\beta_j$ be the smallest value of $\beta \in (0,1)$ such that the sweep cut $S_{\beta_j}$ contains at least $j$ vertices. For notational ease, we will write $S_j := S_{\beta_j}$, and $S_0 = \emptyset$. 

We now introduce the~\emph{Lovasz-Simonovits curve} $h_q(\cdot): [0,2m] \to [0,1]$ to measure the extent to which a distribution $q$ is mixed. To do so, we first define a piecewise linear function $q[\cdot]: [0,2m] \to [0,1]$. Letting $q(S) := \sum_{u \in S} q(u)$, we take $q[\vol(S_j)] = q(S_j)$ for each sweep cut $S_j$, and then extend $q[\cdot]$ by piecewise linear interpolation to be defined everywhere on its domain. Then the mixedness of $q$ is measured by
\begin{equation*}
h_q(k) := q[k] - \frac{k}{2m}.
\end{equation*}
The Lovasz-Simonovits curve is a non-negative function, with $h_q(0) = h_q(2m) = 0$. The stationary distribution $\pi$ is mixed, i.e. $h_{\pi}(k) = 0$ for all $k \in [0,2m]$. Finally, both $q[\cdot]$ and $h_q(\cdot)$ are concave functions, which will be an important fact later on.  

The conductance of $V$ is abbreviated as $\Psi(G) := \Psi(V;G)$, and likewise for the local spread $s(G) := s(V;G)$. Finally, for convenience we introduce the following functionals:
\begin{equation*}
\begin{aligned}
& d_{\max}(C; G) := \max_{u \in C} \deg(u; G), && d_{\min}(C; G) := \min_{u \in C} \deg(u;G) \\
& d_{\max}(G) := d_{\max}(V;G),~~ && d_{\min}(G) := d_{\min}(V; G)
\end{aligned}
\end{equation*}
We note that $d_{\min}(G)^2 \leq d_{\min}(G) \cdot n \leq \vol(G) \leq n \cdot d_{\max}(G)$, and that for any $S \subseteq V$, $|S| \cdot d_{\min}(G) \leq \vol(S;G)$ (where $|S|$ is the cardinality of $S$.)

\paragraph{Organization.} In the following subsections we establish: (\ref{subsec:pf_lem_zhu}) an upper bound on the misclassification error of PPR in terms of $\alpha$ and $\Phi(C;G)$ (Lemma~\ref{lem:zhu}); (\ref{subsec:ppr_uniform_bounds}) a uniform bound on the perturbations of the PPR vector, to be used later in the proof of Theorem~\ref{thm:density_cluster_consistent_recovery} (consistency of PPR);  (\ref{subsec:lovasz_simonovits_bounds}) upper bounds on the mixedness of $q_v^{(t)}$ (as a function of $t$) and $p_v$ (as a function of $\alpha$), which will be helpful in the proofs of Proposition~\ref{prop:pointwise_mixing_time} and Theorem~\ref{thm:ppr_lb}; (\ref{subsec:pf_prop_pointwise_mixing_time}) an upper bound on $\tau_{\infty}(G)$ in terms of $\Psi(G)$ and $s(G)$ (Proposition~\ref{prop:pointwise_mixing_time}); and (\ref{subsec:ppr_spectral_partitioning}) an upper bound on the normalized cut $\Phi(\wh{C};G)$ in terms of $\Phi(C;G)$, to be used later in the proof of Theorem~\ref{thm:ppr_lb} (negative example). 

\subsection{Proof of Lemma~\ref{lem:zhu}}
\label{subsec:pf_lem_zhu}
For a candidate cluster $C \subseteq V$, we use the tilde-notation $\wt{G} = G[C]$ to refer to the subgraph of $G$ induced by $C$. Similarly we write $\wt{q}_v^{(t)} := q(v,t;\wt{G})$ for the $t$-step distribution of the lazy random walk over $\wt{G}$, $\wt{\pi} = \pi(G[C])$ for the stationary distribution of $\wt{q}_v^{(t)}$ (we will always assume $G[C]$ is connected), and $\wt{p}_v := p(v,\alpha;\wt{G})$ for the PPR vector over $\wt{G}$.

As mentioned in the main text, Lemma~\ref{lem:zhu} is equivalent, up to constants, to Lemma~3.4 in \cite{zhu2013}, and the proof of Lemma~\ref{lem:zhu} proceeds along very similar lines to the proof of that lemma. In fact, we directly use the following three inequalities, derived in that work:
\begin{itemize}
	\item \textbf{(c.f. Lemma 3.2 of \cite{zhu2013})} For any seed node $v \in C$, the PPR vector is lower bounded,
	\begin{equation}
	\label{pf:zhu1}
	\wt{p}_v(u) \geq \frac{3}{4}\bigl(1 - \alpha \cdot \tau_{\infty}(\wt{G})\bigr) \cdot \wt{\pi}(u),~~\textrm{for every $u \in C$.}
	\end{equation}
	\item \textbf{(c.f. Corollary 3.3 of \cite{zhu2013})} For any seed node $v \in C$, there exists a so-called leakage distribution $\ell = \ell(v)$ such that $\mathrm{supp}(\ell) \subseteq C$, $\|\ell\|_1 \leq 2\Phi(C;G)/\alpha$, and 
	\begin{equation}
	\label{pf:zhu2}
	p_v(u) \geq \wt{p}_v(u) - \wt{p}_{\ell}(u),~~\textrm{for every $u \in C$.}
	\end{equation}
	\item \textbf{(c.f. Lemma 3.1 of \cite{zhu2013})} There exists a set $C^g \subset C$ with $\vol(C^g;G) \geq \frac{1}{2}\vol(C;G)$ such that for any seed node $v \in C^g$, the following inequality holds
	\begin{equation}
	\label{pf:zhu3}
	p_v(C^c) \leq 2\frac{\Phi(C;G)}{\alpha}.
	\end{equation}
\end{itemize}
We use~\eqref{pf:zhu1}-\eqref{pf:zhu3} to separately upper bound $\vol(S_{\beta,v} \setminus C;G)$, $\vol(C^{\mathrm{int}} \setminus S_{\beta,v};G)$ and $\vol(C^{\mathrm{bdry}} \setminus S_{\beta,v};G)$; here $C^{\mathrm{int}} \cup C^{\mathrm{bdry}} = C$ is a partition of $C$, with
\begin{equation*}
C^{\mathrm{int}} := \Bigl\{u \in C: \deg(u;\wt{G}) > \bigl(1 - \alpha \cdot \beta \cdot \vol(C;G)\bigr) \deg(u;G) \Bigr\},
\end{equation*}
consisting of those vertices $u \in C$ with sufficient large degree in $\wt{G}$. 

First we upper bound $\vol(S_{\beta,v} \setminus C;G)$. Observe that for any $u \in S_{\beta,v} \setminus C$, $p_v(u) > \beta \cdot \deg(u;G)$. Summing up over all such vertices, from~\eqref{pf:zhu3} we conclude that
\begin{equation}
\label{pf:zhu3.5}
\vol(S_{\beta,v} \setminus C; G) \leq \frac{p_v(C^c)}{\beta} \leq 2\frac{\Phi(C;G)}{\beta \cdot \alpha}.
\end{equation} 

Next we upper bound $\vol(C^{\mathrm{int}} \setminus S_{\beta,v};G)$. From~\eqref{pf:zhu1} and~\eqref{pf:zhu2} we see that 
\begin{equation*}
p_v(u) \geq \frac{3}{4}\bigl(1 - \alpha \cdot \tau_{\infty}(\wt{G})\bigr) \cdot \wt{\pi}(u) - \wt{p}_{\ell}(u)~~\textrm{for all $u \in C$.}
\end{equation*}
If additionally $u \not\in S_{\beta,v}$ then $p_v(u) \leq \beta \deg(u;G)$, and for all such $u \in C \setminus S_{\beta,v}$,
\begin{equation}
\label{pf:zhu4}
\frac{3}{4}\bigl(1 - \alpha \cdot \tau_{\infty}(\wt{G})\bigr) \cdot \wt{\pi}(u) -  \beta\deg(u;G) \leq \wt{p}_{\ell}(u).
\end{equation}
On the other hand, for any $u \in C^{\mathrm{int}}$ it holds that
\begin{equation*}
\wt{\pi}(u) = \frac{\deg(u;\wt{G})}{\vol(\wt{G})} \geq \frac{\deg(u;\wt{G})}{\vol(G)} \geq \frac{(1 - \alpha \beta \vol(C;G))\deg(u;G)}{\vol(C;G)};
\end{equation*}
by plugging this in to~\eqref{pf:zhu4} we obtain
\begin{equation*}
\biggl(\frac{3(1 - \alpha \beta \vol(C;G))\cdot\bigl(1 - \alpha \tau_{\infty}(\wt{G})\bigr)}{4\vol(C;G)} - \beta\biggr) \cdot \deg(u;G) \leq \wt{p}_{\ell}(u),~~\textrm{for all $u \in C^{\mathrm{int}} \setminus S_{\beta,v}$},
\end{equation*}
and summing over all such $u$ gives
\begin{equation*}
\biggl(\frac{3(1 - \alpha \beta \vol(C;G))\cdot\bigl(1 - \alpha \tau_{\infty}(\wt{G})\bigr)}{4\vol(C;G)} - \beta\biggr) \cdot \vol\bigl(C^{\mathrm{int}} \setminus S_{\beta,v}; G\bigr) \leq \wt{p}_{\ell}\bigl(C^{\mathrm{int}} \setminus S_{\beta,v}\bigr) \leq 2\frac{\Phi(C;G)}{\alpha}.
\end{equation*}
The upper bounds in~\eqref{eqn:zhu_condition} imply
\begin{equation*}
\biggl(\frac{3(1 - \alpha \beta \vol(C;G))\cdot\bigl(1 - \alpha \tau_{\infty}(\wt{G})\bigr)}{4\vol(C;G)} - \beta\biggr) \geq \frac{2}{3}\beta,
\end{equation*}
and we conclude that
\begin{equation}
\label{pf:zhu5}
\vol(C^{\mathrm{int}} \setminus S_{\beta,v}; G) \leq \frac{3\Phi(C;G)}{\alpha\beta}.
\end{equation}

Finally, we upper bound $\vol(C^{\mathrm{bdry}} \setminus S_{\beta,v};G)$. Indeed, for any $u \in C^{\mathrm{bdry}}$,
\begin{equation*}
\frac{1}{\vol(C;G)}\sum_{w \not\in C} \1((u,w) \in E) \geq \alpha \cdot \beta \cdot \deg(u;G)
\end{equation*}
and summing over all such vertices yields
\begin{equation}
\label{pf:zhu6}
\vol(C^{\mathrm{bdry}};G) \leq \frac{1}{\alpha \beta \vol(C;G)}\sum_{\substack{u \in C^{\mathrm{bdry}} \\ w \not\in C}} \1((u,w) \in E) \leq \frac{\Phi(C;G)}{\alpha \cdot \beta}.
\end{equation} 
The claim follows upon summing the upper bounds in~\eqref{pf:zhu3.5}, \eqref{pf:zhu5} and~\eqref{pf:zhu6}.

\subsection{Uniform bounds on PPR}
\label{subsec:ppr_uniform_bounds}
As mentioned in our main text, in order to prove Theorem~\ref{thm:density_cluster_consistent_recovery}, we require a uniform bound on the PPR vector. Actually, we require two such bounds: for a candidate cluster $C \subseteq V$ and an alternative cluster $C' \subseteq V$, we require a lower bound on $p_v(u)$ for all $u \in C$, and an upper bound on $p_v(u')$ for all $u' \in C'$. In Lemma~\ref{lem:ppr_uniform_bound} we establish an upper bound that holds for all vertices $u$ in the interior $C_{o}$ of $C$, and a lower bound holds for all vertices $u'$ in the interior of $C_{o}'$ of $C'$; here
\begin{equation*}
C_{o} = \Bigl\{u \in C: \deg(u,\wt{G}) =  \deg(u;G)\Bigr\},~~\textrm{and}~~C_{o}'= \Bigl\{u \in C': \deg(u,G[C']) =  \deg(u;G)\Bigr\},
\end{equation*}
and we remind the reader that $\wt{G} = G[C]$. 
\begin{lemma}
	\label{lem:ppr_uniform_bound}
	Let $C$ and $C'$ be disjoint subsets of $V$, and suppose that
	\begin{equation*}
	\alpha \leq \frac{1}{2\tau_{\infty}(\wt{G})}.
	\end{equation*}
	Then there exists a set $C^g \subseteq C$ with $\vol(C^g;G) \geq \vol(C;G)/2$ such that for any $v \in C^g$,
	\begin{equation}
	\label{eqn:ppr_uniform_bound_C}
	p_v(u) \geq \frac{3}{8}\wt{\pi}(u) - \frac{2 \Phi(C;G)}{d_{\min}(\wt{G})\cdot \alpha}~~\textrm{for all $u \in C_{o}$}
	\end{equation}
	and
	\begin{equation}
	\label{eqn:ppr_uniform_bound_Cprime}
	p_v(u') \leq \frac{2\Phi(C;G)}{d_{\min}(C';G) \cdot \alpha}~~\textrm{for all $u \in C_{o}'$.}
	\end{equation}
\end{lemma}

\paragraph{``Leakage'' and ``soakage'' vectors.} To prove Lemma~\ref{lem:ppr_uniform_bound}, we will make use of the following explicit representation of the \emph{leakage} distribution $\ell$ from~\eqref{pf:zhu3}, as well as an analogously defined \emph{soakage} distribution $s$:
\begin{equation}
\label{eqn:leakage_soakage}
\begin{aligned}
\ell^{(t)} & := e_v(W \wt{I})^t(I - D^{-1}\wt{D}),~~&& \ell = \sum_{t = 0}^{\infty} (1 - \alpha)^t \ell^{(t)} \\
s^{(t)} & := e_v(W \wt{I})^t W (I - \wt{I}),~~&& s = \sum_{t = 0}^{\infty} (1 - \alpha)^t s^{(t)}.
\end{aligned}
\end{equation}
In the above, $\wt{I} \in \Reals^{n \times n}$ is a diagonal matrix with $I_{uu} = 1$ if $u \in C$ and $0$ otherwise, and $\wt{D}$ is the diagonal matrix with $\wt{D}_{uu} = \deg(u;\wt{G})$ if $u \in C$, and $0$ otherwise. 

These quantities admit a natural interpretation in terms of random walks. For $u \in C$, $\ell^{(t)}(u)$ is the probability that a lazy random walk over $G$ originating at $v$ stays within $\wt{G}$ for $t$ steps, arriving at $u$ on the $t$th step, and then ``leaks out'' of $C$ on the $(t + 1)$st step. On the other hand, for $u \not\in C$, $s^{(t)}(u)$ is the probability that a lazy random walk over $G$ originating at $v$ stays within $\wt{G}$ for $t$ steps and is then ``soaked up'' into $u$ on the $(t + 1)$st step. The vectors $\ell$ and $s$ then give the total mass leaked and soaked, respectively, by the PPR vector. 

Three properties of $\ell$ and $s$ are worth pointing out. First, $\mathrm{supp}(\ell) \subseteq C \setminus C_o$ and $\mathrm{supp}(s) \subseteq V \setminus C$. Second, $\|\ell^{(t)}\|_1 = \|s^{(t)}\|_1$ for all $t \in \mathbb{N}$, and so $\|\ell\|_1 = \|s\|_1$. Third, for any $u \in V \setminus C$, $p_v(u) = p_s(u)$. The first two properties are immediate. The third property follows by the law of total probability, which implies that
\begin{equation*}
q_v^{(\tau)}(u) = \sum_{t = 0}^{\tau} q_{s^{(t)}}^{(\tau - t)}(u),~~\textrm{for all $u \in V \setminus C$.}
\end{equation*}
or in terms of the PPR vector,
\begin{equation*}
p_v(u) = \alpha \sum_{\tau = 0}^{\infty} (1 - \alpha)^{\tau} q_v^{(\tau)}(u) = \alpha \sum_{\tau = 0}^{\infty} \sum_{t = 0}^{\tau} (1 - \alpha)^{\tau} q_{s^{(t)}}^{(\tau - t)}(u).
\end{equation*}
Substituting $\Delta = \tau + t$ and rearranging gives the claimed property, as
\begin{equation*}
p_v(u) = \alpha \sum_{\tau = 0}^{\infty} \sum_{t = 0}^{\tau} (1 - \alpha)^{\tau} q_{s^{(t)}}^{(\tau - t)}(u) = \sum_{\Delta = 0}^{\infty} \sum_{t = 0}^{\infty} (1 - \alpha)^{\Delta + t} q_{s^{(t)}}^{(\Delta)}(u) = \alpha \sum_{\Delta = 0}^{\infty} (1 - \alpha)^{\Delta} q_s^{(\Delta)}(u) = p_s(u).
\end{equation*}

\begin{proof}[Proof (of Lemma~\ref{lem:ppr_uniform_bound})]
	We first show~\eqref{eqn:ppr_uniform_bound_C}. From~\eqref{pf:zhu3} and~\eqref{pf:zhu2}, we have that
	\begin{equation*}
	p_v(u) \geq \frac{3}{4}\bigl(1 - \alpha \cdot \tau_{\infty}(\wt{G})\bigr) \cdot \wt{\pi}(u) - \wt{p}_{\ell}(u)~~\textrm{for all $u \in C$,}
	\end{equation*}
	where $\ell$ has support $\mathrm{supp}(\ell) \subseteq C$ with $\|\ell\|_1 \leq 2\Phi(C;G)/\alpha$. Recalling that $u \in C_{o}$ implies that $u \not\in \mathrm{supp}(C_{o})$, as a consequence of~\eqref{pf:interpolator_bound_max_entry}, 
	\begin{equation*}
	\wt{p}_{\ell}(u) \leq \frac{\|\ell\|_1}{d_{\min}(\wt{G})}~~\textrm{for all $u \in C_{o}$,}
	\end{equation*}
	establishing~\eqref{eqn:ppr_uniform_bound_C}. The proof of~\eqref{eqn:ppr_uniform_bound_Cprime} follows similarly:
	\begin{equation*}
	p_v(u) = p_s(u) \overset{(i)}{\leq} \frac{\|s\|_1}{d_{\min}(C';G)} = \frac{\|\ell\|_1}{d_{\min}(C';G)},~~\textrm{for all $u \in C_{o}'$},
	\end{equation*}
	where the presence of $d_{\min}(C';G)$ on the right hand side of $(i)$ can be verified by inspecting~\eqref{pf:interpolator_bound_max_entry_inductive_step}. 	
\end{proof}

\subsection{Mixedness of lazy random walk and PPR vectors}
\label{subsec:lovasz_simonovits_bounds}
In this subsection, we give upper bounds on $h^{(t)} := h_{q_v^{(t)}}$ and $h^{(\alpha)} := h_{p_v}$. Although similar bounds exist in the literature (see in particular Theorem~1.1 of \citep{lovasz1990} and Theorem~3 of~\citep{andersen2006}), we could not find precisely the results we needed, and so for completeness we state and prove these results ourselves. 

\begin{theorem}
	\label{thm:mixing_time_rw}
	For any $k \in [0, 2m]$, $t_0 \in \mathbb{N}$ and $t \geq t_0$,
	\begin{equation}
	\label{eqn:mixing_time_rw_1}
	h^{(t)}(k) \leq \frac{1}{2^{t_0}} + \frac{d_{\max}(G)}{d_{\min}(G)^2} + \frac{m}{d_{\min}(G)^2} \biggl(1 - \frac{\Psi(G)^2}{8}\biggr)^{t - t_0}.
	\end{equation}
\end{theorem}


\begin{theorem}
	\label{thm:mixing_time_PPR}
	Let $\phi$ be any constant in $[0,1]$. Either the following bound holds for any $t \in \mathbb{N}$ and any $k \in [d_{\max}(G),2m - d_{\max}(G)]$:
	\begin{equation*}
	h^{(\alpha)}(k) \leq \alpha t + \frac{2\alpha}{1 + \alpha} + \frac{d_{\max}(G)}{d_{\min}(G)^2} + \frac{m}{d_{\min}(G)^2} \left(1 - \frac{\phi^2}{8}\right)^{t},
	\end{equation*}
	or there exists some sweep cut $S_j$ of $p_v$ such that $\Phi(S_j;G) < \phi$.
\end{theorem}

The proofs of these upper bounds will be similar to each other (in places word-for-word alike), and will follow a similar approach and use similar notation to that of \citep{lovasz1990,andersen2006}. For $h: [0,2m] \to [0,1]$, $0 \leq K_0 \leq m$ and $k \in [K_0,2m - K_0]$, define
\begin{equation*}
L_{K_0}(k;h) = \frac{2m - K_0 - k}{2m - 2K_0}h(K_0) + \frac{k - K_0}{2m - 2K_0}h(2m - K_0)
\end{equation*}
to be the linear interpolant of $h(K_0)$ and $h(2m - K_0)$, and additionally let
\begin{equation*}
C(K_0;h) := \max\set{\frac{h(k) - L_{K_0}(k;h)}{\sqrt{\overline{k}}}: K_0 \leq k \leq  2m - K_0}.
\end{equation*}
where we use the notation $\overline{k} := \min\{k, 2m - k\}$, and treat $0/0$ as equal to $1$. Our first pair of Lemmas upper bound $h^{(t)}$ and $h^{(\alpha)}$ as a function of $L_{K_0}$ and $C(K_0)$. Lemma~\ref{lem:mixing_random_walk} implies that if $t$ is large relative to $\Psi(G)$, then $h^{(t)}(\cdot)$ must be small.

\begin{lemma}[\textbf{c.f. Theorem 1.2 of~\citep{lovasz1990}}]
	\label{lem:mixing_random_walk}
	For any $K_0 \in [0,m]$, $k \in [K_0, 2m - K_0]$, $t_0 \in \mathbb{N}$ and $t \geq t_0$,
	\begin{equation}
	\label{eqn:mixing_random_walk}
	h^{(t)}(k) \leq L_{K_0}(k;h^{(t_0)}) + C(K_0; h^{(t_0)}) \sqrt{\wb{k}} \cdot \Bigl(1 - \frac{\Psi(G)^2}{8}\Bigr)^{t - t_0}
	\end{equation}
\end{lemma}

Lemma~\ref{lem:mixing_time_PPR} implies that if the PPR random walk is not well mixed, then some sweep cut of $p_v$ must have small normalized cut.

\begin{lemma}[\textbf{c.f Theorem~3 of \citep{andersen2006}}]
	\label{lem:mixing_time_PPR}
	Let $\phi \in [0,1]$. Either the following bound holds for any $t \in \mathbb{N}$, any $K_0 \in [0,m]$, and any $k \in [K_0,2m - K_0]$:
	\begin{equation}
	\label{eqn:mixing_time_PPR}
	h^{(\alpha)}(k) \leq \alpha t + L_{K_0}(k; h^{(\alpha)}) + C(K_0;h^{(\alpha)})\sqrt{\overline{k}}\left(1 - \frac{\phi^2}{8}\right)^t
	\end{equation}
	or else there exists some sweep cut $S_{j}$ of $p_v$ such that $\Phi(S_j;G) < \phi$.
\end{lemma}

In order to make use of these Lemmas, we require upper bounds on $L_{K_0}(\cdot,h)$ and $C(K_0;h)$, for each of $h = h^{(t_0)}$ and $h = h^{(\alpha)}$. Of course, trivially $L_{K_0}(k;h) \leq \max\{h(K_0); h(2m - K_0)\}$ for any $k \in [K_0, 2m - K_0]$. As it happens, this observation will lead to sufficient upper bounds on $L_{K_0}(k,h)$ for both $h = h^{(t_0)}$ (Lemma~\ref{lem:interpolator_bound_rw}) and $h = h^{(\alpha)}$ (Lemma~\ref{lem:interpolator_bound_ppr}).  
\begin{lemma}
	\label{lem:interpolator_bound_rw}
	For any $t_0 \in \mathbb{N}$ and $K_0 \in [0,m]$, the following inequalities hold:
	\begin{equation}
	\label{eqn:interpolator_bound_rw}
	h^{(t_0)}\bigl(2m - K_0\bigr) \leq \frac{K_0}{2m}~~\textrm{and}~~h^{(t_0)}\bigl(K_0\bigr) \leq \frac{K_0}{d_{\min}(G)^2} + \frac{1}{2^{t_0}}.
	\end{equation}
	As a result, for any $k \in [K_0, 2m - K_0]$,
	\begin{equation}
	\label{eqn:interpolator_bound_rw_2}
	L_{K_0}(k;h^{(t_0)}) \leq \max\Bigl\{\frac{K_0}{2m}, \frac{K_0}{d_{\min}(G)^2} + \frac{1}{2^{t_0}}\Bigr\} = \frac{K_0}{d_{\min}(G)^2} + \frac{1}{2^{t_0}}.
	\end{equation}
\end{lemma}

\begin{lemma}
	\label{lem:interpolator_bound_ppr}
	For any $\alpha \in [0,1]$ and $K_0 \in [0,m]$, the following inequalities hold:
	\begin{equation}
	\label{eqn:interpolator_bound_ppr}
	h^{(\alpha)}\bigl(2m - K_0\bigr) \leq \frac{K_0}{2m}~~\textrm{and}~~h^{(\alpha)}\bigl(K_0\bigr) \leq \frac{K_0}{d_{\min}(G)^2} + \frac{2\alpha}{1 + \alpha}.
	\end{equation}
	As a result, for any $k \in [K_0, 2m - K_0]$,
	\begin{equation}
	\label{eqn:interpolator_bound_ppr_2}
	L_{K_0}(k;h^{(\alpha)}) \leq \max\Bigl\{\frac{K_0}{2m}, \frac{K_0}{d_{\min}(G)^2} + \frac{2\alpha}{1 + \alpha}\Bigr\} = \frac{K_0}{d_{\min}(G)^2} + \frac{2\alpha}{1 + \alpha}.
	\end{equation}
\end{lemma}

We next establish an upper bound on $C_{K_0}(k;h)$, which rests on the following key observation: since $h(k)$ is concave and $L_{K_0}(K_0;h) = h(K_0)$, it holds that
\begin{equation}
\label{pf:linearization_bound_1}
\frac{h(k) - L_{K_0}(k)}{\sqrt{\overline{k}}} \leq
\begin{cases}
h'(K_0) \sqrt{k},~& k \leq m \\
-h'(2m - K_0) \sqrt{2m - k},~& k > m.
\end{cases}
\end{equation}
(Since $h$ is not differentiable at $k = k_j$, here $h'$ refers to the right derivative of $h$.)  

Lemma~\ref{lem:linearization_bound} gives good estimates for $h'(K_0)$ and $h'(2m - K_0)$, which hold for both $h = h^{(t_0)}$ and $h = h^{(\alpha)}$, and result in an upper bound on $C(K_0;h)$. Both the statement and proof of this Lemma rely on the following explicit representation of the Lovasz-Simonovits curve $h_q(\cdot)$. Order the vertices $q(u_{(1)})/\deg(u_{(1)};G) \geq q(u_{(2)})/\deg(u_{(2)};G) \geq \cdots \geq q(u_{(n)})/\deg(u_{(n)};G)$. Then for each $j = 0,\ldots,n - 1$, and for all $k \in [\vol(S_j),\vol(S_{j + 1}))$,  the function $h_q(k)$ satisfies
\begin{equation}
\label{eqn:lovasz_simonovits}
h_q(k) = \sum_{i = 0}^{j} \left(q(u_{(i)}) - \pi(u_{(i)})\right) + \frac{\bigl(k - \vol(S_j;G)\bigr)}{\deg(u_{(j + 1)};G)} \left(q(u_{(j+1)}) - \pi(u_{(j+1)})\right). 
\end{equation}
\begin{lemma}
	\label{lem:linearization_bound}
	The following statements hold for both $h = h^{(\alpha)}$ and $h = h^{(t_0)}$. 
	\begin{itemize}
		\item Let $K_0 = k_1 = \deg(v;G)$ if $u_{(1)} = v$, and otherwise $K = 0$. Then  
		\begin{equation}
		\label{eqn:right_derivative_1}
		h'\bigl(K_0\bigr) \leq \frac{1}{d_{\min}(G)^2}.
		\end{equation}
		\item For all $K_0 \in [0,m]$,
		\begin{equation}
		\label{eqn:right_derivative_2}
		h'(2m - K_0) \geq -\frac{d_{\max}(G)}{d_{\min}(G)\cdot \vol(G)}.
		\end{equation}
	\end{itemize}
	As a result, letting $K_0 = \deg(v;G)$ if $u_{(1)} = v$, and otherwise letting $K_0 = 0$, we have
	\begin{equation*}
	C(K_0,h) \leq \frac{\sqrt{m}}{d_{\min}(G)^2}.
	\end{equation*}
\end{lemma}
\paragraph{Proof of Theorems~\ref{thm:mixing_time_rw} and~\ref{thm:mixing_time_PPR}.}
\begin{proof}[Proof (of Theorem~\ref{thm:mixing_time_rw})]
	Take $K_0 = 0$ if $u_{(1)} \neq v$, and otherwise take $K_0 = \deg(v;G)$. Combining Lemmas~\ref{lem:mixing_random_walk},~\ref{lem:interpolator_bound_rw} and~\ref{lem:linearization_bound}, we obtain that for any $k \in [K_0,2m - K_0]$, 
	\begin{align*}
	h^{(t)}(k) & \leq \frac{1}{2^{t_0}} + \frac{K_0}{d_{\min}(G)^2}  + \frac{\sqrt{m}}{d_{\min}(G)^2} \sqrt{\wb{k}} \Bigl(1 - \frac{\Psi^2(G)}{8}\Bigr)^{t-t_0} \\
	& \leq \frac{1}{2^{t_0}} + \frac{d_{\max}(G)}{d_{\min}(G)^2}  + \frac{m}{d_{\min}(G)^2} \Bigl(1 - \frac{\Psi^2(G)}{8}\Bigr)^{t-t_0},
	\end{align*}
	where the second inequality follows since we have chosen $K_0 \leq d_{\max}(G)$, and since $\wb{k} \leq m$. If $K_0 = 0$, we are done. 
	
	Otherwise, we must still establish that~\eqref{eqn:mixing_random_walk} is a valid upper bound when $k \in [0, \deg(v;G)) ~\cup~ (2m - \deg(v;G),2m]$. If $k \in [0, \deg(v;G))$ then
	\begin{equation}
	\label{pf:mixing_random_walk_1}
	h^{(t)}(k) \overset{\eqref{pf:mixing_random_walk_inductive_step}}{\leq} h^{(t_0)}(k) \overset{(\textrm{i})}{\leq} h^{(t_0)}(K_0) \overset{\eqref{eqn:interpolator_bound_rw}}{\leq} \frac{K_0}{d_{\min}(G)^2} + \frac{1}{2^{t_0}},
	\end{equation}
	where $(\textrm{i})$ follows since $k \in [0,K_0]$, and $h^{(t_0)}$ is linear over $[0,K_0)$ with $h^{(t_0)}(0) = 0$ and $h^{(t_0)}(K_0) \geq 0$. For similar reasons,
	\begin{equation}
	\label{pf:mixing_random_walk_2}
	h^{(t)}(k) \leq h^{(t_0)}(k) \leq h^{(t_0)}(2m - K_0) \leq \frac{\deg(v;G)}{2m}.
	\end{equation}
	Since the ultimate upper bounds in~\eqref{pf:mixing_random_walk_1} and~\eqref{pf:mixing_random_walk_2} are each no greater than that of~\eqref{eqn:mixing_random_walk}, the claim follows.
\end{proof}

\begin{proof}[Proof (of Theorem~\ref{thm:mixing_time_PPR})]
	The proof of Theorem~\ref{thm:mixing_time_PPR} follows immediately from Lemmas~\ref{lem:mixing_time_PPR},~\ref{lem:interpolator_bound_ppr} and~\ref{lem:linearization_bound}, taking $K_0 = 0$ if $u_{(1)} \neq v$ and otherwise $K_0 = \deg(v;G)$. 
\end{proof}

\paragraph{Proofs of Lemmas.}
In what follows, for a distribution $q$ and vertices $u,w \in V$, we write $q(u,w) := q(u)/d(u) \cdot 1\{(u,w) \in E\}$, and similarly for a collection of dyads $\wt{E} \subseteq V \times V$ we write $q(\wt{E}) := \sum_{(u,w) \in \wt{E}} q(u,w)$. 
\begin{proof}[Proof (of Lemma~\ref{lem:mixing_random_walk})]
	 We will prove Lemma~\ref{lem:mixing_random_walk} by induction on $t$. In the base case $t = t_0$, observe that $C(K_0;h^{(t_0)}) \cdot \sqrt{\wb{k}} \geq h^{(t_0)}(k) - L_{K_0}(k;h^{(t_0)})$ for all $k \in [K_0, 2m - K_0]$, which implies 
	\begin{equation*}
	L_{K_0}(k;h^{(t_0)}) + C(K_0; h^{(t_0)}) \cdot \sqrt{\wb{k}} \geq h^{(t_0)}(k).
	\end{equation*}
	
	Now, we proceed with the inductive step, assuming that the inequality holds for $t_0,t_0 + 1,\ldots,t - 1$, and proving that it thus also holds for $t$. By the definition of $L_{K_0}$, the inequality~\eqref{eqn:mixing_random_walk} holds when $k = K_0$ or $k = 2m - K_0$. We will additionally show that~\eqref{eqn:mixing_random_walk} holds for every $k_j = \vol(S_j), j = 1,2,\ldots,n$ such that $k_j \in [K_0, 2m - K_0]$. This suffices to show that the inequality~\eqref{eqn:mixing_random_walk} holds for all $k \in [K_0,2m - K_0]$, since the right hand side of~\eqref{eqn:mixing_random_walk} is a concave function of $k$.
	
	Now, we claim that for each $k_j$, it holds that
	\begin{equation}
	\label{pf:mixing_random_walk_inductive_step}
	q_v^{(t)}[k_j] \leq \frac{1}{2}\Bigl(q_v^{(t - 1)}[k_j - \wb{k}_j \Psi(G)] + q_v^{(t - 1)}[k_j + \wb{k}_j \Psi(G)]\Bigr).
	\end{equation}
	To establish this claim, we note that for any $u \in V$
	\begin{equation*}
	q_v^{(t)}(u) = \frac{1}{2}q_v^{(t - 1)}(u) + \frac{1}{2}\sum_{w \in V}q_v^{(t - 1)}(w,u) = \frac{1}{2} \sum_{w \in V} \bigl(q_v^{(t - 1)}(u,w) + q_v^{(t - 1)}(w,u)\bigr),
	\end{equation*}
	and consequentially for any $S \subset V$,
	\begin{align*}
	q_v^{(t)}(S) & = \frac{1}{2}\bigl\{q_v^{(t - 1)}(\mathrm{in}(S)) +  q_v^{(t - 1)}(\mathrm{out}(S))\bigr\} \\
	& = \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl(\mathrm{in}(S) \cup \mathrm{out}(S)\bigr) +  q_v^{(t - 1)}\bigl(\mathrm{in}(S) \cap \mathrm{out}(S)\bigr)\bigr\}
	\end{align*}
	where $\mathrm{in}(S) = \{(u,w) \in E: u \in S\}$ and $\mathrm{out}(S) = \{(w,u) \in E: w \in S\}$. We deduce that
	\begin{align*}
	q_v^{(t)}[k_j] = q_v^{(t)}(S_j) & = \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl(\mathrm{in}(S_j) \cup \mathrm{out}(S_j)\bigr) +  q_v^{(t - 1)}\bigl(\mathrm{in}(S_j) \cap \mathrm{out}(S_j)\bigr)\bigr\} \\
	& \leq \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl[|\mathrm{in}(S_j) \cup \mathrm{out}(S_j)|\bigr] +  q_v^{(t - 1)}\bigl[|\mathrm{in}(S_j) \cap \mathrm{out}(S_j)|\bigr]\bigr\} \\
	& = \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl[k_j + \mathrm{cut}(S_j;G)\bigr] +  q_v^{(t - 1)}\bigl[k_j - \mathrm{cut}(S_j;G)\bigr]\bigr\} \\
	& \leq \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl[k_j + \wb{k}_j\Phi(S_j;G)\bigr] +  q_v^{(t - 1)}\bigl[k_j - \wb{k}_j\Phi(S_j;G)\bigr]\bigr\} \\
	& \leq \frac{1}{2}\bigl\{q_v^{(t - 1)}\bigl[k_j + \wb{k}_j\Psi(G)\bigr] +  q_v^{(t - 1)}\bigl[k_j - \wb{k}_j\Psi(G)\bigr]\bigr\},
	\end{align*}
	establishing~\eqref{pf:mixing_random_walk_inductive_step}. The final two inequalities both follow from the concavity of $q_v^{(t)}[\cdot]$. 
	
	Subtracting $k_j/2m$ from both sides, we get
	\begin{equation}
	\label{pf:mixing_random_walk_inductive_step_h}
	h^{(t)}(k_j) \leq \frac{1}{2}\bigl\{h^{(t - 1)}\bigl(k_j + \wb{k}_j\Psi(G)\bigr)+  h^{(t - 1)}\bigl(k_j - \wb{k}_j\Psi(G)\bigr)\bigr\}.
	\end{equation}
	At this point, we divide our analysis into cases.
	
	\textbf{Case 1.}
	Assume $k_j - \Psi(G) \overline{k}_j$ and $k_j + 2 \Psi(G) \overline{k}_j$ are both in $[K_0,2m  - K_0]$. We are therefore in a position to apply our inductive hypothesis to both terms on the right hand side of~\eqref{pf:mixing_random_walk_inductive_step_h} and obtain the following:
	\begin{align*}
	h^{(t)}(k_j) & \leq \frac{1}{2}\biggl(L_{K_0}\bigl(k_j - \Psi(G) \overline{k}_j; h^{(t_0)}\bigr) + L_{K_0}\bigl(k_j + \Psi(G) \overline{k}_j; h^{(t_0)}\bigr) \biggr) ~~+ \\
	& \quad~ \frac{1}{2}C\bigl(K_0; h^{(t_0)}\bigr) \cdot \Bigl(\sqrt{\overline{k_j - \Psi(G) \overline{k}_j}} + \sqrt{\overline{k_j + \Psi(G) \overline{k}_j}}\Bigr)\left(1 - \frac{\Psi(G)^2}{8}\right)^{t-t_0 - 1} \\
	& = L_{K_0}(k;h^{(t_0)}) + \frac{1}{2}\biggl(C(K_0;h^{t_0})\bigl(\sqrt{\overline{k_j - \Psi(G) \overline{k}_j}} + \sqrt{\overline{k_j + \Psi(G) \overline{k}_j}}\bigr)\left(1 - \frac{\Psi(G)^2}{8}\right)^{t-t_0 - 1} \biggr) \\
	& \leq L_{K_0}(k;h^{(t_0)}) + \frac{1}{2}\biggl(C(K_0;h^{(t_0)})\bigl(\sqrt{\overline{k}_j - \Psi(G) \overline{k}_j} + \sqrt{\overline{k}_j + \Psi(G) \overline{k}_j}\bigr)\left(1 - \frac{\Psi(G)^2}{8}\right)^{t-t_0 - 1} \biggr).
	\end{align*}
	A Taylor expansion of $\sqrt{1 + \Psi(G)}$ around $\Psi(G) = 0$ yields the following bound:
	\begin{equation*}
	\sqrt{1 + \Psi(G)} + \sqrt{1 - \Psi(G)} \leq 2 - \frac{\Psi(G)^2}{4},
	\end{equation*}
	and therefore
	\begin{align*}
	h^{(t)}(k_j) & \leq L_{K_0}(k;h^{(t_0)}) + \frac{C(K_0;h^{(t_0)})}{2}\cdot \sqrt{\overline{k}_j}\cdot\left(2 - \frac{\Psi(G)^2}{4}\right)\left(1 - \frac{\Psi(G)^2}{8}\right)^{t-1} \\
	&= L_{K_0}(k_j;h^{(t_0)}) + C(K_0;h^{(t_0)})\sqrt{\overline{k}_j}\left(1 - \frac{\Psi(G)^2}{8}\right)^{t - t_0}.
	\end{align*}
	
	\textbf{Case 2.} Otherwise one of $k_j - 2 \Psi(G) \overline{k}_j$ or $k_j + 2 \Psi(G) \overline{k}_j$ is not in $[K_0,2m  - K_0]$. Without loss of generality assume $k_j < m$, so that (i) we have $k_j - 2 \Psi(G) \overline{k}_j < K_0$ and (ii) $k_j + (k_j - K_0) \leq 2m - K_0$. We deduce the following:
	\begin{align*}
	h^{(t)}(k_j) & \overset{(\textrm{i})}{\leq} \frac{1}{2}\Bigl(h^{(t - 1)}(K_0) + h^{(t - 1)}\bigl(k_j + (k_j - K_0)\bigr)\Bigr) \\
	& \overset{(\textrm{ii})}{\leq} \frac{1}{2}\Bigl(h^{(t_0)}(K_0) + h^{(t)}\bigl(k_j + (k_j - K_0)\bigr)\Bigr) \\
	& \overset{(\textrm{iii})}{\leq}\frac{1}{2}\Bigl(L_{K_0}(K_0;h^{(t_0)}) + L_{K_0}(2k_j - K_0; h^{(t_0)}\bigr) + C(K_0;h^{(t_0)})\sqrt{\overline{2k_j - K_0}}\left(1 - \frac{\Psi(G)^2}{8}\right)^{t - t_0 - 1}\Bigr) \\
	& \leq L_{K_0}(k_j; h^{(t_0)}) + C(K_0;h^{(t_0)}) \frac{\sqrt{2\overline{k}_j}}{2} \left(1 - \frac{\Psi(G)^2}{8}\right)^{t - t_0 - 1} \\
	& \leq L_{K_0}(k_j;h^{(t_0)}) + C(K_0;h^{(t_0)}) \sqrt{\overline{k}_j} \cdot \left(1 - \frac{\Psi(G)^2}{8}\right)^{t - t_0}
	\end{align*}
	where $(\textrm{(i)})$ follows from~\eqref{pf:mixing_random_walk_inductive_step_h} and the concavity of $h^{(t - 1)}$,  we deduce $(\textrm{ii})$ from~\eqref{pf:mixing_random_walk_inductive_step_h}, which implies that $h^{(t)}(k) \leq h^{(t_0)}(k)$, and $(\textrm{iii})$ follows from applying the inductive hypothesis to $h^{(t - 1)}(2k_j - K_0)$. 	
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:mixing_time_PPR}).]
	We will show that if $\Phi(S_j; g) \geq \phi$ for each $j = 1,\ldots,n$, then \eqref{eqn:mixing_time_PPR} holds for all $t$ and any $k \in [K_0,2m - K_0]$.
	
	We proceed by induction on $t$. Our base case will be $t = 0$. Observe that $C(K_0;h^{(\alpha)}) \cdot \sqrt{\overline{k}} \geq h^{(\alpha)}(k) - L_{K_0}(k;h^{(\alpha)})$ for all $k \in [K_0,2m - K_0]$, which implies
	\begin{equation*}
	L_{K_0}(k;h^{(\alpha)}) + C(K_0;h^{(\alpha)}) \cdot \sqrt{\overline{k}} \geq h^{(\alpha)}(k).
	\end{equation*}
	
	Now, we proceed with the inductive step. By the definition of $L_{K_0}$, the inequality~\eqref{eqn:mixing_time_PPR} holds when $k = K_0$ or $k = 2m - K_0$. We will additionally show that~\eqref{eqn:mixing_time_PPR} holds for every $k_j = \vol(S_j), j = 1,2,\ldots,n$ such that $k_j \in [K_0, 2m - K_0]$. This suffices to show that the inequality~\eqref{eqn:mixing_time_PPR} holds for all $k \in [K_0,2m - K_0]$, since the right hand side of~\eqref{eqn:mixing_time_PPR} is a concave function of $k$.
	
	By Lemma 5 of \citet{andersen2006}, we have that
	\begin{align}
	p_v[k_j] & \leq \alpha + \frac{1}{2} \bigl(p_v[k_j - \mathrm{cut}(S_j;G)] + p_v[k_j + \mathrm{cut}(S_j;G)] \bigr) \nonumber\\
	& \leq \alpha + \frac{1}{2} \bigl(p_v[k_j - \Phi(S_j;G) \overline{k}_j] + p_v[k_j + \Phi(S_j;G) \overline{k}_j]  \bigr) \nonumber \\
	& \leq \alpha + \frac{1}{2} \bigl(p_v[k_j - \phi \overline{k}_j] + p_v[k_j + \phi \overline{k}_j]\bigr) \nonumber
	\end{align}
	and subtracting $k_j/2m$ from both sides, we get
	\begin{equation}
	\label{eqn:mixing_time_PPR_pf1}
	h^{(\alpha)}(k_j) \leq \alpha + \frac{1}{2} \bigl(h^{(\alpha)}(k_j - \phi \overline{k}_j) + h^{(\alpha)}(k_j +  \phi \overline{k}_j) \bigr)
	\end{equation}
	From this point, we divide our analysis into cases. 
	
	\textbf{Case 1.}
	Assume $k_j - 2 \phi \overline{k}_j$ and $k_j + 2 \phi \overline{k}_j$ are both in $[K_0,2m  - K_0]$. We are therefore in a position to apply our inductive hypothesis to \eqref{eqn:mixing_time_PPR_pf1}, yielding
	\begin{align*}
	h^{(\alpha)}(k_j) & \leq \alpha + \alpha(t-1) \frac{1}{2}\biggl(L_{K_0}(k_j - \phi \overline{k}_j) + L_{K_0}(k_j + \phi \overline{k}_j) + C(K_0;h^{(\alpha)})\bigl(\sqrt{\overline{k_j - \phi \overline{k}_j}} + \sqrt{\overline{k_j + \phi \overline{k}_j}}\bigr)\left(1 - \frac{\phi^2}{8}\right)^{t-1} \biggr) \\
	& \leq \alpha t + L_{K_0}(k;h^{(\alpha)}) + \frac{1}{2}\biggl(C(K_0;h^{(\alpha)})\bigl(\sqrt{\overline{k_j - \phi \overline{k}_j}} + \sqrt{\overline{k_j + \phi \overline{k}_j}}\bigr)\left(1 - \frac{\phi^2}{8}\right)^{t-1} \biggr) \\
	& \leq \alpha t + L_{K_0}(k;h^{(\alpha)}) + \frac{1}{2}\biggl(C(K_0;h^{(\alpha)})\bigl(\sqrt{\overline{k}_j - \phi \overline{k}_j} + \sqrt{\overline{k}_j + \phi \overline{k}_j}\bigr)\left(1 - \frac{\phi^2}{8}\right)^{t-1} \biggr).
	\end{align*}
	and therefore
	\begin{equation*}
	h^{(\alpha)}(k_j) \leq  \alpha t + L_{K_0}(k;h^{(\alpha)}) + \frac{C(K_0;h^{(\alpha)})}{2}\cdot \sqrt{\overline{k}_j}\cdot\left(2 - \frac{\phi^2}{4}\right)\left(1 - \frac{\phi^2}{8}\right)^{t-1} = \alpha t + L_{K_0}(k;h^{(\alpha)}) + C(K_0;h^{(\alpha)})\sqrt{\overline{k}_j}\left(1 - \frac{\phi^2}{8}\right)^{t}.
	\end{equation*}
	
	\textbf{Case 2.} Otherwise one of $k_j - 2 \phi \overline{k}_j$ or $k_j + 2 \phi \overline{k}_j$ is not in $[K_0,2m  - K_0]$. Without loss of generality assume $k_j < m$, so that (i) we have $k_j - 2 \phi \overline{k}_j < K_0$ and (ii) $k_j + (k_j - K_0) \leq 2m - K_0$. By the concavity of $h$, and applying the inductive hypothesis to $h^{(\alpha)}2k_j - K_0)$, we have
	\begin{align*}
	h^{(\alpha)}(k_j) & \leq \alpha + \frac{1}{2}\Bigl(h^{(\alpha)}(K_0) + h\bigl(k_j + (k_j - K_0)\bigr)\Bigr) \\
	& \leq\alpha + \frac{\alpha(t - 1)}{2} + \frac{1}{2}\Bigl(L_{K_0}(K_0;p^{\alpha}) + L_{K_0}(2k_j - K_0\bigr) + C(K_0;h^{(\alpha)})\sqrt{\overline{2k_j - K_0}}\left(1 - \frac{\phi^2}{8}\right)^{t - 1}\Bigr) \\
	& \leq \alpha t + L_{K_0}(k_j) + C(K_0;h^{(\alpha)}) \frac{\sqrt{2\overline{k}_j}}{2} \left(1 - \frac{\phi^2}{8}\right)^{t - 1} \\
	& \leq \alpha t + L_{K_0}(k_j) + C(K_0;h^{(\alpha)}) \sqrt{\overline{k}_j} \cdot \left(1 - \frac{\phi^2}{8}\right)^{t}
	\end{align*}
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:interpolator_bound_rw})]
	We will prove that the inequalities of~\eqref{eqn:interpolator_bound_rw} hold at the knot points of $h^{(t_0)}$, whence they follow for all $K_0 \in [0,m]$. 
	
	We first prove the upper bound on $h^{(t_0)}(2m - K_0)$, when $2m - K_0 = k_j$ for some $j = 0,\ldots,n - 1$. Indeed, the following manipulations show the upper bound holds for $h_q(\cdot)$ regardless of the distribution $q$. Noting that $h_q(2m) = 0$, we have that,
	\begin{equation*}
	h_q(k_j) = h_q(k_j) - h_q(2m) = \sum_{i = j + 1}^{n} q(u_{(i)}) - \pi(u_{(i)}) \leq \sum_{i = j + 1}^{n} \pi(u_{(i)}) = 1 - \frac{k_j}{2m} = \frac{K_0}{2m}.
	\end{equation*}
	
	In contrast, when $K_0 = k_j$ the upper bound on $h^{(t_0)}(\cdot)$ depends on the properties of $q = q_v^{(t_0)}$. In particular, we claim that for any $t \in \mathbb{N}$,
	\begin{equation}
	\label{pf:interpolator_bound_max_entry}
	q_v^{(t)}(u) \leq
	\begin{dcases*}
	\frac{1}{d_{\min}(G)},& ~~\textrm{if $u \neq v$} \\
	\frac{1}{d_{\min}(G)} + \frac{1}{2^t},& ~~\textrm{if $u = v$.}
	\end{dcases*}
	\end{equation}
	This claim follows straightforwardly by induction. In the base case $t = 0$, the claim is obvious. If the claim holds true for a given $t \in \mathbb{N}$, then for $u \neq v$,
	\begin{equation}
	\label{pf:interpolator_bound_max_entry_inductive_step}
	\begin{aligned}
	q_v^{(t + 1)}(u) & = \frac{1}{2}\sum_{w \neq u}q_v^{(t)}(w,u)  + \frac{1}{2}q_v^{(t)}(u) \\
	& \leq \frac{1}{2d_{\min}(G)}\sum_{w \neq u}q_v^{(t)}(w)  + \frac{1}{2d_{\min}(G)} \\
	& \leq \frac{1}{d_{\min}(G)},
	\end{aligned}
	\end{equation}
	where the last inequality holds because $q_v^{(t)}$ is a probability distribution (i.e. the sum of its entries is equal to $1$). Similarly, if $u = v$, then
	\begin{align*}
	q_v^{(t + 1)}(v) & = \frac{1}{2}\sum_{w \neq v}q_v^{(t)}(w,v)  + \frac{1}{2}q_v^{(t)}(v) \\
	& \leq \frac{1}{2d_{\min}(G)}\sum_{w \neq u}q_v^{(t)}(w)  + \frac{1}{2d_{\min(G)}} + \frac{1}{2^{t + 1}} \\
	& \leq \frac{1}{d_{\min}(G)} + \frac{1}{2^{t + 1}},
	\end{align*}
	and the claim~\eqref{pf:interpolator_bound_max_entry} is shown. The upper bound on $h^{(t_0)}(K_0)$ for $K_0 = k_j$ follows straightforwardly:
	\begin{equation*}
	h^{(t_0)}(K_0) \leq \sum_{i = 0}^{j} q_v^{(t_0)}(u_{(j)}) \leq \frac{j}{d_{\min}(G)} + \frac{1}{2^{t_0}} \leq \frac{K_0}{d_{\min}(G)^2} + \frac{1}{2^{t_0}},
	\end{equation*}
	where the last inequality follows since $\vol(S) \geq |S| \cdot d_{\min}(G)$ for any set $S \subseteq V$. 
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:interpolator_bound_ppr})]
	We have already established the first upper bound in~\eqref{eqn:interpolator_bound_ppr}, in the proof of Lemma~\ref{lem:interpolator_bound_rw}. Then, noting that from~\eqref{pf:interpolator_bound_max_entry}, 
	\begin{equation}
	\label{eqn:ppr_max_entry}
	p_v(u) = \alpha \sum_{t = 0}^{\infty} (1 - \alpha)^t q_v^{(t)}(u) \leq
	\begin{dcases*}
	\alpha \sum_{t = 0}^{\infty} (1 - \alpha)^t \Bigl(\frac{1}{d_{\min}(G)} + \frac{1}{2^t}\Bigr) = \frac{1}{d_{\min}(G)} + \frac{2\alpha}{1 - \alpha}& ~~\textrm{if $u = v$} \\
	\alpha \sum_{t = 0}^{\infty} (1 - \alpha)^t \frac{1}{d_{\min}(G)} = \frac{1}{d_{\min}(G)} & ~~\textrm{if $u \neq v$,}
	\end{dcases*}
	\end{equation}
	the second upper bound in~\eqref{eqn:interpolator_bound_ppr} follows similarly to the proof of the equivalent upper bound in~Lemma~\ref{lem:interpolator_bound_rw}.
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:linearization_bound}).]
	The result of the Lemma follows obviously from~\eqref{pf:linearization_bound_1}, once we show \eqref{eqn:right_derivative_1}-\eqref{eqn:right_derivative_2}. We begin by showing~\eqref{eqn:right_derivative_1}. Inspecting the representation~\eqref{eqn:lovasz_simonovits}, we see that for any distribution $q$ and knot point $k_j$, the right derivative of $h_q$ can always be upper bounded,
	\begin{equation*}
	h_{q}'(k_j) \leq \frac{q(u_{(j + 1)})}{\deg(u_{(j + 1)};G)}.
	\end{equation*}
	Thus, as long as $u_{(j + 1)} \neq v$ we have from~\eqref{pf:interpolator_bound_max_entry} that---for either $q = q_v^{(t)}$ or $q = p_v$---the inequality $h_{q}'(k_j) \leq 1/(d_{\min}(G)^2)$ holds. The statement of~\eqref{eqn:right_derivative_1} guarantees that $u_{(j + 1)} \neq v$. 
	
	On the other hand, the inequality \eqref{eqn:right_derivative_2} follows immediately from the representation \eqref{eqn:lovasz_simonovits}, since for any $K \in [0,m]$, taking $j$ so that $2m - K \in [k_j, k_{j + 1})$, 
	\begin{equation*}
	h'(2m - K) \geq -\frac{\pi(u_{(j+1)})}{\mathrm{deg}(u_{(j + 1)};G)} \geq -\frac{d_{\max}(G)}{d_{\min}(G) \cdot \vol(G)}.
	\end{equation*}
\end{proof}

\subsection{Proof of Proposition~\ref{prop:pointwise_mixing_time}}
\label{subsec:pf_prop_pointwise_mixing_time}
To prove Proposition~\ref{prop:pointwise_mixing_time}, we will upgrade from an upper bound on the total variation distance between $q_v^{(t)}$ and $\pi$ to the desired uniform upper bound. The \emph{total variation distance} between distributions $q$ and $p$ is
\begin{equation*}
\mathrm{TV}(q,p) := \frac{1}{2}\sum_{u \in v} \bigl|q(u) - p(u)\bigr|
\end{equation*}
It follows from the representation~\eqref{eqn:lovasz_simonovits} that
\begin{equation*}
\mathrm{TV}(q,\pi) = \max_{S \subseteq V} \Bigl\{q(S) - \pi(S)\Bigr\} = \max_{j = 1,\ldots,n} \Bigl\{q(S_j) - \pi(S_j)\Bigr\} = \max_{k \in [0,2m]} h_q(k),
\end{equation*}
so that Theorem~\ref{thm:mixing_time_rw} gives an upper bound on $\mathrm{TV}(q_v^{(t)},\pi)$. We can then use the following result to upgrade to a uniform upper bound.

\begin{lemma}
	\label{lem:tv_to_pointwise}
	For any $t \in \mathbb{N}$,
	\begin{equation*}
	\max_{u \in V} \Bigl\{\frac{\pi(u) - q_v^{(t + 1)}(u)}{\pi(u)}\Bigr\} \leq \frac{2 \cdot \mathrm{TV}(q_v^{(t)}, \pi)}{s(G)}.
	\end{equation*}
\end{lemma}

The proof of Proposition~\ref{prop:pointwise_mixing_time} is then straightforward.
\begin{proof}[Proof (of Proposition~\ref{prop:pointwise_mixing_time})]
	Put $t_{\ast} = 8/(\Psi(G)^2) \ln(4/s(G)) + 4$. We will use Theorem~\ref{thm:mixing_time_rw} to show that $\mathrm{TV}(q_v^{(t_{\ast})},\pi) \leq 1/4$. This will in turn imply (\cite{montenegro2002} pg. 13) that for $\tau_{\ast} = t_{\ast} \log_2(8/s(G))$,
	\begin{equation*}
	\mathrm{TV}(q_v^{(\tau_{\ast})},\pi) \leq \frac{1}{8}s(G),
	\end{equation*}
	and applying Lemma~\ref{lem:tv_to_pointwise} gives
	\begin{equation*}
	\max_{u \in V} \Bigl\{\frac{\pi(u) - q_v^{(\tau_{\ast} + 1)}(u)}{\pi(u)}\Bigr\} \leq \frac{1}{4}.
	\end{equation*}
	Taking maximum over all $v \in V$, we conclude that $\tau_{\infty}(G) \leq \tau_{\ast} + 1$, which is exactly the claim of Proposition~\ref{prop:pointwise_mixing_time}.
	
	It remains to show that $\mathrm{TV}(q_v^{(t_{\ast})},\pi) \leq 1/4$. Choosing $t_0 = 4$ in the statement of Theorem~\ref{thm:mixing_time_rw}, we have that
	\begin{align*}
	\mathrm{TV}(q_v^{(t_{\ast})},\pi) & \leq \frac{1}{16} + \frac{d_{\max}(G)}{d_{\min}(G)^2} + \frac{1}{2s(G)} \Bigl(1 - \frac{\Psi(G)^2}{8}\Bigr)^{t_{\ast} - 4} \\
	& \leq \frac{1}{8} + \frac{1}{2s(G)} \Bigl(1 - \frac{\Psi(G)^2}{8}\Bigr)^{t_{\ast} - 4} \\
	& \leq \frac{1}{8} + \frac{1}{2s(G)} \exp\Bigl(-\frac{\Psi(G)^2}{8}(t_{\ast} - 4)\Bigr) = \frac{1}{4},
	\end{align*}
	where the middle inequality follows by assumption.
\end{proof}

\begin{proof}[Proof (of Lemma~\ref{lem:tv_to_pointwise})]
	We proceed by induction. In the base case $t = 0$, we have that
	\begin{equation*}
	\max_{u \in V} \Bigl\{\frac{\pi(u) - q_v^{(t + 1)}(u)}{\pi(u)}\Bigr\} \leq 1 \leq 2(1 - \pi(v)) \leq 2\frac{\mathrm{TV}(q_v^{(0)},\pi)}{s(G)},
	\end{equation*},
	where the second inequality follows since $\pi(v) \leq d_{\max}(G)/(2m) \leq d_{\max}(G)/d_{\min}(G)^2 \leq 1/16$. 
	
	To prove the inductive step, the key observation is the following equivalence (see equation (16) of \citep{morris2005}):
	\begin{align}
	\label{pf:tv_to_pointwise_1}
	\frac{\pi(u) - q_v^{(t + 1)}(u)}{\pi(u)} & = \sum_{w \in V} \bigl(\pi(w) - q_v^{(t)}(w) \bigr) \cdot \Bigl(\frac{q_w^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr) \nonumber \\
	& = \sum_{w \neq u} \bigl(\pi(w) - q_v^{(t)}(w) \bigr) \cdot \Bigl(\frac{q_w^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr) + \bigl(\pi(u) - q_v^{(t)}(u) \bigr) \cdot \Bigl(\frac{q_u^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr)
	\end{align}
	We separately upper bound each term on the right hand side of~\eqref{pf:tv_to_pointwise_1}. The sum over all $w \neq u$ can be related to the TV distance between $q_v^{(t)}$ and $\pi$ using H{\"o}lder's inequality,
	\begin{align*}
	\sum_{w \neq u} \bigl(\pi(w) - q_v^{(t)}(w) \bigr) \cdot \Bigl(\frac{q_w^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr) & \leq 2\mathrm{TV}(q_v^{(t)},\pi) \cdot \max_{w \neq u}\Bigl|\frac{q_w^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr| \\
	& \leq 2\mathrm{TV}(q_v^{(t)},\pi) \cdot \max\biggl\{1, \max_{w \neq u} \frac{q_w^{(1)}(u)}{\pi(u)} \biggr\} \\
	& \leq 2\mathrm{TV}(q_v^{(t)},\pi) \cdot \frac{m}{d_{\min}(G)^2} = \frac{\mathrm{TV}(q_v^{(t)},\pi) }{s(G)}.
	\end{align*}
	On the other hand, the second term on the right hand side of~\eqref{pf:tv_to_pointwise_1} satisfies
	\begin{align*}
	\bigl(\pi(u) - q_v^{(t)}(u) \bigr) \cdot \Bigl(\frac{q_u^{(1)}(u) - \pi(u)}{\pi(u)}\Bigr) \leq \bigl(\pi(u) - q_v^{(t)}(u) \bigr) \cdot \Bigl(\frac{1/2 - \pi(u)}{\pi(u)}\Bigr) \leq \frac{\pi(u) - q_v^{(t)}(u)}{2\pi(u)},
	\end{align*}
	so that we obtain the recurrence relation
	\begin{equation*}
	\frac{\pi(u) - q_v^{(t + 1)}(u)}{\pi(u)} \leq \frac{\mathrm{TV}(q_v^{(t)},\pi) }{s(G)} +\frac{\pi(u) - q_v^{(t)}(u)}{2\pi(u)}.
	\end{equation*}
	Then by the inductive hypothesis $\bigl(\pi(u) - q_v^{(t)}(u)\bigr)/\bigl(2\pi(u)\bigr) \leq \mathrm{TV}(q_v^{(t - 1)},\pi)/s(G)$, and consequentially
	\begin{equation*}
	\frac{\pi(u) - q_v^{(t + 1)}(u)}{\pi(u)} \leq \frac{\mathrm{TV}(q_v^{(t)},\pi) }{s(G)} + \frac{\mathrm{TV}(q_v^{(t - 1)},\pi) }{s(G)} \leq 2\frac{\mathrm{TV}(q_v^{(t)},\pi)}{s(G)}.
	\end{equation*}
	This completes the proof of Lemma~\ref{lem:tv_to_pointwise}.
\end{proof}

\subsection{Spectral partitioning properties of PPR}
\label{subsec:ppr_spectral_partitioning}
The following theorem is the main result of~\ref{subsec:ppr_spectral_partitioning}. It relates the normalized cut of the sweep sets $\Phi(S_{\beta};G)$ to the normalized cut of a candidate cluster $C \subseteq V$, when $p_v$ is properly initialized within $C$.

\begin{theorem}[\textbf{c.f. Theorem~6 of \cite{andersen2006}}]
	\label{thm:normalized_cut_ppr}
	Suppose that
	\begin{equation}
	\label{eqn:normalized_cut_ppr_vol}
	d_{\max}(G) \leq \vol(C;G) \leq \max\Bigl\{\frac{2}{3}\vol(G); \vol(G) - d_{\max}(G)\Bigr\}
	\end{equation}
	and
	\begin{equation}
	\label{eqn:normalized_cut_ppr_ncut}
	\max\Bigl\{288\Phi(C;G)\cdot \ln\Bigl(\frac{36}{s(G)}\Bigr),72\Phi(C;G) + \frac{d_{\max}(G)}{d_{\min}(G)^2}\Bigr\} < \frac{1}{18}.
	\end{equation}
	Set $\alpha = 36 \cdot \Phi(C;G)$. The following statement holds: there exists a set $C^g \subseteq C$ of large volume, $\vol(C^g;G) \geq 5/6 \cdot \vol(C;G)$, such that for any $v \in C^g$,  the minimum normalized cut of the sweep sets of $p_v$ satisfies 
	\begin{equation}
	\label{eqn:normalized_cut_ppr}
	\min_{\beta \in (0,1)}\Phi(S_{\beta,v};G) < 72\sqrt{\Phi(C;G) \cdot \ln\Bigl(\frac{36}{s(G)}\Bigr)}.
	\end{equation}
\end{theorem}
A few remarks:
\begin{itemize}
	\item Theorem~\ref{thm:normalized_cut_ppr} is similar to Theorem 6 of \citet{andersen2006}, but crucially the above bound depends on $\log\bigl(1/s(G)\bigr)$ rather than $\log m$. In the case where $d_{\min}(G)^2 \asymp \vol(G)$ and thus $s(G) \asymp 1$, this amounts to replacing a factor of $O(\log m)$ by a factor of ${O}(1)$, and therefore allows us to obtain meaningful results in the limit as $m \to \infty$. 
	\item For simplicity, we have chosen to state Theorem~\ref{thm:normalized_cut_ppr} with respect to a specific choice of $\alpha = 36 \cdot \Phi(C;G)$, but if $\alpha \approx 36 \cdot \Phi(C;G)$ then the Theorem will still hold up to constant factors.
\end{itemize}

It follows from Markov's inequality (see Theorem~4 of \cite{andersen2006}) that there exists a set $C^g \subseteq C$ of volume $\vol(C^g;G) \geq 5/6 \cdot \vol(C;G)$ such that for any $v \in C^g$,
\begin{equation}
\label{eqn:ppr_leakage}
p_v(C) \geq 1 - \frac{6\Phi(C;G)}{\alpha}.
\end{equation}
The claim of Theorem~\ref{thm:normalized_cut_ppr} is a consequence of~\eqref{eqn:ppr_leakage} along with Theorem~\ref{thm:mixing_time_PPR}, as we now demonstrate.
\begin{proof}[Proof (of Theorem~\ref{thm:normalized_cut_ppr})]
	From~\eqref{eqn:ppr_leakage}, the upper bound in~\eqref{eqn:normalized_cut_ppr_vol}, and the choice of $\alpha = 36 \cdot \Phi(C;G)$, 
	\begin{equation}
	\label{pf:normalized_cut_ppr}
	p_v(C) - \pi(C) \geq \frac{1}{3} - \frac{6 \Phi(C;G)}{\alpha} = \frac{1}{6}.
	\end{equation}
	Now, put 
	\begin{equation*}
	t_{\ast} = \frac{1}{648 \Phi(C;G)},~~\phi_{\ast}^2 = \frac{8}{t_{\ast}} \cdot \ln\Bigl(\frac{36}{s(G)}\Bigr),
	\end{equation*}
	and note that by~\eqref{eqn:normalized_cut_ppr_ncut} $\phi_{\ast}^2 \in [0,1]$. It therefore follows from~\eqref{pf:normalized_cut_ppr} and Theorem~\ref{thm:mixing_time_PPR} that either
	\begin{equation}
	\label{pf:normalized_cut_ppr_2}
	\frac{1}{6} \leq p_v(C) - \pi(C) \leq \frac{1}{18} + 72 \Phi(C;G) + \frac{d_{\max}(G)}{d_{\min}(G)^2} + \frac{1}{2s(G)} \cdot \left(1 - \frac{\phi_{\ast}^2}{8}\right)^{t_{\ast}},
	\end{equation}
	or $\min_{\beta \in (0,1)} \Phi(S_{\beta,v};G) \leq \phi_{\ast}^2$. But by~\eqref{eqn:normalized_cut_ppr_ncut}
	\begin{equation*}
	72 \Phi(C;G) + \frac{d_{\max}(G)}{d_{\min}(G)^2} < \frac{1}{18},
	\end{equation*}
	and we have chosen $\phi_{\ast}$ precisely so that
	\begin{equation*}
	\frac{1}{2s(G)} \cdot \left(1 - \frac{\phi_{\ast}^2}{8}\right)^{t_{\ast}} \leq \frac{1}{2s(G)} \exp\Bigl(-\frac{\phi_{\ast}^2 t_{\ast}}{8}\Bigr) \leq \frac{1}{18}.
	\end{equation*}
	Thus the inequality~\eqref{pf:normalized_cut_ppr_2} cannot hold, and so it must be the case $\min_{\beta \in (0,1)} \Phi(S_{\beta,v};G) \leq \phi_{\ast}^2$. This is exactly the claim of the theorem.
\end{proof}

\section{Finite sample results}
In this section, we prove high-probability finite-sample bounds on various functionals of the random graph $G_{n,r}$: cut, volume, minimum and maximum degree, normalized cut, local spread, and conductance. To do so, we will need several different \textcolor{red}{(concentration inequalities)}, and we begin by reviewing these.

\subsection{Concentration inequalities}
\textcolor{red}{(TODO)}

\section{Population functionals for density clusters}

\section{Proof of Major Theorems}
\label{sec:pf_major_theorems}

\section{aPPR and Misclassification Error}
\label{sec:appr_misclassification_error}

\section{Experimental Details}
\label{sec:experimental_details}

\clearpage
\bibliographystyle{plainnat}
\bibliography{../../local_spectral_bibliography} 

\end{document}