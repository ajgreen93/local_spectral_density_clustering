\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[parfill]{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{fullpage}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{hyperref}       % hyperlinks
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	citecolor=blue
}

\usepackage{natbib}
\renewcommand{\bibname}{REFERENCES}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\wb}{0}{mathx}{"73}

\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand{\eqdist}{\ensuremath{\stackrel{d}{=}}}
\newcommand{\Graph}{\mathcal{G}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Identity}{\mathbb{I}}
\newcommand{\Xsetistiid}{\overset{\text{i.i.d}}{\sim}}
\newcommand{\convprob}{\overset{p}{\to}}
\newcommand{\convdist}{\overset{w}{\to}}
\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Risk}[2][P]{\mathcal{R}_{#1}\left[ #2 \right]}
\newcommand{\Prob}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\iset}{\mathbf{i}}
\newcommand{\jset}{\mathbf{j}}
\newcommand{\myexp}[1]{\exp \{ #1 \}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\restr}[2]{\ensuremath{\left.#1\right|_{#2}}}
\newcommand{\ext}[1]{\widetilde{#1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\set{#1}_{n \in \N}}
\newcommand{\Xsetotp}[2]{\langle #1, #2 \rangle}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Xsetiam}{\mathrm{diam}}

\newcommand{\emC}{C_n}
\newcommand{\emCpr}{C'_n}
\newcommand{\emCthick}{C^{\sigma}_n}
\newcommand{\emCprthick}{C'^{\sigma}_n}
\newcommand{\emS}{S^{\sigma}_n}
\newcommand{\estC}{\widehat{C}_n}
\newcommand{\hC}{\hat{C^{\sigma}_n}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\spansp}{\mathrm{span}~}
\newcommand{\1}{\mathbf{1}}

\newcommand{\Linv}{L^{\Xsetagger}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\emF}{\mathbb{F}_n}
\newcommand{\emG}{\mathbb{G}_n}
\newcommand{\emP}{\mathbb{P}_n}
\newcommand{\F}{\mathcal{F}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\Nbb}{\mathbb{N}}

%%% Vectors
\newcommand{\thetast}{\theta^{\star}}
\newcommand{\betap}{\beta^{(p)}}
\newcommand{\betaq}{\beta^{(q)}}
\newcommand{\vardeltapq}{\varDelta^{(p,q)}}


%%% Matrices
\newcommand{\X}{X} % no bold
\newcommand{\Y}{Y} % no bold
\newcommand{\Z}{Z} % no bold
\newcommand{\Lgrid}{L_{\grid}}
\newcommand{\Xsetgrid}{D_{\grid}}
\newcommand{\Linvgrid}{L_{\grid}^{\Xsetagger}}
\newcommand{\Lap}{{\bf L}}
\newcommand{\NLap}{{\bf N}}
\newcommand{\PLap}{{\bf P}}

%%% Sets and classes
\newcommand{\Xset}{\mathcal{X}}
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Hclass}{\mathcal{H}}
\newcommand{\Pclass}{\mathcal{P}}
\newcommand{\Leb}{L}
\newcommand{\mc}[1]{\mathcal{#1}}

%%% Distributions and related quantities
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Ibb}{\mathbb{I}}

%%% Operators
\newcommand{\Tadj}{T^{\star}}
\newcommand{\Xsetive}{\mathrm{div}}
\newcommand{\Xsetif}{\mathop{}\!\mathrm{d}}
\newcommand{\gradient}{\mathcal{D}}
\newcommand{\Hessian}{\mathcal{D}^2}
\newcommand{\dotp}[2]{\langle #1, #2 \rangle}
\newcommand{\Dotp}[2]{\Bigl\langle #1, #2 \Bigr\rangle}

%%% Misc
\newcommand{\grid}{\mathrm{grid}}
\newcommand{\critr}{R_n}
\newcommand{\Xsetx}{\,dx}
\newcommand{\Xsety}{\,dy}
\newcommand{\Xsetr}{\,dr}
\newcommand{\Xsetxpr}{\,dx'}
\newcommand{\Xsetypr}{\,dy'}
\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\spec}{\mathrm{spec}}
\newcommand{\LE}{\mathrm{LE}}
\newcommand{\LS}{\mathrm{LS}}
\newcommand{\OS}{\mathrm{OS}}
\newcommand{\PLS}{\mathrm{PLS}}
\newcommand{\dist}{\mathrm{dist}}

%%% Order of magnitude
\newcommand{\soom}{\sim}

% \newcommand{\span}{\textrm{span}}

\newtheoremstyle{alden}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{alden} 


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}

\begin{document}
First of all, we would like to thank the reviewers and associate editor for their thoughtful comments and helpful suggestions. In response to this feedback, we have significantly reworked our manuscript, making the following major changes: 

\begin{itemize}
	\item \textbf{New theoretical results.} In a new Section 2, we establish conditions under which Personalized PageRank (PPR), run on a geometric graph, will recover an \emph{generic} candidate cluster $\mc{C} \subseteq \Rd$ (i.e. not necessarily a density cluster).  Our upper bound in Theorem 12 (new version) on the error $\Delta(\wh{C},\mc{C}[X])$ depends on population-level functionals---normalized cut,conductance, and local spread---that have previously been used to analyze the behavior of global spectral clustering algorithms in nonparametric mixture models, and we comment in detail on the relation between our results and previous work.
	
	\item \textbf{Improved upper bounds.} In Section 3, we apply this general theory to the specific case where $\mc{C} = \mc{C}_{\lambda}$ is a density cluster. In  Theorem 16, we upper bound the volume of the symmetric set difference between the cluster estimate and the true density cluster; this theorem is comparable to Theorem~5 of our previous submission, but with significantly smaller universal constants.
	
	\item \textbf{Improved empirical section.} We have modified the first figure in our empirical section (Figure~2) to allow for a precise comparison between our theoretical bounds on graph functionals of a density cluster, and their empirical values. We see that in almost all cases, our bounds correctly predict the behavior of these graph functionals as the geometry of the density cluster is changed, and are off by at most one order of magnitude.
	
	\item \textbf{Reworked exposition.} In particular: 
	\begin{itemize}
		\item We offer informal statements of our main upper bounds, in order to emphasize (what we view as) the crucial takeaways;
		\item We comment more extensively on our major theoretical results, explaining their significance and tying them to related work;
		\item We precisely use universal constants throughout, and in particular offer a cleaner comparison between our upper and lower bounds;
		\item We build slowly to our main results in Section 2 and 3. This exposes three modular components of our analysis---(i) bounds on PPR in terms of graph functionals; (ii) bounds on graph functionals in terms of population functionals; (iii) in the case of density cluster recovery, bounds on population functionals in terms of geometric parameters---and has the effect of making our major theorems easier to digest, as well as highlighting our various technical contributions.
	\end{itemize}
\end{itemize}

\section{Response to Associate Editor's Comments}

(\textit{``...usefulness of the key Theorem 5...''}) As outlined above, in our resubmission we have replaced Theorem 5 (old version) by Theorem 12 and Theorem 16 (new version). 

Theorem 12 (new version)  is substantially more general than Theorem~5 (old version), as it upper bounds $\Delta(\wh{C},\mc{C}[X])$ for a generic clusters $\mc{C} \subseteq \Rd$ in terms of the population-level normalized cut, conductance, and local spread. To the best of our knowledge, this is the first result on the clustering performance of PPR, or indeed any local spectral clustering algorithm, in the non-parametric statistical setup. In some remarks, we compare Theorem 12 to some related literature on global spectral clustering, showing both that Theorem 12 is in certain important respects sharper than previous results while naturally requiring (weaker) local rather than global conditions.

Theorem 16 (new version) treats the case where $\mc{C} = \mc{C}_{\lambda}$ is a $\lambda$-density cluster, and is thus directly comparable to Theorem~5 (old version). It gives a substantially tighter (in terms of constants) upper bound on $\Delta(\wh{C},\mc{C}_{\lambda,\sigma}[X])$ than Theorem~5 (old version). To our knowledge, this is the first result tying the performance of a spectral clustering method to recovery of a density cluster.

(\textit{``...sets of zero measure may dramatically alter the topological structure of the level sets.''}) Following the lead of~\citet{chaudhuri2010}, we assume the density $f(x) \geq \lambda_{\sigma} > 0$ everywhere on the full-dimensional set $\mc{C}_{\lambda,\sigma} = \mc{C}_{\lambda} + B(0,\sigma)$. Our ultimate bound on density cluster recovery is with respect to the enlarged set $\mc{C}_{\lambda,\sigma}$, and depends on $\lambda_{\sigma}$. If $f$ satisfies a uniform continuity condition, the density will be lower bounded over $\mc{C}_{\lambda,\sigma}$, and we now make this point explicitly in our setup. We also mention that such continuity conditions can be weakened, and point to relevant references \citep{rinaldo2010, steinwart2015}.

(\textit{``...literature mentioned on density-based clustering (see page 5) abruptly ends in 2013...''}) We have updated our literature review to include more recent works.

\section{Response to Reviewer 1's Comments}

(\textit{``Is $\kappa(\mc{C})$ uniquely defined?''}) We have modified our definition of the condition number so as to make it uniquely defined in terms of functionals of a generic cluster $\mc{C}$. In our revision, for $\delta \in (0,1/4)$ we define
\begin{equation*}
\kappa_{\Pbb,r}(\mc{C},\delta) := \frac{(1 + 3\delta)(1+2\delta)}{(1 - 4\delta)^2(1 - \delta)} \cdot \frac{\Phi_{\Pbb,r}(\mc{C})}{\alpha_{\Pbb,r}(\mc{C},\delta)}.
\end{equation*}
where
\begin{equation*}
\alpha_{\Pbb,r}(\mc{C},\delta) := \frac{\ln(2)}{13} \cdot \frac{\Psi_{\Pbb,r}^2(\mc{C})}{\ln^2\Bigl(\frac{8}{(1 - 4\delta)s_{\Pbb,r}(\mc{C})}\Bigr)}.
\end{equation*}
The condition number thus depends on the population-level normalized cut $\Phi_{\Pbb,r}(\mc{C})$, conductance $\Psi_{\Pbb,r}(\mc{C})$, and local spread $s_{\Pbb,r}(\mc{C})$ of the candidate cluster, as well as on the high-probability parameter $\delta$. 

(\textit{``...not clear whether or not $c \cdot \kappa(\mc{C}) \leq \vol(X \setminus \mc{C}) / \vol(\mc{C} \cap X)$''}) For a given $a > 0$, our revised definition of condition number $\kappa_{\Pbb,r}(\mc{C},\delta)$ will be less than $a/60$ if
\begin{equation}
\label{eqn:rebuttal1}
\Phi_{\Pbb,r}(\mc{C}) < a \cdot \frac{\ln 2(1 - \delta)(1 - 4\delta)^2}{780(1 + 3\delta)(1 + 2\delta)} \cdot \frac{\ln^2\Bigl(\frac{8}{(1 - 4\delta)s_{\Pbb,r}(\mc{C})}\Bigr)}{\Psi_{\Pbb,r}(\mc{C})^2}.
\end{equation}
Theorem~12 (new version) guarantees that if $\mc{C}$ satisfies~\eqref{eqn:rebuttal1}, then for $n$ sufficiently large \smash{$\Delta(\wh{C},\mc{C}[X])/\vol_{n,r}(\mc{C}[X]) < a$} with high probability. Taking $a = \min\{\vol_{\Pbb,r}(\mc{C}),\vol_{\Pbb,r}(\Rd \setminus \mc{C})\} $ (or some small fraction thereof), our theory shows that $\Delta(\wh{C},\mc{C}[X])$ will be (much) smaller than that of the trivial estimators $\wh{C} = \emptyset$ or $\wh{C} = X$.

If $\mc{C} = \mc{C}_{\lambda,\sigma}$ is a (thickened) density cluster, Proposition~14 (new version) tells us that~\eqref{eqn:rebuttal1} will hold if $f$ satisfies a sufficiently strong low-noise condition around the boundary of $\mc{C}_{\lambda,\sigma}$---i.e. $f$ satisfies~(A4) for sufficiently large $\theta$ and $\gamma$.

(\textit{``...there is a risk that the upper bounds presented here could be too loose that they do not reflect the real performance of the algorithm.''}) Our response to this comment focuses on the density cluster recovery results of Section~3, since these results are directly comparable to those of our previous submission, and are thus the results the reviewer was concerned about. However, it is worth pointing out two facts about our new generic cluster recovery results of Section~2: first, that it is widely accepted that the local spectral clustering algorithms such as PPR intrinsically depend on functionals such as normalized cut and conductance; and second, that our particular bounds on PPR are in some key respects stronger than those derived in previous works, as we comment on in a remark after Theorem~12.

Now, with regards to our density cluster recovery results: in our view the most useful way to interpret our upper and lower bounds is by focusing on how these bounds depend on the geometric parameters $\sigma, \rho$ and $L$, the density parameters $\lambda,\Lambda_{\sigma},\lambda_{\sigma},\gamma$ and $\theta$, and the dimension $d$. We have tried to emphasize this in our exposition. At the same time, we have also substantially improved the constants in our bounds on normalized cut, conductance, and local spread. In Figure~2, we show that our bounds on each of these functionals are reasonably close to empirical reality, off by an order of magnitude or less.

However, we would like to emphasize that despite our best efforts at optimizing various constants and dependencies, our ultimate bounds are still too loose to make quantitative predictions on the behavior of PPR. Our upper bound on $\kappa_{\Pbb,r}(\mc{C}_{\lambda,\sigma},\delta)$ often yields correct qualitative predictions (as we have confirmed in theory by complementary lower bounds) but, as we acknowledge in the main text, will yield quite pessimistic quantitative predictions.

Indeed, even for the much simpler density clustering algorithm of~\citet{chaudhuri2010}, selecting tuning parameters according to their prescription(s) (based on finite-sample concentration inequalities) would lead to vacuous guarantees except for extremely large sample-sizes, or for extreme geometric cluster parameters (separation and density-drop). When these geometric parameters take large values the density clustering problem is trivial. As is a general theme in most work in statistical learning theory, their work gives useful qualitative insights even when the precise quantitative bounds are vacuous.

% AJG: Removed this, as our response to this point is the same as the one above.
%
%(``[In] \textit{the example used to produce Fig. 2... Thm.5 becomes $\vol\bigl(\wh{C} \vartriangle (\mc{C} \cap X)\bigr) \leq 70950 \cdot \vol\bigl(\mc{C} \cap X\bigr)$. This looks much worse than the trivial case $\wh{C} = \emptyset$.''}) We have substantially improved our bounds, including our bounds on the population-level normalized cut and conductance of density clusters, as well as the difference between sample and population-level normalized cut and conductance. In the specific example used for Fig. 2, our upper bound on $\Delta(\wh{C},\mc{C}[X])/\vol_{n,r}(\mc{C}[X])$ is $\leq 1/2$ whenever $\theta \geq \textcolor{red}{(...)}$.

(``\textit{$\kappa(\mc{C})$ should be explicitly reported} [in experiments]'') The condition number will be a loose bound on the actual performance of the algorithm, a fact we frankly acknowledge in our main text, and so we do not think it would be useful to report it.

(``\textit{The use of universal constants throughout the paper is not consistent.}'') We apologize for the confusion. In our new submission, we have clarified our use of constants so as to avoid any ambiguity. We explicitly keep track of all constants in our appendices.

(``\textit{...parameters used in the first experiment...}'') We now report all parameters for our experiment in Appendix~F. 

(``\textit{Minor comments}.'') We have addressed all of the errors noticed by the reviewer, and we thank them for their careful attention. 

\section{Response to Reviewer 2's Comments}

(\textit{``...no explicit smoothness assumption on the density function...''}) See our comment above; the assumption that $f(x) \geq \lambda_{\sigma}$ on the thickened set $\mc{C}_{\lambda,\sigma}$ is analogous to (more specifically, implied by) a uniform continuity condition on $f$ within $\mc{C}_{\lambda,\sigma}$. 

(\textit{``The bound in Theorem 5 doesn't appear to be something that can go to 0 as $n \to \infty$ and $r \to 0$...''}) This comment is correct. This bound---in Theorem~5 (old version), and now in Theorem~12 (new version)--- is
\begin{equation}
\label{eqn:volume_ssd_ppr}
\frac{\Delta(\wh{C},\mc{C}[X])}{\vol_{n,r}(\mc{C}_{\sigma}[X])} \leq 60 \cdot \kappa_{\Pbb,r}(\mc{C},\delta),
\end{equation}
and the right hand side will not converge to $0$ as $n \to \infty$. This is typical in the analysis of spectral clustering algorithms in a non-parametric statistical context~\citep{shi2009,schiebinger2015,garciatrillos19}, and we make some detailed comparison between our bound and these related works after Theorem~12 (new version). We also confirm, both theoretically and experimentally, that indeed the misclassification error of PPR will not go to zero as $n \to \infty$, even for some fairly well-conditioned density clusters.

(``\textit{...assumptions for Theorem 6 to hold seem very strong...}'')  We would point out that Theorem~6  is, to the best of our knowledge, the first of its kind---that is, the first to show that a spectral clustering method will distinguish two density clusters. However, we agree that the assumptions are quite strong, and in our revised submission we have moved this theorem to one of our appendices, so as not to distract from our major points. 

(``\textit{...(Def 2) doesn't appear to guarantee a bijection between the true clusters and the estimated clusters...''}) The reviewer is correct that this is fundamental to the local nature of the problem, in which we ask the algorithm to return only one cluster, rather than a clustering. We have clarified this point in our exposition. 

\bibliographystyle{plainnat}
\bibliography{../../local_spectral_bibliography} 

\end{document}