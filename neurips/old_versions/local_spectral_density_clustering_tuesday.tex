\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2019}

\PassOptionsToPackage{numbers}{natbib}
\usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{float}
\usepackage[export]{adjustbox}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{xr-hyper}
\usepackage{hyperref}
\usepackage[reqno]{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[parfill]{parskip}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{bm}
\usepackage{mathtools}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}


\newcommand{\diam}{\rho}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\defeq}{\overset{\mathrm{def}}{=}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\cut}{\mathrm{cut}}
% \newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\1}{\mathbf{1}}
\newcommand{\Phibf}{\Phi_{u}}
\newcommand{\Psibf}{\Psi_{u}}
\newcommand{\dist}{\mathrm{dist}}

%%% Vectors
\newcommand{\pbf}{p}        % removed bold font
\newcommand{\qbf}{\mathbf{q}}
\newcommand{\ebf}[1]{{e}_{#1}}
\newcommand{\pibf}{\bm{\pi}}

%%% Matrices (no bold font)
\newcommand{\Abf}{A}
\newcommand{\Xbf}{X}             % removed bold font 
\newcommand{\Wbf}{W}
\newcommand{\Lbf}{L}
\newcommand{\Dbf}{D}
\newcommand{\Ibf}[1]{I_{#1}}

%%% Probability distributions (and related items)
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Cbb}{\mathbb{C}}
\newcommand{\Ebb}{\mathbb{E}}

%%% Sets
\newcommand{\Cset}{\mathcal{C}}
\newcommand{\Aset}{\mathcal{A}}
\newcommand{\Asig}{\Aset_{\sigma}}
\newcommand{\Csig}{\Cset_{\sigma}}
\newcommand{\Sset}{\mathcal{S}}

%%% Graph quantities
\newcommand{\Cest}{\widehat{C}}

%%% Operators
\DeclareMathOperator*{\argmin}{arg\,min}

%%% Algorithm notation
\newcommand{\ppr}{{\sc PPR}}
\newcommand{\pprspace}{{\sc PPR~}}

\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\newtheoremstyle{aldenrmrk}
{6pt} % Space above
{6pt} % Space below
{} % Body font
{} % Indent amount
{\itshape} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenrmrk}
\newtheorem{remark}{Remark}

\title{Local Spectral Clustering of Density Upper Level Sets}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
\vskip 0.1 in
  Spectral clustering methods are a family of popular nonparametric clustering
  tools.  Recent works have proposed and analyzed \emph{local} spectral methods,
  which extract clusters using locally-biased random walks around a user-specified
  seed node.  In contrast to existing results, we analyze PPR in a traditional
  statistical learning setup, where we obtain samples from an unknown
  distribution, and aim to identify connected regions of high-density (density
  clusters).  We prove that PPR, run on a neighborhood graph, extracts
  sufficiently salient density clusters, and provide empirical support for our theory.
\end{abstract}

\section{Introduction}
\label{sec: introduction}

Let $\Xbf = \{x_1, \ldots, x_n\}$ be a sample drawn i.i.d.\ from a
distribution $\Pbb$ on $\Rd$, with density $f$, and consider the problem of 
clustering: splitting the data into groups which satisfy some notion of
within-group similarity and between-group difference.  We focus on spectral
clustering methods, a family of powerful nonparametric clustering algorithms.
Roughly speaking, a spectral technique first constructs a geometric graph $G$,
where vertices are associated with samples, and edges correspond to proximities
between samples. It then learns a feature embedding based on the Laplacian of
$G$, and applies a simple clustering technique (such as k-means clustering) in
the embedded feature space.

When applied to geometric graphs constructed from a large number of samples,
global spectral clustering methods can be computationally cumbersome and   
insensitive to the local geometry of the underlying distribution
\citep{leskovec2010,mahoney2012}.  This has led to recent increased interest in
local spectral algorithms, which leverage locally-biased spectra computed using
random walks around a user-specified seed node.  A popular local clustering
algorithm is Personalized PageRank (\ppr), first introduced by
\citet{haveliwala2003}, and further developed in
\citep{spielman2011,spielman2014,andersen2006,mahoney2012,zhu2013},
among others.  

Local spectral clustering techinques have been practically very successful
\citep{leskovec2010,andersen2012,gleich2012,mahoney2012,wu2012}, which has led
many authors to develop supporting theory
\citep{spielman2013,andersen2009,gharan2012,zhu2013} that gives worst-case
guarantees on traditional graph-theoretic notions of cluster quality (like
conductance).  In this paper, we adopt a more traditional statistical viewpoint,
and examine what the output of a local clustering algorithm on $\Xbf$ reveals
about the unknown density $f$.  In particular, we examine the ability of the \pprspace algorithm to recover \emph{density clusters} of $f$, which are defined as the
connected components of the upper level set $\{x \in \Rd : f(x) \geq \lambda\}$
for some threshold $\lambda > 0$ (a central object of central interest in the
classical statistical literature on clustering, dating back to
\citet{hartigan1981}).

\paragraph{PPR on a neighborhood graph} We now describe the clustering algorithm that will be our focus for the rest of 
the paper. We start with the geometric graph that we form based on the samples 
$\Xbf$: for a radius $r > 0$, we consider the \emph{$r$-neighborhood graph} of 
$\Xbf$, denoted $G_{n,r}=(V,E)$, an unweighted, undirected graph with vertices
$V=\Xbf$, and an edge $(x_i,x_j) \in E$ if and only if $\norm{x_i - x_j}
\leq  r$, where $\norm{\cdot}$ denotes Euclidean norm. We denote by $\Abf \in \Reals^{n \times n}$ the adjacency matrix, with entries $\Abf_{uv} = 1$ if and only if $(u,v) \in E$, and by $\Dbf$ the diagonal degree matrix, with $\Dbf_{uu} = \sum_{v \in V} \Abf_{uv}$.

Next, we define the PPR vector $\pbf = \pbf(v,\alpha;G_{n,r})$, with respect to  
a seed node $v \in V$ and a teleportation parameter $\alpha \in [0,1]$, to be
the solution of the following linear system:
\begin{equation}
\label{eqn: ppr_vector}
\pbf = \alpha \ebf{v} + (1 - \alpha) \pbf \Wbf,
\end{equation}
where $\Wbf = (\Ibf{} + \Dbf^{-1}\Abf)/2$ is the (lazy) random walk matrix over $G_{n,r}$ 
and $e_{v}$ denotes indicator vector for node $v$ (with a 1 in the $v$th
position and 0 elsewhere).

For a level $\beta > 0$ and a target volume $\vol_0 > 0$, we define a
\emph{$\beta$-sweep cut} of $\pbf = (p_u)_{u \in V}$ as  
\begin{equation}
\label{eqn: sweep_cuts}
S_\beta = \set{u \in V: \frac{p_u}{\Dbf_{uu}} > \frac{\beta}{\vol_{0}}}.
\end{equation}
We need a metric to determine which sweep cut $S_{\beta}$ is the best cluster estimate. For a set $S \subseteq V$ with complement $S^c = V \setminus S$, we define the cut as $\cut(S;G_{n,r}) := \sum_{u \in S, v \in S^c} \Abf_{uv}$, the volume to be $\vol(S; G_{n,r}) := \sum_{u \in S} \Dbf_{uu}$, and the \emph{normalized cut} as
\begin{equation}
\label{eqn: normalized_cut}
\Phi(S; G_{n,r}) := \frac{\cut(S;G_{n,r})}{\min \set{\vol(S; G_{n,r}), \vol(S^c; G_{n,r})}}.
\end{equation}
Having computed sweep cuts $S_{\beta}$ over a range \smash{$\beta \in  
(\frac{1}{40},\frac{1}{11})$},\footnote{The choice of a specific range such as 
\smash{$(\frac{1}{40}, \frac{1}{11})$} is standard in the analysis of PPR
algorithms, see, e.g., \citep{zhu2013}.}, we then output the cluster estimate $\widehat{C} = S_{\beta^*}$ which has minimum normalized cut $\Phi(S_{\beta^{\star}}; G_{n,r})$. For concreteness, we summarize this procedure in Algorithm \ref{alg: ppr}.

\begin{algorithm}
	\caption{PPR on a Neighborhood Graph}
	\label{alg: ppr}	
	{\bfseries Input:} data $\Xbf=\{x_1,\ldots,x_n\}$, radius $r > 0$, teleportation 
	parameter $\alpha \in [0,1]$, seed $v \in \Xbf$, target stationary volume $\vol_0 >
	0$. \\   
	{\bfseries Output:} cluster $\Cest \subseteq V$.
	\begin{algorithmic}[1]
		\STATE Form the neighborhood graph $G_{n,r}$.
		\STATE Compute the PPR vector $\pbf(v, \alpha; G_{n,r})$ as in \eqref{eqn:
			ppr_vector}. 
		\STATE For $\beta \in (\frac{1}{40}, \frac{1}{11})$ compute sweep cuts
		$S_{\beta}$ as in \eqref{eqn: sweep_cuts}.
		\STATE Return \smash{$\Cest = S_{\beta^*}$}, where 
		$$
		\beta^* = \argmin_{\beta \in (\frac{1}{40}, \frac{1}{11})} \Phi(S_{\beta}; G_{n,r}).
		$$
	\end{algorithmic}
\end{algorithm}

\paragraph{Estimation of density clusters}

Let $\Cbb_f(\lambda)$ denote the connected components of the density upper level
set $\{x \in \Rd: f(x) > \lambda\}$.  For a given density cluster $\Cset \in
\Cbb_f(\lambda)$, we call $\Cset[\Xbf] = \Cset \cap \Xbf$ the \emph{empirical
density cluster}. The symmetric set difference between estimated and empirical cluster is perhaps the most frequently used metric to quantify cluster estimation error \citep{korostelev2012,polonik1995,rigollet2009}.

\begin{definition}[Symmetric set difference]
	\label{def: symmetric_set_diff}
	For an estimator \smash{$\Cest \subseteq \Xbf$} and set
	$\mathcal{S} \subseteq \Reals^d$, the symmetric set difference of $\Cest$ and $\mathcal{S} \cap \Xbf$ is 
	\begin{equation}
	\label{eqn: misclassification_rate}
	\Delta(\Cest, \mathcal{S}) := \abs{\Cest \setminus \mathcal{S}[\Xbf] \cup \mathcal{S}[\Xbf] \setminus \Cest}.
	\end{equation}
\end{definition}

However, the symmetric set difference does not account for the distance points in $\widehat{C} \setminus \mathcal{S}[\Xbf]$ may be from $\mathcal{S}$ \citep{singh2009}. We therefore give a second notion of cluster estimation, first introduced by \citet{hartigan1981} and defined asymptotically, which measures whether $\widehat{C}$ can distinguish any two distinct elements $\Cset, \Cset' \in \Cbb_f(\lambda)$. 

\begin{definition}[Consistent density cluster estimation]
	\label{def: consistent_density_cluster_estimation}
	For an estimator \smash{$\Cest \subseteq \Xbf$} and cluster 
	$\Cset \in \Cbb_f(\lambda)$, we say \smash{$\Cest$} is a consistent
	estimator of $\Cset$ if for all $\Cset' \in \Cbb_f(\lambda)$ with $\Cset \not=
	\Cset'$ the following holds as $n \to \infty$: 
	\begin{equation}
	\label{eqn: consistent_density_cluster_recovery}
	\Cset[\Xbf] \subseteq \Cest \quad \text{and} \quad
	\Cest \cap \Cset'[\Xbf] = \emptyset,
	\end{equation}
	with probability tending to 1.
\end{definition}
\paragraph{Summary of results}
A summary of our main results (and outline for the rest of this paper) is as
follows.  

\begin{enumerate}
	\item In Section \ref{sec: consistent_cluster_estimation_with_ppr}, we introduce a set of natural geometric conditions, formalize a measure of difficulty based on these geometric conditions, and show that when properly initialized, the symmetric set difference of Algorithm \ref{alg: ppr} is upper bounded by this difficulty measure.
	
	\item We further show that if the density cluster $\Cset$ is particularly well-conditioned, Algorithm \ref{alg: ppr} consistently estimate a density cluster in the sense of \eqref{eqn: consistent_density_cluster_recovery}.
	
	\item In Section \ref{sec: analysis}, we detail some of the main technical machinery required to prove our main results, and expose the part various geometric quantities play in the ultimate difficulty of the clustering problem.
	
	\item In Section \ref{sec: experiments}, we empirically
	demonstrate the tightness of the bounds in Theorems \ref{thm: conductance_upper_bound} and \ref{thm: mixing_time_upper_bound}, and provide examples showing how violations of the geometric conditions we require manifestly
	impact density cluster recovery by \ppr.  
\end{enumerate}

Our main takeaway can be summarized as follows: \ppr, run on a neighborhood graph, recovers geometrically compact high-density clusters.

\paragraph{Related Work. }

In addition to the background given previously, a few related lines of work are worth
highlighting. Similar in spirit to our results are the works
\citep{shi2009,schiebinger2015}, who study the consistency of
spectral algorithms in recovering the latent labels in certain parametric and
nonparametric mixture models. These results focus on global rather than local
algorithms, and as such impose global rather than local conditions on the nature
of the density. Moreover, they do not in general guarantee recovery of density
clusters, which is the focus in our work. Perhaps most importantly, these works rely on general cluster saliency conditions, which depend implicitly on many distinct geometric aspects of the cluster $\Cset$ under consideration. We make this dependence explicit, and in so doing, expose the role each geometric condition plays in the clustering problem.

Additionally, we note that density clustering and level set estimation is a well-studied problem. \citep{polonik1995, rigollet2009} study density clustering under symmetric set difference, \citep{tsybakov1997, singh2009} prove minimax optimal level set estimators under Hausdorff loss and \citep{hartigan1981, chaudhuri2010} consider consistent estimation of the cluster tree, to note but a few works on the subject. Our goal is not to improve on these results, or offer yet another algorithm for level set estimation; indeed, seen as a density clustering algorithm, \pprspace has none of the optimality guarantees of the previous works. This is in fact a major point of our article: \pprspace can provably recover density clusters, but only under strong geometric conditions. 

\section{Estimation of well-conditioned density clusters}
\label{sec: consistent_cluster_estimation_with_ppr}

We formalize some geometric conditions, before using these to define a condition number $\kappa(\Cset)$ which measures the difficulty \pprspace will have in estimating $\Cset$. We motivate this measure, and the underlying geometric conditions, by giving density cluster estimation guarantees for Algorithm \ref{alg: ppr} in terms of $\kappa(\Cset)$.

\paragraph{Geometric conditions on density clusters}

As mentioned previously, successful recovery of a density cluster by \pprspace requires the density cluster to be geometrically well-conditioned. At a minimum, we wish to avoid sets $\Cset$ which contain arbitrarily thin bridges or spikes, and therefore as in \cite{chaudhuri2010} we introduce a buffer zone around $\Cset$. Let $B(x,r)$ be the closed ball of radius $r > 0$ centered at $x \in \Rd$. For a some $\lambda > 0$, consider a given cluster $\Cset \in \Cbb_f(\lambda)$. We denote the distance between $x$ and $\Cset$ as $\dist(x,\Cset) := \inf_{y \in \Cset} \norm{y - x}$, and for a given $\sigma > 0$, we refer to $\Csig := \set{x \in \Reals^d: \dist(x,\Cset) \leq \sigma}$ as the $\sigma$-expansion of $\Cset$. We now state our conditions with respect to $\Csig$, and provide some intuition afterwards.

\begin{enumerate}[label=(A\arabic*)]
	\item
	\label{asmp: bounded_density}
	\emph{Bounded density within cluster:} There exist constants $\lambda_{\sigma}, \Lambda_{\sigma}$ such that $0 < \lambda_{\sigma} = \inf_{x \in \Csig} f(x) \leq \sup_{x \in \Csig} f(x) \leq \Lambda_{\sigma} < \infty$.
	
	\item
	\label{asmp: cluster_separation}
	\emph{Cluster separation:}
	For all $\Cset' \in \Cbb_f(\lambda)$ with $\Cset' \not= \Cset$, $\dist(\Csig,\Csig') > \sigma$,
	where \smash{$\dist(\Csig,\Csig') := \inf_{x \in \Csig} \dist(x,\Csig')$}. 
	
	\item 
	\label{asmp: low_noise_density}
	\emph{Low noise density:} There exists $\gamma,c_0 > 0$ such that for all $x
	\in \Rd$ with $0 < \dist(x, \Csig) \leq \sigma$,   
	$$
	\inf_{x' \in \Csig} f(x') - f(x) \geq  c_0 \dist(x, \Csig)^{\gamma},
	$$ 
	
	%%% AJG 5/20: Should I turn this into two conditions?
	\item
	\label{asmp: embedding}
	\emph{Lipschitz embedding:}
	There exists $g: \Reals^d \to \Reals^d$ which has the following properties: i) there exists a convex set $\mathcal{K} \subseteq \Rd$ with $\mathrm{diam}(\mathcal{K}) = \sup_{x,y \in \mathcal{K}}\norm{x - y} =: \diam < \infty$, such that $\Csig = g(\mathcal{K})$, ii) $\det(\nabla g (x)) = 1$ for all $x \in \Csig$, where $\nabla g(x)$ is the Jacobian of $g$ evaluated at $x$, and iii) for some $L \geq 1$, 
	\begin{equation*}
	\frac{1}{L}\norm{x - y} \leq \norm{g(x) - g(y)} \leq L \norm{x - y} ~ \text{for all $x,y \in \mathcal{K}$}.
	\end{equation*}
	Simply put, $\Csig$ is the image of a convex set with finite diameter, under a  measure preserving, biLipschitz transformation.
	\item
	\label{asmp: bounded_volume}
	\emph{Bounded volume:}
	Let the neighborhood graph radius $0 < r \leq \sigma/2d$ be such that
	\begin{equation*}
	2 \int_{\Csig} \Pbb(B(x,r)) f(x) dx \leq \int_{\Rd} \Pbb(B(x,r)) f(x) dx
	\end{equation*}
\end{enumerate}

Thinking of $\Csig[\Xbf]$ as a subset of vertices in $G_{n,r}$, we would like $\Csig[\Xbf]$ to be internally well-connected, while being poorly connected to the rest of $\Xbf$. The cluster separation \ref{asmp: cluster_separation} and low noise density \ref{asmp: low_noise_density} conditions guarantee low connectivity between $\Csig[\Xbf]$ and $\Xbf \setminus \Csig[\Xbf]$ in $G_{n,r}$, whereas \ref{asmp: bounded_density} and \ref{asmp: embedding} ensure high connectivity within $\Csig[\Xbf]$. It may not be immediately obvious how \ref{asmp: embedding} contributes to geometric conditioning. For now, we observe merely that random walks will mix slowly over sets with large diameter, and comment on this condition in more detail in Section \ref{sec: analysis}. Finally, \ref{asmp: bounded_volume} is a relatively harmless technical condition, merely excluding the case where $\vol(\Csig[\Xbf]; G_{n,r}) > \vol(\Xbf; G_{n,r})/2$. 

We can now formally define the condition number, $\kappa(\Cset)$, which reflects the difficulty of the local spectral clustering task. The smaller $\kappa(\Cset)$ is, the more success \pprspace will have in recovering $\Cset$. Let $\theta := (r, \sigma, \lambda, \lambda_{\sigma}, \Lambda_{\sigma}, \gamma, \diam, L)$ contain those geometric parameters detailed in \ref{asmp: bounded_density} - \ref{asmp: bounded_volume}.

\begin{definition}[Well-conditioned density clusters]
	For $\lambda > 0$ and $\Cset \in \Cbb_f(\lambda)$, let $\Cset$ satisfy \ref{asmp: bounded_density} - \ref{asmp: bounded_volume} for some $\theta$, Then, for universal constants $c_1, c_2, c_3 > 0$ to be specified later, we set
	\begin{equation}
	\label{eqn: condition_number_1}
	\Phibf(\theta) 
	:= c_1 r \frac{d}{\sigma} \frac{\lambda}{\lambda_{\sigma}} \frac{(\lambda_{\sigma} - c_0 \frac{r^{\gamma}}{\gamma + 1})}{\lambda_{\sigma}},~ 
	\Psibf(\theta) := \Biggl(c_2 \frac{\Lambda_{\sigma}^4 d^3 D^2 L^2}{\lambda_{\sigma}^4 r^2} \log^2\left(\frac{1}{r}\right) + c_3 \Biggr)^{-1}
	\end{equation}
	and letting $\kappa(\Cset) := \frac{\Phibf(\theta)}{\Psibf(\theta)}$, we call $\Cset$ a \textrm{$\kappa$-well-conditioned density cluster}.
\end{definition}

At first glance \eqref{eqn: condition_number_1} may appear mysterious, but as will be shown in Section \ref{sec: analysis}, $\Phibf(\theta)$ and $\Psibf(\theta)$ are merely upper bounds on the normalized cut \eqref{eqn: normalized_cut} of $\Csig[\Xbf]$ in $G_{n,r}$, and the inverse of the mixing time (defined in Section \ref{sec: analysis} by \eqref{eqn: mixing_time}) of $\Csig[\Xbf]$ in $G_{n,r}$. In \cite{zhu2013}, building on the work of \cite{andersen2006} and others, it is shown that the ratio of normalized cut to inverse mixing time (or equivalently, the product of normalized cut and mixing time) is a fundamental quantity governing the clustering performance of \pprspace on a general graph. The condition number $\kappa(\Cset)$ is an asymptotic upper bound of this ratio for an empirical density cluster over the neighborhood graph $G_{n,r}$, and is therefore a natural criterion to measure difficulty of the density clustering task.

\paragraph{Well-initialized algorithm}

As is typical in the local clustering literature, our algorithmic results will be stated with respect to specific choices or ranges of each of the user-specified parameters.

In particular, for a well-conditioned density cluster $\Cset$ (with respect to some $\theta$), we require
\begin{align}
\label{eqn: initialization}
r \leq \frac{\sigma}{2d}, & ~\alpha \in [1/10, 1/9] \cdot \Psibf(\theta) \nonumber,  \\
v \in \Csig[\Xbf]^g, & ~\vol_0 \in [3/4,5/4] \cdot n(n-1) \int_{\Csig} \Pbb(B(x,r)) f(x) dx
\end{align}
where $\Csig[\Xbf]^g \subseteq \Csig[\Xbf]$ will be some large subset of $\Csig[\Xbf]$. In particular, letting $\vol_{n,r}(S) := \vol(S; G_{n,r})$ for $S \subseteq \Xbf$, we have $\vol_{n,r}(\Csig[\Xbf]^g) \geq \vol_{n,r}(\Csig[\Xbf])/2$.

\begin{definition}
	If the input parameters to Algorithm \ref{alg: ppr} satisfy \eqref{eqn: initialization} for some well-conditioned density cluster $\Cset$, we say the algorithm is \emph{well-initialized}.
\end{definition}

In practice it is clearly not feasible to set hyperparameters based on the underlying (unknown) density $f$. Typically, one tunes \pprspace over a range of hyperparameters and optimizes for some criterion such as minimum normalized cut; it is unclear how this scheme would affect the performance of \pprspace in the density clustering context.

\paragraph{Density cluster estimation by \ppr}

Theorem 1 of \cite{zhu2013}, combined with the results of Section \ref{sec: analysis}, immediately implies a bound on the volume of $\Cest \setminus \Csig[\Xbf]$ (and likewise $\Csig[\Xbf] \setminus \Cest$),\footnote{For sequences $a_n$ and $b_n$, we write $a_n \lesssim b_n$ if there exists constant $c$ such that $a_n \leq c b_n$ for all $n$ sufficiently large. }
\begin{equation}
\label{eqn: graph_symmetric_set_difference}
\vol_{n,r}(\Cest \setminus \Csig[\Xbf]), \vol_{n,r}(\Csig[\Xbf] \setminus \Cest) \lesssim \kappa(\Cset) \vol_{n,r}(\Csig[\Xbf]).
\end{equation}
To translate \eqref{eqn: graph_symmetric_set_difference} into meaningful bounds on the symmetric set difference $\Delta(\Csig[\Xbf], \Cest)$, we wish to preclude vertices $x \in \Xbf$ from having arbitrarily small degree. To do so, we make some regularity assumptions on $\mathcal{X} := \mathrm{supp}(f)$. Let $\nu$ denote the Lebesgue measure on $\Rd$, and $\nu_d = \nu(B)$ be the measure of the unit ball $B = B(0,1)$.
\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{5}
	\item 
	\label{asmp: valid_region}
	\emph{Regular support:} There exists some number $\lambda_{\min} > 0$ such that $\lambda_{\min} < f(x)$ for all $x \in \mathcal{X}$. Additionally, there exists some $c > 0$ such that for each $x \in \partial \mathcal{X}$, $\nu(B(x,r) \cap \mathcal{X}) \geq c \nu_d r^d$.
\end{enumerate}
Note that the latter condition in $\ref{asmp: valid_region}$ will be satisfied if, for instance, the support $\mathcal{X}$ is a $\sigma$-expanded set.

\begin{theorem}
	\label{thm: misclassification_rate}
	Fix $\lambda > 0$, let $\Cset \in \Cbb_f(\lambda)$ be a $\kappa$-well conditioned density cluster (with respect to some $\theta$), and additionally assume $f$ satisfies \ref{asmp: valid_region}. Then, there exists a universal constant $c_4 > 0$ such that with probability tending to one as $n \to \infty$, 
	\begin{equation}
	\label{eqn: misclassification_rate_ub}
	\Delta(\Csig[\Xbf], \Cest) \leq c_4 \kappa(\Cset) \frac{\Lambda_{\sigma}}{\lambda_{\min}}.
	\end{equation}
\end{theorem}

The proof of Theorem \ref{thm: misclassification_rate}, along with all other proofs in this paper, can be found in the supplementary material. We observe that the symmetric set difference $\Delta(\Csig[\Xbf], \Cest)$  is proportional to the difficulty of the clustering problem, as measured by the condition number.

Neither \eqref{eqn: graph_symmetric_set_difference} nor Theorem \ref{thm: misclassification_rate} imply consistent density cluster estimation in the sense of \eqref{eqn: consistent_density_cluster_recovery}. This notion of consistency requires a uniform bound over $\pbf$: namely, for all $\Cset' \in \Cbb_f(\lambda), \Cset' \neq \Cset$, and each $u \in \Cset, w \in \Cset'$,
\begin{equation}
\label{eqn: ppr_gap}
\frac{p_{w}}{\Dbf_{ww}} \leq \frac{1}{40\vol_0} < \frac{1}{11\vol_0} \leq \frac{p_u}{\Dbf_{uu}},
\end{equation}
so that any sweep cut $S_{\beta}$ for $\beta \vol_0 \in [1/40,1/11]$ (i.e. any sweep cut considered by Algorithm \ref{alg: ppr}) will fulfill both conditions laid out in \eqref{eqn: consistent_density_cluster_recovery}. In Theorem \ref{thm: consistent_recovery_of_density_clusters}, we show that a sufficiently small upper bound on $\kappa(\Cset)$ ensures such a gap exists with probability one as $n \to \infty$, and therefore guarantees $\Cest$ will be a consistent estimator. As was the case before, we wish to preclude arbitrarily low degree vertices, this time for points $x \in \Cset'[\Xbf]$.
\begin{enumerate}[label=(A\arabic*)]
	\setcounter{enumi}{6}
	\item 
	\label{asmp: C'_bounded_density}
	\emph{Bounded density:} Letting $\sigma,\lambda_{\sigma}$ be as in \ref{asmp: bounded_density}, for each $\Cset' \in \Cbb_f(\lambda)$ and for all $x \in \Csig'$, $\lambda_{\sigma} \leq f(x)$.
\end{enumerate}

\begin{theorem}
	\label{thm: consistent_recovery_of_density_clusters}
	Fix $\lambda > 0$, let $\Cset \in \Cbb_f(\lambda)$ be a $\kappa$-well conditioned cluster (with respect to some $\theta$), and additionally assume \ref{asmp: C'_bounded_density} holds. If Algorithm \ref{alg: ppr} is well-initialized, there exists a universal constant $c_5 > 0$ such that if
	\begin{equation}
	\label{eqn: kappa_ub}
	\kappa(\Cset) \leq c_5 \frac{\lambda_{\sigma}^2r^d \nu_d}{\Lambda_{\sigma}\Pbb(\Csig)},
	\end{equation}
	then the output set $\Cest \subseteq \Xbf$ is a consistent estimator for $\Cset$, in the sense of Definition \ref{def: consistent_density_cluster_estimation}.
\end{theorem}

\begin{remark}
	We note that the restriction on $\kappa(\Cset)$ imposed by \eqref{eqn: kappa_ub} results in a misclassification rate on the order of $r^d$. (See Theorem \ref{thm: misclassification_rate}). In plain terms, we are able to recover a density cluster $\Cset$ in the sense of \eqref{eqn: consistent_density_cluster_recovery} only when we can guarantee a very small fraction of points will be misclassified. This strong condition is the price we pay in order to obtain the uniform bound of \eqref{eqn: ppr_gap}.
\end{remark}

\begin{remark}
	While taking the radius of the neighborhood graph $r \to 0$ as $n \to \infty$---and thereby ensuring $G_{n,r}$ is sparse---is computationally attractive, the presence of a factor of $\frac{\log^2(1/r)}{r}$ in $\kappa(\Cset)$ unfortunately prevents us from making claims about the behavior of \pprspace in this regime. Although the restriction to a kernel function fixed in $n$ is standard for theoretical analysis of spectral clustering \cite{schiebinger2015,vonluxburg2008}, it is an interesting question whether \pprspace exhibits some degeneracy over $r$-neighborhood graphs as $r \to 0$, or if this is merely looseness in our upper bounds.
\end{remark}

\paragraph{Approximate \pprspace vector}

In practice, exactly solving \eqref{eqn: ppr_vector} may be too computationally expensive. To address this limitation, \citet{andersen2006} introduced the \emph{$\epsilon$-approximate \pprspace vector} (aPPR), which we will denote $\pbf^{(\epsilon)}$. We refer the curious reader to \cite{andersen2006} for a formal algorithmic definition of the a\pprspace vector, and limit ourselves to highlighting a few salient points. Namely, the aPPR vector can be computed in  order $\mathcal{O}\left(\frac{1}{\epsilon \alpha}\right)$ time, while satisfying the following uniform error bound:
\begin{equation}
\label{eqn: appr_error}
\textrm{for all $u \in V$}, \quad \pbf(u) - \epsilon \Dbf_{uu}\leq \pbf^{(\epsilon)}(u) \leq \pbf(u). 
\end{equation}

Application of \eqref{eqn: appr_error} within the proofs of Theorems \ref{thm: misclassification_rate} and \ref{thm: consistent_recovery_of_density_clusters} leads to analogous results which hold with respect to $\pbf^{(\epsilon)}$. We formally state and prove this fact in the supplementary material.

\section{Analysis}
\label{sec: analysis}

Given an arbitrary graph $G = (V,E)$ and subset $S \subseteq G$, \citet{zhu2013} bound the volume of $\Cest \setminus S$ and $S \setminus \Cest$ in terms of the normalized cut and inverse mixing time of $S$. The key to deriving the algorithmic results of the previous section is therefore to show that the geometric conditions \ref{asmp: bounded_density} - \ref{asmp: embedding} translate to meaningful bounds on the normalized cut and inverse mixing time of $\Csig[\Xbf]$ in $G_{n,r}$. In doing so, we expose how some of the geometric conditions introduced in Section \ref{sec: consistent_cluster_estimation_with_ppr} contribute to the difficulty of the clustering problem.

\paragraph{Normalized cut} We start with a finite sample upper bound on the normalized cut \eqref{eqn: normalized_cut} of 
$\Cset_\sigma[\Xbf]$. For simplicity, we write $\Phi_{n,r}(\Csig[\Xbf]) := \Phi(\Csig[\Xbf]; G_{n,r})$.

\begin{theorem}
	\label{thm: conductance_upper_bound}
	Fix $\lambda > 0$, and let $\Cset \in \Cbb_f(\lambda)$ satisfy
	Assumptions \ref{asmp: bounded_density}-\ref{asmp: low_noise_density}, and \ref{asmp: bounded_volume} for some 
	$r, \sigma, \lambda_{\sigma}, c_0, \gamma > 0$ (no bound on maximum density is needed). 
	Then for any $0 < \delta < 1$, $\epsilon > 0$, if
	\begin{equation}
	\label{eqn: conductance_sample_complexity}
	n \geq \frac{(2+\epsilon)^2\log(3/\delta)}{\epsilon^2}\left(\frac{25}
	{6 \lambda_{\sigma}^2\nu(\Csig) \nu_d r^d}\right)^2,
	\end{equation}
	then
	\begin{equation}
	\label{eqn: conductance_additive_error_bound}
	\frac{\Phi_{n,r}(\Csig[\Xbf])}{r} \leq c_1 \frac{d}{\sigma}
	\frac{\lambda}{\lambda_{\sigma}} \frac{(\lambda_{\sigma} -
		c_0\frac{r^{\gamma}}{\gamma+1})}{\lambda_{\sigma}} + \epsilon, 
	\end{equation}
	with probability at least $1-\delta$ (where $c_1 > 0$ is a universal constant).
\end{theorem}

\begin{remark}
	Observe that the diameter $\rho$ is absent from Theorem \ref{thm: conductance_upper_bound}, in contrast to the difficulty function $\kappa(\Cset)$, which worsens (increases) as $\rho$ increases. This phenomenon reflects established wisdom regarding spectral partitioning algorithms more generally \cite{guattery1995, hein2010}, albeit newly applied to the density clustering setting. It suggests that \pprspace may fail to recover $\Csig[\Xbf]$ even when $\Cset$ is sufficiently well-conditioned to ensure $\Csig[\Xbf]$ has a small normalized cut in $G_{n,r}$, if the diameter $\rho$ is large. This intuition will be supported by simulations in Section \ref{sec: experiments}.
\end{remark}

\paragraph{Inverse mixing time}
For $S \subseteq V$, denote by $G[S] = (S, E_S)$ the subgraph induced by 
$S$ (where the edges are $E_S = E \cap (S \times S)$). Let $\Wbf_S$ be the (lazy) random walk matrix over $G[S]$, and write 
$$
q_{v}^{(t)}(u) = e_v\Wbf_S^t e_u
$$
for the $t$-step transition probability of the lazy random walk over $G[S]$
originating at $v \in V$. Also write \smash{$\pi = (\pi(u))_{u \in S}$}
for the stationary distribution of this random walk.  (As
$\Wbf_S$ is the transition matrix of a lazy random walk, it is well-known that a unique stationary distribution exists and is given by
\smash{$\pi(u) = (\Dbf_S)_{uu}/\vol(S; G[S])$}, where $\Dbf_S$ is the degree matrix of $G[S]$.)

Then, the \emph{relative pointwise mixing time} of $G[S]$ is 
\begin{equation}
\label{eqn: mixing_time}
\tau_{\infty}(G[S]) = \min\set{ t: \frac{\pi(u) - q_{v}^{(t)}(u)
	}{\pi(u)} \leq \frac{1}{4}, 
	\; \text{for $u,v \in V$}}. 
\end{equation}
We lower bound the inverse mixing time $\Psi_{n,r}(\Csig[\Xbf]) := 1/\tau_{\infty}(G_{n,r}[\Csig[\Xbf]])$ of $\Csig[\Xbf]$, or equivalently we upper bound the mixing time.

\begin{theorem}
	\label{thm: mixing_time_upper_bound}
	Fix $\lambda > 0$, and let $\Cset \in \Cbb_f(\lambda)$ satisfy Assumptions \ref{asmp: bounded_density} and \ref{asmp: embedding} for some $\sigma, \lambda_{\sigma}, \Lambda_{\sigma}, \rho, L > 0$. Then, for any $0 < r < \sigma/2\sqrt{d}$, with probability $1$
	\begin{equation}
	\label{eqn: mixing_time_upper_bound}
	\limsup_{n \to \infty}\tau_{\infty}(G_{n,r}[\Csig[\Xbf]]) \leq c_2 \frac{\Lambda_{\sigma}^4 d^3 \rho^2 L^2}{\lambda_{\sigma}^4 r^2} \log^2\left(\frac{1}{r}\right) + c_3
	\end{equation}
	for $c_2,c_3 > 0$ universal constants. 
\end{theorem}

To the best of our knowledge, Theorem \ref{thm: mixing_time_upper_bound} is the first bound, albeit asymptotic, on the mixing time of random walks over neighborhood graphs which is independent of $n$, the number of vertices.
\begin{remark}
	The embedding assumption \ref{asmp: embedding} and Lipschitz parameter $L$ play an important role in proving the upper bound of Theorem \ref{thm: mixing_time_upper_bound}. There is some interdependence between $L$ and other geometric parameters $\sigma$ and $\rho$, which might lead one to hope that \ref{asmp: embedding} is non-essential. However, it is not possible to eliminate this condition without incurring an additional factor of at least $(\rho/\sigma)^d$ in \eqref{eqn: mixing_time_upper_bound}, achieved, for instance, when $\Csig$ is a dumbbell-like set consisting of two balls of diameter $\rho$ linked by a cylinder of radius $\sigma$.  \citep{abbasi-yadkori2016, abbasi-yadkori2016a} develop theory regarding biLipschitz deformations of convex sets, wherein it is observed that star-shaped sets as well as half-moon shapes of the type we consider in Section \ref{sec: experiments} both satisfy \ref{asmp: embedding} for reasonably small values of $L$.
\end{remark}

\section{Experiments}
\label{sec: experiments}

We provide numerical experiments to investigate the tightness of our bounds in on normalized cut and mixing time of $\Csig[\Xbf]$, and examine the performance of \pprspace on the 'two moons' dataset. For space reasons, we defer details of the experimental settings to the supplement.

\paragraph{Validating theoretical bounds}

\begin{figure}
	\centering
	\begin{adjustbox}{minipage=\linewidth}
		\begin{subfigure}{.33\linewidth}
			\includegraphics[width=\linewidth]{example1plots/sample2}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.33\linewidth}
			\includegraphics[width=\linewidth]{example1plots/sample1}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.33\linewidth}
			\includegraphics[width=\linewidth]{example1plots/sigma_normalized_cut_plot}
			\caption{}
		\end{subfigure}
		
		
		\begin{subfigure}{.33\linewidth}
			\includegraphics[width=\linewidth]{example1plots/sigma_mixing_time_plot}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.33\linewidth}
			\includegraphics[width=\linewidth]{example1plots/diameter_normalized_cut_plot}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.33\linewidth}
			\includegraphics[width=\linewidth]{example1plots/diameter_mixing_time_plot}
			\caption{}
		\end{subfigure}
		\caption{Samples, empirical results, and theoretical bounds for mixing time and normalized cut as diameter and thickness are varied. In (a) and (b), points in $\Cset$ are colored in red; points in $\Csig \setminus \Cset$ are colored in yellow; and remaining points in blue.}
		\label{fig:fig1}
	\end{adjustbox}
\end{figure}

As we do not provide any theoretical lower bounds, we investigate the tightness of Theorems \ref{thm: conductance_upper_bound} and \ref{thm: mixing_time_upper_bound} via simulation. Figure \ref{fig:fig1} shows these theoretical bounds compared to the empirical quantities \eqref{eqn: normalized_cut} and \eqref{eqn: mixing_time}, as we vary the diameter $\rho$ and thickness $\sigma$ of a cluster $\Cset$. Panels $(a)$ and $(b)$ show the resulting empirical clusters for two different values of $\rho$ and $\sigma$.

Panels $(d)$ and $(f)$ show our theoretical bounds on mixing time tracking closely with empirical mixing time, in both 2 and 3 dimensions.\footnote{Note that we have rescaled all values of theoretical upper bounds by a constant, in order to mask the effect of large universal constants in these bounds. Therefore only comparison of slopes, rather than intercepts, is meaningful.} This provides empirical evidence that the upper bound on mixing time given by Theorem \ref{thm: mixing_time_upper_bound} has the right dependency on both expansion parameter $\sigma$ and diameter $\rho$. The story in panels $(c)$ and $(e)$ is less obvious. We note that while, broadly speaking, the trends do not appear to match, this gap between theory and empirical results seems largest when $\sigma $ and $\rho$ are approximately equal. As the ratio $\rho/\sigma$ grows, we see the slopes of the empirical curves becoming more similar to those predicted by theory.

\paragraph{Empirical behavior of \ppr}

\begin{figure}
	\centering
	\begin{adjustbox}{minipage=\linewidth}
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth,scale = .5]{example2plots/row1_true_density_cluster}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row1_ppr_cluster}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row1_conductance_cluster}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row1_density_cluster}
			\caption{}
		\end{subfigure}
		
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row2_true_density_cluster}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row2_ppr_cluster}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row2_conductance_cluster}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row2_density_cluster}
			\caption{}
		\end{subfigure}
		
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row3_true_density_cluster}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row3_ppr_cluster}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row3_conductance_cluster}
			\caption{}
		\end{subfigure}
		\begin{subfigure}{.24\linewidth}
			\includegraphics[width=\linewidth]{example2plots/row3_density_cluster}
			\caption{}
		\end{subfigure}
		\caption{True density (column 1), \pprspace (column 2), normalized cut (column 3) and estimated density (column 4) clusters for 3 different simulated data sets. Seed node for \pprspace denoted by a black cross.}
		\label{fig:fig2}
	\end{adjustbox}
\end{figure}

To drive home the main implications of Theorems \ref{thm: misclassification_rate} and \ref{thm: consistent_recovery_of_density_clusters}, in Figure \ref{fig:fig2} we show the behavior of \ppr, normalized cut, and the density clustering algorithm of \citep{chaudhuri2010} on the well known 'two moons' dataset (with added 2d Gaussian noise), considered a prototypical success story for spectral clustering algorithms. The first column consists of the empirical density clusters $C_n$ and $C_n'$ for a particular threshold $\lambda$ of the density function; the second column shows the cluster recovered by \ppr; the third column shows the global minimum normalized cut, computed according to the algorithm of \cite{szlam2010}; and the last column shows a cut of the density cluster tree estimator of \citep{chaudhuri2010}.

Figure \ref{fig:fig2} shows the degrading ability of \pprspace to recover density clusters as the two moons become less well-separated. Of particular interest is the fact that \pprspace fails to recover one of the moons even when normalized cut still succeeds in doing so, and that a density clustering algorithm recovers a moon even when both \pprspace and normalized cut fail. 

\section{Discussion}
\label{sec: discussion}
For given data, there are an almost limitless number of ways to define what the 'right' clustering is. We have considered one such notion -- density upper level sets -- and have detailed a set of natural geometric criteria which, when appropriately satisfied, translate to provable bounds on estimation of the cluster by \ppr. We do not provide a theoretical lower bound showing that our geometric conditions are required for successful recovery on an upper level set. Although we investigate the matter empirically, this is a direction for future work.

\clearpage

\bibliographystyle{plainnat}
\bibliography{../local_spectral_bibliography}

\end{document}