%%%%%%%% ICML 2019 submission %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{icml2019}
% \usepackage[accepted]{icml2019}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[parfill]{parskip}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{xr-hyper}
%\usepackage[colorlinks=true,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}

\externaldocument{icml_local_density_clustering}

\newcommand{\diam}{\mathrm{diam}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\defeq}{\overset{\mathrm{def}}{=}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\Asig}{A_{\sigma}}
\newcommand{\Asigr}{A_{\sigma,\sigma + r}}
\newcommand{\1}{\mathbf{1}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\Err}{\mathrm{Err}}

\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\icmltitlerunning{Local clustering of density upper level sets}

\begin{document}

\twocolumn[
\icmltitle{Supplement to ``Local clustering of density upper level sets''}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alden Green}{cmu}
\icmlauthor{Sivaraman Balakrishnan}{cmu}
\icmlauthor{Ryan Tibshirani}{cmu}
\end{icmlauthorlist}

\icmlaffiliation{cmu}{Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh PA, USA}

\icmlcorrespondingauthor{Alden Green}{ajgreen@andrew.cmu.edu}

\icmlkeywords{local clustering}

\vskip 0.3in
]

\printAffiliationsAndNotice{} % otherwise use the standard text.

In this supplement, we present proofs for ``Local Clustering of Density Upper Level Sets''. We begin by providing technical lemmas, before moving on to proving the main results of the paper. 

\section{Technical Lemmas}
For $A \subset \mathcal{X}$, let $P(A) = \mathbb{P}_{X \sim P}(X \in A)$. To simplify expressions, we will write $A_{\sigma, \sigma + r} := \set{x: 0 < \rho(x, \Asig) \leq r}$. We further let $\widetilde{\mathcal{E}} = \abs{E(\Asig[\mathbf{X}], \mathbf{X} \setminus \Asig[\mathbf{X}]; G_{n,r})}$ be the number of edges between $\Asig[\mathbf{X}]$ and $\mathbf{X} \setminus \Asig[\mathbf{X}]$ in the graph $G_{n,r}$; $\widetilde{\mu} = \mathbb{E}\left[\widetilde{\mathcal{E}}\right]$ be the expected number of such edges; and $\widetilde{p} = \widetilde{\mu} / {n \choose 2}$ the probability of any two vertices $x_i$ and $x_j$ having such an edge. Similarly, $\mathcal{V} = \vol(\Asig[\mathbf{X}]; G_{n,r})$ is the volume of $\Asig[\mathbf{X}]$; $\mu = \mathbb{E}\left[\mathcal{V}\right]$ is the expected volume; and $p = \mu / {n \choose 2}$. Finally, we denote $rB = B(0,r)$.

\subsection{\textcolor{red}{Expected Values}}
\begin{lemma}
	\label{lem: expected_number_boundary_points}
	Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound}, and for any $r < \sigma$,
	\begin{equation*}
	P(A_{\sigma, \sigma + r}) \leq 2^{d-1} \nu(\Asig) \frac{rd}{\sigma}  \left(\tau_{\sigma} - \frac{r^{\gamma}}{\gamma + 1}\right)
	\end{equation*}	
\end{lemma}
\begin{proof}
	Recalling that $f$ is the density function for $P$, we have
	\begin{equation}
	\label{eqn: integral_over_epsilon_neighborhood}
	P(A_{\sigma, \sigma + r}) = \int_{A_{\sigma, \sigma + r}} f(x) dx
	\end{equation}
	Now, for $0 = t_0 < t_1 < \ldots < t_k = 1$, we divide up $A_{\sigma, \sigma + r} = \bigcup_{i = 0}^{k-1} \mathcal{T}_i$ where $\mathcal{T}_i = \set{x: rt_i < \rho(x, \Asig) \leq rt_{i+1}}$. We can rewrite the right hand side of (\ref{eqn: integral_over_epsilon_neighborhood}) as
	\begin{align*}
	\int_{A_{\sigma, \sigma + r}} f(x) dx & = \sum_{i = 0}^{k-1} \int_{\mathcal{T}_i} f(x) dx \\
	& \leq \sum_{i = 0}^{k-1} \nu(\mathcal{T}_i) \max_{x \in \mathcal{T}_i} f(x).
	\end{align*}
	By definition, 
	\begin{equation*}
	\nu(\mathcal{T}_i) = \nu(\Asig + rt_{i+1}B) - \nu(\Asig + rt_{i}B).
	\end{equation*}
	 Moreover, by \ref{asmp: cluster_regularity} and \ref{asmp: low_noise_density} we have
	 \begin{equation*}
	 \max_{x \in \mathcal{T}_i} f(x) \leq \tau_{\sigma} - (rt_i)^{\gamma}.
	 \end{equation*}
	 since for all $x \in \mathcal{T}_i$, $\rho(x, \Asig) > rt_i$.
	 Therefore
	\begin{align}
	\label{eqn: telescoping_sum}
	\sum_{i = 0}^{k-1} \int_{\mathcal{T}_i} f(x) dx & \leq \nonumber \sum_{i = 0}^{k-1} \biggl\{\nu(\Asig + rt_{i+1}B) - \\
	& \nu(\Asig + rt_{i}B)\biggr\} \left(\tau_{\sigma} - (rt_i)^{\gamma}\right).
	\end{align}
	
	Now, we have that $\sigma B \subset \Asig$  which implies, 
	\begin{equation*}
	\nu(\Asig + rt_iB) \leq \nu(\Asig + \frac{r t_i}{\sigma} \Asig)
	\end{equation*}
	and we therefore have the upper bound
	\begin{align}
	\label{eqn: riemann_sum_lem1}
	& \sum_{i = 0}^{k-1} \biggl\{\nu(\Asig + rt_{i+1}B) - \nu(\Asig + rt_iB)\biggr\} \left(\tau_{\sigma} - (rt_i)^{\gamma}\right) \nonumber \\
	& \leq \sum_{i = 0}^{k-1} \biggl\{\nu(\Asig + \frac{r t_{i+1}}{\sigma} \Asig) - \nu(\Asig + \frac{r t_i}{\sigma} \Asig)\biggr\} \left(\tau_{\sigma} - (rt_i)^{\gamma}\right) \nonumber \\
	& = \nu(\Asig) \sum_{i = 0}^{k-1} \biggl\{ (1 + \frac{r t_{i+1}}{\sigma})^d - (1 + \frac{r t_{i}}{\sigma})^d \biggr\} \left(\tau_{\sigma} - (rt_i)^{\gamma}\right) 
	\end{align}
	where the upper bound holds because $\tau_{\sigma} - (rt)^{\gamma}$ is decreasing in $t$.
	
	Let $t_{i} = i/k$ for $i = 0, \ldots, k$. Taking the limit as $k \to \infty$, we have
	\begin{align*}
	& \lim_{k \to \infty} \sum_{i = 0}^{k-1} \biggl\{ \left(1 + \frac{r (i+1)}{k\sigma}\right)^d - \left(1 + \frac{r i}{k\sigma}\right)^d \biggr\} \left(\tau_{\sigma} - \left(\frac{ri}{k}\right)^{\gamma}\right)  \\
	 & = \int_{0}^{1} \frac{rd}{\sigma} (1 + \frac{r t}{\sigma})^{d-1} (\tau_{\sigma} - (rt)^{\gamma}) dt \\
	& \leq 2^{d-1} \frac{rd}{\sigma} \left(\tau_{\sigma} - \frac{r^{\gamma }}{\gamma + 1}\right)
	\end{align*}
	where the inequality comes from $t \leq 1$ and $r < \sigma$. 
	
	Finally, note that (\ref{eqn: riemann_sum_lem1}) holds for any $k$ and arbitrary $0 = t_0 < t_1 < \ldots < t_k = 1$. In particular, it holds for $t_{i} = i/k$ for $i = 0, \ldots, k$, and in the limit as $k \to \infty$. Therefore, we have
	\begin{equation*}
	P(A_{\sigma, \sigma + r}) \leq \nu(\Asig) 2^{d-1} \frac{rd}{\sigma}  \left(\tau_{\sigma} - \frac{r^{\gamma}}{\gamma + 1}\right)
	\end{equation*}
	which is exactly the stated result of Lemma \ref{lem: expected_number_boundary_points}.
\end{proof}

\begin{lemma}
	\label{lem: expected_density_cut}
	Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound}, and for any $r < \sigma$,
	\begin{equation*}
	\widetilde{p} \leq \frac{2^d d}{\sigma} \nu(\Asig) \nu_d r^{d+1} \tau \left(\tau_{\sigma} - \frac{r^{\gamma}}{\gamma + 1}\right)
	\end{equation*}
\end{lemma}
\begin{proof}
	We can write $\widetilde{\mathcal{E}}$ as the sum of indicator functions,
	\begin{equation}
	\label{eqn: density_cut_expansion}
	\widetilde{\mathcal{E}} = \sum_{i = 1}^{n} \sum_{j = 1}^{n} \1(x_i \in \Asigr) \1(x_j \in B(x_i,r) \cap \Asig)
	\end{equation}
	Ignoring the cross terms (which are zero), normalizing by $1/{n \choose 2}$, and taking expectation, we have
	\begin{align*}
	\frac{\widetilde{\mu}}{{n \choose 2}} & = 2 \int_{\Asigr} f(x) \left\{ \int_{B(x,r) \cap \Asig} f(x') dx'\right\} dx \\
	& \overset{(i)}{\leq} 2 \int_{\Asigr} f(x) \nu_d r^d \tau dx \\
	& \overset{(ii)}{\leq} 2^d \nu_d r^d \tau \nu(A_\sigma) \frac{rd}{\sigma} \left(\tau_{\sigma} - \frac{r^{\gamma}}{\gamma + 1}\right) 
	\end{align*}
	where $(i)$ follows from Assumption \ref{asmp: cluster_separation}, which implies $f(x') \leq \tau$ for all $x' \in \Asig \setminus A$, and $(ii)$ follows from Lemma \ref{lem: expected_number_boundary_points}.
\end{proof}

\begin{lemma}
	\label{lem: expected_density_volume}
	Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound},
	\begin{equation*}
	p \geq 2 \tau_{\sigma}^2 \nu(\Asig) \nu_d \left(\frac{r}{2}\right)^d
	\end{equation*}
\end{lemma}
\begin{proof}
	We can write $\mathcal{V}$ as the sum of indicator functions,
	\begin{equation}
	\label{eqn: volume_expansion}
	\mathcal{V} = \sum_{i = 1}^{n} \sum_{j = 1}^{n} \1(x_i \in \Asig) \1(x_j \in B(x_i, r))
	\end{equation}
		Ignoring the cross terms (which are zero), normalizing by $1/{n \choose 2}$, and taking expectation, we have
	\begin{equation}
	\label{eqn: expected_volume_integral}
	\frac{\mu}{{n \choose 2}} = 2 \int_{\Asig} f(x) \left\{\int_{B(x,r)} f(x') dx' \right\} dx
	\end{equation}
	For $x \in \Asig$, take $x_0 \in A$ such that $\norm{x - x_0} = \rho(x,A)$ (note that such a minimizer exists because $A$ is closed). Then, by the triangle inequality, we have
	\begin{equation*}
	B\left(\frac{x + x_0}{2}, \frac{r}{2}\right) \in \Asig \cap B(x,r)
	\end{equation*}
	Recall that by (\ref{asmp: cluster_regularity}), we have $f(x') \geq \tau_{\sigma}$ for all $x' \in \Asig$. We can therefore lower bound the right hand side of (\ref{eqn: expected_volume_integral}) by
	\begin{align*}
	& 2\int_{\Asig} f(x) \tau_{\sigma} \nu_d \left(\frac{r}{2}\right)^{d} dx \\
	& \leq 2\tau_{\sigma}^2 \nu(\Asig) \nu_d \left(\frac{r}{2}\right)^d.
	\end{align*}
\end{proof}

\subsection{\textcolor{red}{Concentration inequalities.}}

Given a symmetric kernel function $k: \mathcal{X}^m \to \Reals$, and data $\set{x_1, \ldots, x_n}$, we define the \textit{order-$m$ $U$ statistic} to be 
\begin{equation*}
U := \frac{1}{ {n \choose m} } \sum_{1 \leq i_1 < \ldots < i_m \leq n} k(x_{i_1},\ldots,x_{i_m})
\end{equation*}

For both Lemmas \ref{lem: bounded_difference} and \ref{lem: bernstein}, let $X_1, \ldots, X_n \in \mathcal{X}$ be independent and identically distributed. We will additionally assume the order-$m$ kernel function $k$ satisfies the boundedness property $\sup_{x_1, \ldots, x_m} \abs{k(x_1, \ldots, x_m)} \leq 1$. 

\begin{lemma}[Hoeffding's inequality for $U$-statistics.]
	\label{lem: bounded_difference}
	For any $t > 0$,
	\begin{equation*}
	\mathbb{P}(\abs{U - \mathbb{E}U} \geq t) \leq 2 \exp\left\{- \frac{2nt^2}{m}\right\}
	\end{equation*}
	Further, for any $\delta > 0$, we have
	\begin{align*}
	U & \leq \mathbb{E}U + \sqrt{\frac{m \log(1 / \delta)}{2n} }, \\
	U & \geq \mathbb{E}U - \sqrt{\frac{m \log(1 / \delta)}{2n} }
	\end{align*}
	each with probability at least $1 - \delta$. 
\end{lemma}

\begin{lemma}[Bernstein's inequality for $U$-statistics]
	\label{lem: bernstein}
	Additionally, assume $\sigma^2 = \var\left(k(X_1, \ldots, X_m) \right) < \infty$. Then for any $\delta > 0$, 
	\begin{align*}
	\mathbb{P}(U - \mathbb{E}U \geq t) \leq \exp\left\{-\frac{n}{2m}\frac{t^2}{\sigma^2 + t/3}\right\},
	\end{align*}
	
	Moreover if $\sigma^2 \leq \mu/n$, 
	\begin{align*}
	U & \leq \mathbb{E}U \cdot \left(1 + \max\left\{ \sqrt{\frac{2m\log(1/\Delta)}{\mu}}, \frac{2m \log(1/\Delta)}{3\mu} \right\}\right), \\
	U & \geq \mathbb{E}U \cdot \left(1 - \max\left\{ \sqrt{\frac{2m\log(1/\Delta)}{\mu}}, \frac{2m \log(1/\Delta)}{3\mu} \right\}\right)
	\end{align*}
	each with probability at least $1 - \Delta$.
\end{lemma}

\section{Proof of Theorem \ref{thm: conductance_upper_bound}}

Given the previous lemmas, the proof of Theorem \ref{thm: conductance_upper_bound} is straightforward. We rely on Lemmas \ref{lem: expected_density_cut} and \ref{lem: expected_density_volume} to bound $\widetilde{\mu}$ and $\mu$, respectively, and Lemma \ref{lem: bernstein} to bound the deviations $\widetilde{\mathcal{E}} - \widetilde{\mu}$ and $\mathcal{V} - \mu$ with high probability.

\subsection{Numerator of $\Phi_{n,r}(\Asig[\mathbf{X}])$.}
From (\ref{eqn: density_cut_expansion}), we can see that $\widetilde{\mathcal{E}}$, properly scaled, can be expressed as an order-$2$ $U$-statistic,
\begin{equation*}
\frac{1}{{n \choose 2}}\widetilde{\mathcal{E}} = \frac{1}{{n \choose 2}} \sum_{1 \leq i < j \leq n} \widetilde{k}(x_i, x_j)
\end{equation*}
where 
\begin{align*}
\tilde{k}(x_i,x_j) = & \1(x_i \in \Asigr) \1(x_j \in B(x_i,r) \cap \Asig) + \\
& \1(x_j \in \Asigr) \1(x_i \in B(x_j,r) \cap \Asig)
\end{align*}

From Lemma \ref{lem: bounded_difference} we therefore have
\begin{equation}
\label{eqn: numerator_additive_bound}
\frac{\widetilde{\mathcal{E}}}{{n \choose 2}} \leq \widetilde{p} + \sqrt{\frac{\log(1/\delta)}{n}}
\end{equation}
with probability at least $1 - \delta$. 


\textcolor{red}{\textbf{Multiplicative bound}: As $\tilde{k}(x_1,x_2)$ is the sum of two Bernoulli random variables with negative covariance (since $\1(x_i \in \Asigr) \1(x_j \in B(x_i,r) \cap \Asig) = 1$ implies $\1(x_j \in \Asigr) \1(x_i \in B(x_j,r) \cap \Asig) = 0$ and vice versa), we can upper bound $\var\left(\tilde{k}(x_1, x_2)\right) \leq  \widetilde{p}$, where we recall 
\begin{equation*}
\widetilde{p} = 2\cdot \mathbb{P}\left(\1(x_1 \in \Asigr) \1(x_2 \in B(x_1,r) \cap \Asig)\right)
\end{equation*}
From Lemma \ref{lem: bernstein}, we therefore have
\begin{equation*}
\frac{\widetilde{\mathcal{E}}}{{n \choose 2}} \leq \widetilde{p} + \max\left\{ \sqrt{\frac{4\log(1/\Delta)\widetilde{p}}{n}}, \frac{4 \log(1/\Delta)}{3n} \right\}
\end{equation*}
with probability at least $1 - \Delta$.}

\paragraph{Denominator of $\Phi_{n,r}(\Asig[\mathbf{X}])$.}
We follow a very similar set of steps as above.

By (\ref{eqn: density_cut_expansion}), we see that $\mathcal{V}$ can also be expressed as an order-2 $U$-statistic,
\begin{equation*}
\frac{\mathcal{V}}{ {n \choose 2} } = \frac{1}{{n \choose 2}} \sum_{1 \leq i < j \leq n } k'(x_i, x_j)
\end{equation*}
with
\begin{align*}
k'(x_i,x_j)  = & \1(x_i \in A_\sigma)\1(x_j \in B(x_i,r)) ~ + \\
& \1(x_j \in A_{\sigma})\1(x_i \in B(x_j,r))
\end{align*}

From Lemma \ref{lem: bounded_difference} we therefore have
\begin{equation}
\label{eqn: denominator_additive_bound}
\frac{\mathcal{V}}{{n \choose 2}} \geq p - \sqrt{\frac{\log(1/\delta)}{n}}
\end{equation}
with probability at least $1 - \delta$.

\textcolor{red}{Multiplicative bound: The two terms on the right hand side are both distributed $\mathrm{Bernoulli}(p/2)$. Moreover, since $\1(x_i \in A_\sigma) = 1$ implies $\1(x_j \in A_{\sigma}) = 0$, they have negative covariance. We can therefore upper bound $\var(k'(x_i,x_j)) \leq p$, and so from Lemma \ref{lem: bernstein}, we have
\begin{equation*}
\frac{\mathcal{V}}{{n \choose 2}} \geq p - \max\left\{ \sqrt{\frac{4\log(1/\Delta)p}{n}}, \frac{4 \log(1/\Delta)}{3n} \right\}
\end{equation*}
with probability at least $1 - \Delta$. }

\paragraph{Proof of the additive error bound.}
Noting that $\Phi_{n,r}(\Asig[\mathbf{X}]) = \mathcal{\widetilde{E}} / \mathcal{V}$, and multiplying and dividing by ${n \choose 2}$, we have
\begin{equation}
\label{eqn: conductance_representation_1}
\Phi_{n,r}(\Asig[\mathbf{X}]) = \frac{\widetilde{p} + \left(\frac{\widetilde{\mathcal{E}}}{{n \choose 2}} - \widetilde{p}\right)}{p + \left(\frac{\widetilde{\mathcal{V}}}{{n \choose 2}} - p \right)}
\end{equation}
We assume (\ref{eqn: numerator_additive_bound}) and (\ref{eqn: denominator_additive_bound}) hold, keeping in mind that this will happen with probability at least $1 - 2\delta$. Along with (\ref{eqn: conductance_representation_1}) this means
\begin{equation*}
\Phi_{n,r}(\Asig[\mathbf{X}]) \leq \frac{\widetilde{p} + \Err_n}{p - \Err_n}
\end{equation*}
for $\Err_n = \sqrt{\frac{\log(1/\delta)}{n}}$.
Now, some straightforward algebraic manipulations yield
\begin{align*}
\frac{\widetilde{p} + \Err_n}{p - \Err_n} & = \frac{\widetilde{p}}{p} + \left(\frac{\widetilde{p}}{p - \Err_n} - \frac{\widetilde{p}}{p}\right) + \frac{\Err_n}{p - \Err_n} \\
& = \frac{\widetilde{p}}{p} + \frac{\Err_n}{p - \Err_n}\left(\frac{\widetilde{p}}{p} + 1\right) \\
& \leq \frac{\widetilde{p}}{p} + 2 \frac{\Err_n}{p - \Err_n}.
\end{align*}
Finally, combining the upper bound given by Lemma \ref{lem: expected_density_volume} with the lower bound on $n$ specified in the statement of Theorem \ref{thm: conductance_upper_bound}, we have
\begin{equation*}
2 \frac{\Err_n}{p - \Err_n} \leq \epsilon
\end{equation*}
By Lemmas \ref{lem: expected_density_cut} and Lemma \ref{lem: expected_density_volume}, we have
\begin{equation*}
\frac{\widetilde{p}}{p} \leq C_{\sigma} \frac{\tau}{\tau_{\sigma}} \frac{(\tau_{\sigma} - \frac{r^{\gamma+1}}{\gamma+1})}{\tau_{\sigma}}
\end{equation*}
and thus we have shown (\ref{eqn: conductance_additive_error_bound}) occurs with probability at least $1 - 2\delta$. Plugging in $\delta' = \delta/2$ gives the exact statement in Theorem \ref{thm: conductance_upper_bound}.

\end{document}