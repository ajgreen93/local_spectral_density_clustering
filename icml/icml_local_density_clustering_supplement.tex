%%%%%%%% ICML 2019 submission %%%%%%%%%%%%%%%%%

\documentclass{article}

% \usepackage{icml2019}
% \usepackage[accepted]{icml2019}

\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\theequation}{A.\arabic{equation}}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
%\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[parfill]{parskip}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{xr-hyper}
\usepackage{bm}
\usepackage[colorlinks=true,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}

\externaldocument{icml_local_density_clustering}

\newcommand{\diam}{\mathrm{diam}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\defeq}{\overset{\mathrm{def}}{=}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\1}{\mathbf{1}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\Err}{\mathrm{Err}}

%%% Graph terms
\newcommand{\cut}{\mathrm{cut}}

%%% Vectors
\newcommand{\pbf}{\mathbf{p}}
\newcommand{\ebf}[1]{\mathbf{e}_{#1}}
\newcommand{\pibf}{\bm{\pi}}
\newcommand{\rhobf}{\bm{\rho}}

%%% Matrices
\newcommand{\Abf}{\mathbf{A}}
\newcommand{\Xbf}{\mathbf{X}}
\newcommand{\Wbf}{\mathbf{W}}
\newcommand{\Lbf}{\mathbf{L}}
\newcommand{\Dbf}{\mathbf{D}}
\newcommand{\Ibf}[1]{\mathbf{I}_{#1}}

%%% Probability distributions (and related items)
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Cbb}{\mathbb{C}}
\newcommand{\Ebb}{\mathbb{E}}

%%% Sets
\newcommand{\Sset}{\mathcal{S}}
\newcommand{\Cset}{\mathcal{C}}
\newcommand{\Aset}{\mathcal{A}}
\newcommand{\Asig}{\Aset_{\sigma}}
\newcommand{\Csig}{\Cset_{\sigma}}
\newcommand{\Asigr}{\Aset_{\sigma,\sigma + r}}
\newcommand{\Csigr}{\Cset_{\sigma,\sigma + r}}

%%% Operators
\DeclareMathOperator*{\argmin}{arg\,min}


%%% Algorithm notation
\newcommand{\ppr}{{\sc PPR}}
\newcommand{\pprspace}{{\sc PPR~}}


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}


%\newcommand{\theHalgorithm}{\arabic{algorithm}}


%\icmltitlerunning{Local clustering of density upper level sets}

\begin{document}

%\twocolumn[
%\icmltitle{Supplement to ``Local clustering of density upper level sets''}

%\icmlsetsymbol{equal}{*}

%\begin{icmlauthorlist}
%\icmlauthor{Alden Green}{cmu}
%\icmlauthor{Sivaraman Balakrishnan}{cmu}
%\icmlauthor{Ryan Tibshirani}{cmu}
%\end{icmlauthorlist}

%\icmlaffiliation{cmu}{Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh PA, USA}

%\icmlcorrespondingauthor{Alden Green}{ajgreen@andrew.cmu.edu}

%\icmlkeywords{local clustering}

%\vskip 0.3in
%]

%\printAffiliationsAndNotice{} % otherwise use the standard text.

\section{Proofs}

In this supplement, we present proofs for ``Local Clustering of Density Upper Level Sets''. We begin by providing technical lemmas, before moving on to proving the main results of the paper. 

Throughout, we will fix $\Aset \subset \Rd$ to be an arbitrary set. To simplify expressions, for the $\sigma$-expansion $\Asig$, we will write the set difference between $\Asig$ and the $(\sigma + r)$-expansion $\Aset_{\sigma + r}$ as 
\begin{equation*}
\Asigr := \set{x: 0 < \rho(x, \Asig) \leq r},
\end{equation*}
where $\rho(x, \Aset) = \min_{x' \in \Aset} \norm{x - x'}$.

For notational ease, we write
\begin{align*}
\cut_{n,r} = \cut(\Csig[\Xbf]; G_{n,r}), ~ \mu_K = \mathbb{E}(\cut_{n,r}), ~ p_K = \frac{\mu_K}{{n \choose 2}} \\
\vol_{n,r} = \vol(\Csig[\Xbf]; G_{n,r}), ~ \mu_V = \mathbb{E}(\vol_{n,r}), ~ p_V = \frac{\mu_V}{{n \choose 2}}
\end{align*}
for the random variable, mean, and probability of cut size and volume, respectively.

\subsection{Technical Lemmas}

We state Lemma \ref{lem: expansion_sets} without proof, as it is trivial. We formally include it mainly to comment on its (potential) suboptimality; for sets $\Aset$ with diameter much larger than $\sigma$, the volume estimate of Lemma \ref{lem: expansion_sets} will be quite poor. 

\begin{lemma}
	\label{lem: expansion_sets}
	For any $\sigma > 0$ and the $\sigma$-expansion $\Asig = \Aset + \sigma B$, 
	\begin{equation*}
	\sigma B \subset \Asig, ~~\mathrm{and~ }\nu(\Aset + \sigma B) \leq \nu((1 + \sigma)\Aset) = (1 + \sigma)^d \nu(\Aset).
	\end{equation*}
\end{lemma}

We will need to carefully control the volume of the expansion set using the above estimate; Lemma \ref{lem: Taylor_series} serves this purpose.
\begin{lemma}
	\label{lem: Taylor_series}
	For any $0 \leq x \leq 1/2d$,
	\begin{equation*}
	(1 + x)^d \leq 1 + 2dx.
	\end{equation*}
\end{lemma}
The proof of Lemma \ref{lem: Taylor_series} is based on approximation via Taylor series, and we omit it.

We will repeatedly employ Lemma \ref{lem: expansion_sets} and Lemma \ref{lem: Taylor_series} in tandem. As a first example, in Lemma \ref{lem: interior_of_expansion_sets}, we use it to bound the ratio of $\nu(\Asig)$ to $\nu(\Aset_{\sigma - r})$. This will be useful when we bound $\vol(\Csig)$.

\begin{lemma}
	\label{lem: interior_of_expansion_sets}
	For $\sigma$, $\Asig$ as in Lemma \ref{lem: expansion_sets}, let $r > 0$ satisfy $r \leq \sigma/4d$. Then,
	\begin{equation*}
	\frac{\nu(\Asig)}{\nu(\Aset_{\sigma - r})} \leq 2.
	\end{equation*}
\end{lemma}
\begin{proof}
	Fix $q = \sigma - r$. Then,
	\begin{align*}
	\nu(\Asig) & = \nu(\Aset_{q + \sigma - q}) = \nu(\Aset_q + (\sigma - q)B ) \\
	& \leq \nu(\Aset_q + \frac{(\sigma - q)}{q} \Aset_q) = \left(1 + \frac{\sigma - q}{q}\right)^d \nu(\Aset_q)
	\end{align*}
	where the inequality follows from Lemma \ref{lem: expansion_sets}. Of course, $\sigma - q = r$, and $\frac{r}{q} \leq \frac{1}{2d}$ for $r \leq \frac{1}{4d}$. The claim then follows from Lemma \ref{lem: Taylor_series}.
\end{proof}

As part of the proof of Theorem \ref{thm: inverse_mixing_time_lower_bound}, we will require an estimate of a function $g(t)$ for $t \in [x_0, 1/2]$ for some $x_0 > 0$. For $m > 0$ and $x_0 = t_0 < t_1 < \ldots < t_m = 1/2$, define the \emph{stepwise approximation to $g$} to be $\bar{g}$, given by 
\begin{equation}
\label{eqn: stepwise_approximation}
\bar{g}(t) = g(t_i), ~~ \text{ for $t \in [t_{i-1}, t_i]$ }
\end{equation}
\begin{lemma}
	\label{lem: stepwise_approximation}
	Fix
	\begin{equation*}
	g(t) = \log\left(\frac{1}{t}\right) \text{ for $x \in [x_0, 1/2]$}
	\end{equation*}
	 If for all $i$ in $1,\ldots,m$, $(t_i - t_{i - 1}) \leq x_0/2$, then $g(t) \geq \bar{g}(t)\geq g(t) / 2$.
\end{lemma}
\begin{proof}
	The upper bound $g(t) \geq \bar{g}(t)$ follows immediately from the fact that $g(t)$ is a decreasing function.
	
	By the concavity of the $\log$ function, 
	\begin{equation*}
	\bar{g}(t) = \log\left(\frac{1}{t_i}\right) \geq \log\left(\frac{1}{t}\right) - \frac{(t_i - t)}{t}.
	\end{equation*}
	As a result,
	\begin{equation*}
	\bar{g}(t) - \frac{g(t)}{2} \geq \frac{\log\left(\frac{1}{t}\right)}{2} - \frac{(t_i - t)}{t} \geq 1/2 - 1/2 = 0.
	\end{equation*}
\end{proof}

The proof of Theorem \ref{thm: inverse_mixing_time_lower_bound} also on a parameter -- which we term \emph{discrete local spread} -- to handle the mixing over very small steps. Formally, the discrete local spread $\pi_1(G)$ is given by
\begin{equation}
\label{eqn: local_spread}
\pi_1(G) := \frac{d_{\min}(G)^2}{10 \vol(V; G)} 
\end{equation}
where $d_{\min}(G) = \min_{v \in V} d(v)$ is the minimum degree in $G$. Intuitively, the discrete local spread gauges how much the walk given by $\Wbf$ has mixed after one step, starting from any node $v$. We will denote $\pi_1(G_{n,r}[\Csig[\Xbf]])$ by $\widetilde{\pi}_{1,n}$. 

\begin{lemma}
	\label{lem: local_spread_lb}
	For $\Csig$ satisfying the conditions of Theorem \ref{thm: inverse_mixing_time_lower_bound}:
	\begin{equation*}
	\liminf_{n \to \infty}~ \widetilde{\pi}_{1,n} \geq \frac{\lambda_{\sigma}^2}{\Lambda_{\sigma}^2} \frac{r^d}{(2D)^d}
	\end{equation*}
\end{lemma}

\textcolor{red}{Prove Lemma \ref{lem: local_spread_lb}}.

\subsection{Cut and volume estimates}
\begin{lemma}
	\label{lem: expected_number_boundary_points}
	Under the conditions of Theorem \ref{thm: conductance_upper_bound}, and for any $r < \sigma/2d$,
	\begin{equation*}
	\Pbb(\Csigr) \leq 2 \nu(\Csig) \frac{rd}{\sigma}  \left(\lambda_{\sigma} - \frac{r^{\gamma}}{\gamma + 1}\right)
	\end{equation*}	
\end{lemma}
\begin{proof}
	Recalling that $f$ is the density function for $\Pbb$, we have
	\begin{equation}
	\label{eqn: integral_over_epsilon_neighborhood}
	\Pbb(\Csigr) = \int_{\Csigr} f(x) dx
	\end{equation}
	We partition $\Csigr$ into slices, based on distance from $\Csig$, as follows: for $k \in \N$,
	\begin{equation*}
	\mathcal{T}_{i,k} = \set{x \in \Rd: t_{i,k} < \frac{\rho(x, \Csig)}{r} \leq t_{i+1,k}}, ~~ \Csigr = \bigcup_{i = 0}^{k-1} \mathcal{T}_{i,k}
	\end{equation*}
	where $t_i = i/k$ for $i = 0, \ldots, k - 1$. As a result,
	\begin{equation*}
	\int_{\Csigr} f(x) dx = \sum_{i = 0}^{k-1} \int_{\mathcal{T}_{i,k}} f(x) dx \leq \sum_{i = 0}^{k-1} \nu(\mathcal{T}_{i,k}) \max_{x \in \mathcal{T}_{i,k}} f(x).
	\end{equation*}
	We substitute
	\begin{equation*}
	\nu(\mathcal{T}_{i,k}) = \nu(\Csig + rt_{i+1,k}B) - \nu(\Csig + rt_{i,k}B) := \nu_{i+1,k} - \nu_{i,k}. 
	\end{equation*}
	where for simplicity we've written $\nu_{i,k} = \nu(\Csig + rt_{i+1,k}B)$.
	This, in concert with the upper bound
	\begin{equation*}
	\max_{x \in \mathcal{T}_{i,k}} f(x) \leq \lambda_{\sigma} - (rt_{i,k})^{\gamma},
	\end{equation*}
	which follows from \ref{asmp: bounded_density} and \ref{asmp: low_noise_density}, yields
	\begin{align}
	\label{eqn: telescoping_sum}
	\sum_{i = 0}^{k-1} \nu(\mathcal{T}_{i,k}) \max_{x \in \mathcal{T}_{i,k}} f(x) & \leq \sum_{i = 0}^{k-1} \biggl\{ \nu_{i+1,k} - \nu_{i,k} \biggr\} \biggl( \lambda_{\sigma} - (rt_{i,k})^{\gamma} \biggr) \nonumber \\
	& = \sum_{i = 1}^{k} 
	\underbrace{\nu_{i,k} \biggl( \left[\lambda_{\sigma} - (rt_{i,k})^{\gamma}\right] -  \left[\lambda_{\sigma} - (rt_{i-1,k})^{\gamma}\right]\biggr)}_{:= \Sigma_k} + \underbrace{\biggl(\nu_{k,k}\left[\lambda_{\sigma} - r^{\gamma}\right] - \nu_{1,k}\lambda_{\sigma} \biggr)}_{:= \xi_k}
	\end{align}
	
	We first consider the term $\Sigma_k$. Here we use Lemma \ref{lem: expansion_sets} to upper bound
	\begin{equation*}
	\nu_{i,k} \leq \vol(\Csig)\left(1 + \frac{rt_{i,k}}{\sigma}\right)^d
	\end{equation*}
	and so we can in turn upper bound $\Sigma_k$:
	\begin{equation}
	\label{eqn: Sigmak_riemann_sum}
	\Sigma_k \leq \vol(\Csig) r^\gamma \sum_{i = 1}^{k} \left(1 + \frac{rt_{i,k}}{\sigma}\right)^d \biggl( (t_{i-1,k})^{\gamma} - (t_{i,k})^{\gamma}\biggr).
	\end{equation}
	This, of course, is a Riemann sum, and as the inequality holds for all values of $k$ it holds in the limit as well, which we compute to be
	\begin{align*}
	\lim_{k \to \infty} \sum_{i = 1}^{k} \left(1 + \frac{rt_{i,k}}{\sigma}\right)^d \biggl( (t_{i-1,k})^{\gamma} - (t_{i,k})^{\gamma}\biggr) & = \gamma \int_{0}^{1} \left(1 + \frac{rt}{\sigma}\right)^d t^{\gamma - 1} dt \\
	& \overset{(i)}{\leq} \gamma \int_{0}^{1} \left(1 + \frac{2drt}{\sigma}\right) t^{\gamma - 1} dt = \left(1 + \frac{\gamma 2dr}{\gamma + 1}\right).
	\end{align*}
	where $(i)$ follows from Lemma \ref{lem: Taylor_series}. 
	We plug this estimate in to \eqref{eqn: Sigmak_riemann_sum} and obtain
	\begin{equation*}
	\lim_{k \to \infty} \Sigma_k \leq \vol(\Csig) r^{\gamma} \left(1 + \frac{\gamma 2dr}{\gamma + 1}\right).
	\end{equation*}
	
	We now provide an upper bound on $\xi_k$. It will follow the same basic steps as the bound on $\Sigma_k$, but will not involve integration:
	\begin{align*}
	\xi_k & \overset{(ii)}{\leq} \nu(\Csig) \biggl\{ \left(1 + \frac{r}{\sigma}\right)^d(\lambda - r^{\gamma}) - \lambda \biggr\} \\
	& \overset{(iii)}{\leq} \nu(\Csig) \biggl\{ \left(1 + \frac{2dr}{\sigma}\right)(\lambda - r^{\gamma}) - \lambda \biggr\} = \nu(\Csig) \biggl\{ \frac{2dr}{\sigma}(\lambda - r^{\gamma}) - r^{\gamma} \biggr\}.
	\end{align*}
	where $(ii)$ follows from Lemma \ref{lem: expansion_sets} and $(iii)$ from Lemma \ref{lem: Taylor_series}. The final result comes from adding together the upper bounds on $\Sigma_k$ and $\xi_k$ and taking the limit as $k \to \infty$.
\end{proof}

\begin{lemma}
	\label{lem: expected_density_cut}
	Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound}, and for any $r < \sigma/2d$,
	\begin{equation*}
	p_K \leq \frac{4 \lambda \nu_d r^{d+1} \nu(\Csig)d}{\sigma}  \left(\lambda_{\sigma} - \frac{r^{\gamma}}{\gamma + 1}\right)
	\end{equation*}
\end{lemma}
\begin{proof}
	We can write $\cut_{n,r}$ as the sum of indicator functions,
	\begin{equation}
	\label{eqn: density_cut_expansion}
	\cut_{n,r} = \sum_{i = 1}^{n} \sum_{j = 1}^{n} \1(x_i \in \Csigr) \1(x_j \in B(x_i,r) \cap \Csig)
	\end{equation}
	and by linearity of expectation, we can obtain
	\begin{equation*}
	p_K = \frac{\mu_K}{{n \choose 2}} = 2 \cdot \Pbb(x_i \in \Csigr, x_j \in B(x_i,r) \cap \Csig)
	\end{equation*}
	Writing this with respect to the density function $f$, we have
	\begin{align*}
	p_K & = 2 \int_{\Csigr} f(x) \left\{ \int_{B(x,r) \cap \Csig} f(x') dx' \right\} dx \\
	& \leq 2 \nu_d r^d \lambda  \int_{\Csigr} f(x) dx
	\end{align*}
	where the inequality follows from Assumption \ref{asmp: cluster_separation}, which implies that the density function $f(x') \leq \lambda$ for all $x' \in \Csig \setminus \Cset$ (otherwise, $x'$ would be in some $\Cset' \in \Cbb_f(\lambda)$, which \ref{asmp: cluster_separation} forbids). Then, upper bounding the integral using Lemma \ref{lem: expected_density_cut} gives the final result.
\end{proof}

\begin{lemma}
	\label{lem: expected_density_volume}
	Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound},
	\begin{equation*}
	p_V \geq \lambda_{\sigma}^2 \nu_d r^d \nu(\Csig)
	\end{equation*}
\end{lemma}
\begin{proof}
	The proof will proceed similarly to Lemma \ref{lem: expected_density_cut}. We begin by writing $\vol_{n,r}$ as the sum of indicator functions,
	\begin{equation}
	\label{eqn: volume_expansion}
	\vol_{n,r} = \sum_{i = 1}^{n} \sum_{j = 1}^{n} \1(x_i \in \Csig) \1(x_j \in B(x_i, r))
	\end{equation}
	and by linearity of expectation we obtain
	\begin{equation*}
	p_V = \frac{\mu_V}{{n \choose 2}} = 2 \cdot \Pbb(x_i \in \Csig, x_j \in B(x_i,r)).
	\end{equation*}
	Writing this with respect to the density function $f$, we have
	\begin{align*}
	p_V & = 2 \int_{\Csig} f(x) \left\{ \int_{B(x,r)} f(x') dx' \right\} dx \\
	& \geq 2 \int_{\Cset_{\sigma - r}} f(x) \left\{ \int_{B(x,r)} f(x') dx' \right\} dx \\
	& \overset{(i)}{\geq} 2 \lambda_{\sigma}^2 \nu_d r^d \int_{\Cset_{\sigma - r}} f(x) dx
	\end{align*}
	where $(i)$ follows from the fact that $B(x,r) \subset \Csig$ for all $x \in C_{\sigma - r}$, along with the lower bound in Assumption \ref{asmp: bounded_density}. The claim then follows from Lemma \ref{lem: interior_of_expansion_sets}.
\end{proof}

We now convert from bounds on $p_K$ and $p_V$ to probabilistic bounds on $\cut_{n,r}$ and $\vol_{n,r}$ in Lemmas \ref{lem: prob_bound_cut} and \ref{lem: prob_bound_vol}. The key ingredient will be Lemma \ref{lem: bounded_difference}, Hoeffding's inequality for U-statistics; the proofs for both are nearly identical and we give only a proof for Lemma \ref{lem: prob_bound_cut}.

\begin{lemma}
	\label{lem: prob_bound_cut}
	The following statement holds for any $\delta \in (0,1]$: Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound}, 
	\begin{equation}
	\label{eqn: numerator_additive_bound}
	\frac{\cut_{n,r}}{{n \choose 2}} \leq p_K + \sqrt{\frac{\log(1/\delta)}{n}}
	\end{equation}
	with probability at least $1 - \delta$. 
\end{lemma}

\begin{lemma}
	\label{lem: prob_bound_vol}
	The following statement holds for any $\delta \in (0,1]$: Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound}, 
	\begin{equation}
	\label{eqn: denominator_additive_bound}
	\frac{\vol_{n,r}}{{n \choose 2}} \geq p_V - \sqrt{\frac{\log(1/\delta)}{n}}
	\end{equation}
	with probability at least $1 - \delta$. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem: prob_bound_cut}.]
	From \eqref{eqn: density_cut_expansion}, we see that $\cut_{n,r}$, properly scaled, can be expressed as an order-$2$ $U$-statistic,
	\begin{equation*}
	\frac{\cut_{n,r}}{{n \choose 2}} = \frac{1}{{n \choose 2}} \sum_{1 \leq i < j \leq n} \phi_K(x_i, x_j)
	\end{equation*}
	where 
	\begin{equation*}
	\phi_K(x_i,x_j) = \1(x_i \in \Asigr) \1(x_j \in B(x_i,r) \cap \Asig) + \1(x_j \in \Asigr) \1(x_i \in B(x_j,r) \cap \Asig).
	\end{equation*}
	
	From Lemma \ref{lem: bounded_difference} we therefore have
	\begin{equation*}
	\frac{\cut_{n,r}}{{n \choose 2}} \leq p_k + \sqrt{\frac{\log(1/\delta)}{n}}
	\end{equation*}
	with probability at least $1 - \delta$. 
\end{proof}

\subsection{Proof of Theorem \ref{thm: conductance_upper_bound}}
The proof of Theorem \ref{thm: conductance_upper_bound} is more or less given by Lemmas \ref{lem: expected_density_cut}, \ref{lem: expected_density_volume}, \ref{lem: prob_bound_cut}, and \ref{lem: prob_bound_vol}. All that remains is some algebra, which we take care of below.

Fix $\delta \in (0,1]$ and let $\delta' = \delta/2$. Noting that $\Phi_{n,r}(\Csig[\Xbf]) = \frac{\cut_{n,r}}{\vol_{n,r}}$, some trivial algebra gives us the expression
\begin{equation}
\label{eqn: conductance_representation_1}
\Phi_{n,r}(\Csig[\Xbf]) = \frac{p_K + \left(\frac{\cut_{n,r}}{{n \choose 2}} - p_K\right)}{p_V + \left(\frac{\vol_{n,r}}{{n \choose 2}} - p_V\right)}
\end{equation}
We assume (\ref{eqn: numerator_additive_bound}) and (\ref{eqn: denominator_additive_bound}) hold with respect to $\delta'$, keeping in mind that this will happen with probability at least $1 - \delta$. Along with (\ref{eqn: conductance_representation_1}) this means
\begin{equation*}
\Phi_{n,r}(\Csig[\mathbf{X}]) \leq \frac{p_K + \Err_n}{p_V - \Err_n}
\end{equation*}
for $\Err_n = \sqrt{\frac{\log(1/\delta')}{n}}$.
Now, some straightforward algebraic manipulations yield
\begin{align*}
\frac{p_K + \Err_n}{p_V - \Err_n} & = \frac{p_K}{p_V} + \left(\frac{p_K}{p_V - \Err_n} - \frac{p_K}{p_V}\right) + \frac{\Err_n}{p_V - \Err_n} \\
& = \frac{p_k}{p_V} + \frac{\Err_n}{p_V - \Err_n}\left(\frac{p_K}{p_V} + 1\right) \\
& \leq \frac{p_K}{p_V} + 2 \frac{\Err_n}{p_V - \Err_n}.
\end{align*}
By Lemmas \ref{lem: expected_density_cut} and Lemma \ref{lem: expected_density_volume}, we have
\begin{equation*}
\frac{p_K}{p_V} \leq \frac{4rd}{\sigma} \frac{\lambda}{\lambda_{\sigma}} \frac{\left(\lambda_{\sigma} - \frac{r^{\gamma}}{\gamma + 1}\right)}{\lambda_{\sigma}}
\end{equation*}
Then, the choice of 
\begin{equation*}
n \geq \frac{9\log(2/\delta)}{\epsilon^2}\left(\frac{1}{ \lambda_{\sigma}^2 \nu(\Csig) \nu_d r^d}\right)^2 
\end{equation*}

implies $2 \frac{\Err_n}{p_V - \Err_n} \leq \epsilon$.


\subsection{Concentration inequalities}

Given a symmetric kernel function $k: \mathcal{X}^m \to \Reals$, and data $\set{x_1, \ldots, x_n}$, we define the \textit{order-$m$ $U$ statistic} to be 
\begin{equation*}
U := \frac{1}{ {n \choose m} } \sum_{1 \leq i_1 < \ldots < i_m \leq n} k(x_{i_1},\ldots,x_{i_m})
\end{equation*}

For both Lemmas \ref{lem: bounded_difference} and \ref{lem: bernstein}, let $X_1, \ldots, X_n \in \mathcal{X}$ be independent and identically distributed. We will additionally assume the order-$m$ kernel function $k$ satisfies the boundedness property $\sup_{x_1, \ldots, x_m} \abs{k(x_1, \ldots, x_m)} \leq 1$. 

\begin{lemma}[Hoeffding's inequality for $U$-statistics.]
	\label{lem: bounded_difference}
	For any $t > 0$,
	\begin{equation*}
	\mathbb{P}(\abs{U - \mathbb{E}U} \geq t) \leq 2 \exp\left\{- \frac{2nt^2}{m}\right\}
	\end{equation*}
	Further, for any $\delta > 0$, we have
	\begin{align*}
	U & \leq \mathbb{E}U + \sqrt{\frac{m \log(1 / \delta)}{2n} }, \\
	U & \geq \mathbb{E}U - \sqrt{\frac{m \log(1 / \delta)}{2n} }
	\end{align*}
	each with probability at least $1 - \delta$. 
\end{lemma}

\subsection{Proof of Theorem \ref{thm: inverse_mixing_time_lower_bound}}

\textcolor{red}{Give proof structure.}

We begin by introducing some more notation. Take $G = (V,E)$ to be an undirected and unweighted graph, with associated adjacency matrix $\Abf$, random walk matrix $\Wbf$, and stationary distribution $\pibf$. We slightly overload notation and let the \emph{conductance function} be given by
\begin{equation*}
\Phi(t; G) := \min_{\substack{S \subseteq V \\ \pi(S) \leq t} } \Phi(S; G)
\end{equation*}

We will pay special attention to the conductance function evaluated over the subgraph $G_{n,r}[\Csig[\Xbf]]$. For ease of notation, we therefore introduce $\widetilde{\Phi}_{n,r}(t) = \Phi(t; G_{n,r}[\Csig[\Xbf]])$. 

Lemma \ref{lem: montenegro} is from the \textcolor{red}{PhD thesis of Montenegro.} It leverages the conductance function to produce an upper bound on the \textcolor{red}{total variation distance} between the random walk 
\begin{equation*}
\rhobf^t = \frac{1}{t} \sum_{s = 1}^{t} e_v \Wbf^s
\end{equation*}
and the stationary distribution $\pibf$. 

\begin{lemma}
	\label{lem: montenegro}
	Let $\Wbf$ be the random walk matrix over a graph $G = (V,E)$, with stationary distribution $\pibf$. Then, for any $v \in V$:
	\begin{equation*}
	\norm{\rho^t - \pibf}_{TV} \leq \max\left\{ \frac{1}{4}, \frac{1}{10} +  \frac{70}{t}\left(\frac{4}{\Phi^2(\pi_1(G); G)} + \int_{x = \pi_1(G)}^{1/2} \frac{4}{x \Phi^2(x; G)} dx\right) \right\}
	\end{equation*}
	where $\pi_1(G)$ is defined as in \eqref{eqn: local_spread}.
\end{lemma}

\begin{proof}
	In \textcolor{red}{the PhD thesis of Montenegro}, letting
	\begin{equation*}
	h^t(x_0) = \sup_{S: \pi(S) < x_0}  \bigl(\rhobf^t(S) - \pibf(S) \bigr)
	\end{equation*}
	the following statement is shown: for $\pi_{\min} < x_0 < 1/2$, 
	\begin{equation*}
	\norm{\rho^t - \pibf}_{TV} \leq \max\left\{ \frac{1}{4}, h^t(x_0) +  \frac{70}{t}\left(\frac{4}{\Phi^2(x_0); G)} + \int_{x = x_0}^{1/2} \frac{4}{x \Phi^2(x; G)}\right) \right\}
	\end{equation*}
	It is also observed therein that the function $h^t(x)$ is decreasing for any fixed $x$, as $t$ increases. All that remains for us to show is that $h^s(\pi_1(G)) \leq 1/10$ for some $s \leq t$. We choose $s = 1$, and note that for any set $S \subset V$ with $\pibf(S) \leq \pi_1(G)$,
	\begin{equation*}
	\rhobf^1(S) \leq \frac{\abs{S}}{d_{\min}(G)} \leq \frac{\pibf(S) \vol(V; G)}{d_{\min}(G)^2} \leq \frac{1}{10},
	\end{equation*}
	where we use the fact that $\pibf(S) \geq \frac{\abs{S} d_{\min}(G)}{\vol(V; G)}$.
\end{proof}

We turn to lower bounding $\widetilde{\Phi}_{n,r}(t)$. First, we exhibit a lower bound on a continuous space analogue, over the set $\Csig$. Let $\nu_{\Pbb}(\cdot)$ denote the weighted volume; formally, for $\Sset \subset \Rd$
\begin{equation*}
\nu_{\Pbb}(\Sset) := \int_{\Sset} f(x) dx
\end{equation*} 
 
The $r$-ball walk over $\Csig$ is a Markov chain with transition probability for $x \in \Csig, \Sset, \Sset' \subset \Csig$ given by,
\begin{equation*}
P_{\Pbb, r}(x;\Sset) := \frac{\nu_\Pbb(\Sset \cap B(x,r))}{\nu_\Pbb(\Csig \cap B(x,r))}, ~~~ Q_{\Pbb, r}(\Sset, \Sset') := \int_{x \in \Sset} f(x) P_{\Pbb, r}(x;\Sset') dx. 
\end{equation*}
stationary distribution defined by
\begin{equation*}
\ell_{\Pbb,r}(x) := \frac{\nu_\Pbb(\Csig \cap B(x,r))}{\nu_{\Pbb}(B(x,r))}, ~~~ \pi_{\Pbb,r}(\Sset) := \frac{1}{\int_{\Csig} f(x) \ell_{\Pbb,r}(x) dx} \int_{\Sset} f(x) \ell_{\Pbb,r}(x) dx
\end{equation*}
and corresponding conductance function
\begin{equation*}
\widetilde{\Phi}_{\Pbb,r}(t) := \min_{\substack{\Sset \subset \Csig, \\ \pi_{\Pbb,r}(\Sset) \leq t} } \frac{Q_{\Pbb,r}(\Sset, \Csig \setminus \Sset)}{\pi_{\Pbb,r}(\Sset)}
\end{equation*}
Lemma \ref{lem: continuous_conductance_lb} (\textcolor{red}{a weighted analogue of Theorem 4.6 of Kannan}) provides a lower bound on $\widetilde{\Phi}_{\Pbb, r}(t)$, as well as a stepwise approximation to $\widetilde{\Phi}_{\Pbb, r}$.
\begin{lemma}
	\label{lem: continuous_conductance_lb}
	Under the conditions on $\Csig$ given by Theorem \ref{thm: inverse_mixing_time_lower_bound}, the following bounds hold: 
	\begin{itemize}
		\item 
		for $0 < t < 1/2$,
		\begin{equation*}
		\widetilde{\Phi}_{\Pbb,r}(t) > \min\left\{\frac{1}{288\sqrt{d}},\frac{r}{81 \sqrt{d}D}\log\left(\frac{\lambda_{\sigma}^2}{\Lambda_{\sigma}^2 t}\right)\right\} \cdot \frac{\lambda_{\sigma}^4}{\Lambda_{\sigma}^4}
		\end{equation*}
		\item
		Let
		\begin{equation*}
		m = \frac{2^{d+1}D^d \Lambda_{\sigma}^2}{r^d \lambda_{\sigma}^2}
		\end{equation*}
		 and $t_i = (i + 1)/m$ for $i = 0, \ldots, m - 1$. Then, for $1/m < t < 1/2$
		\begin{equation*}
		\overline{\Phi}_{\Pbb,r}(t) > \min\left\{\frac{1}{288\sqrt{d}},\frac{r}{162 \sqrt{d}D}\log\left(\frac{\Lambda_{\sigma}^2}{\lambda_{\sigma}^2 t}\right)\right\} \cdot \frac{\lambda_{\sigma}^4}{\Lambda_{\sigma}^4}
		\end{equation*}
		where $\overline{\Phi}_{\Pbb,r}(t)$ is defined as in \eqref{eqn: stepwise_approximation} with respect to $t_0, \ldots t_{m - 1}$. 
	\end{itemize}
	
\end{lemma}
\begin{proof}
	We begin by stating directly the result of \textcolor{red}{Kannan et al.}: for $0 < t < 1/2$, and any $S$ such that $\pi_{\nu,r}(S) = t$,
	\begin{equation}
	\label{eqn: kannan_1}
	\frac{Q_{\nu,r}(\Sset, \Csig \setminus \Sset)}{t} > \min\left\{\frac{1}{288\sqrt{d}},\frac{r}{81 \sqrt{d}D}\log\left(\frac{1}{t}\right)\right\}.
	\end{equation}
	Now, we note that
	\begin{equation*}
	\pi_{\Pbb,r}(S) \leq \pi_{\nu,r}(S) \cdot \frac{\Lambda_{\sigma}^2}{\lambda_{\sigma}^2}, ~~~ Q_{\Pbb,r}(\Sset, \Csig \setminus \Sset) \geq Q_{\nu,r}(\Sset, \Csig \setminus \Sset) \cdot \frac{\lambda_{\sigma}^2}{\Lambda_{\sigma}^2}
	\end{equation*}
	and plugging these estimates in to \eqref{eqn: kannan_1} gives the final bound on $\widetilde{\Phi}_{\Pbb,r}(t)$.  The bound on $\overline{\Phi}_{\Pbb,r}(t)$ then follows immediately from application of Lemma \ref{lem: stepwise_approximation}.
\end{proof}

The introduction of the stepwise approximation allows us to make use of Lemma \ref{lem: consistency_of_conductance_function}, which gives us (pointwise) consistency of the discrete graph functionals $\widetilde{\Phi}_{n,r}(t)$ to the continuous functionals $\widetilde{\Phi}_{\Pbb,r}(t)$. 

\begin{lemma}
	\label{lem: consistency_of_conductance_function}
	Under the conditions on $\Csig$ given by Theorem \ref{thm: inverse_mixing_time_lower_bound}, fix any $0 < t < 1/2$. Then the following statement holds: with probability one, as $n \to \infty$,
	\begin{equation*}
	\liminf_{n \to \infty} \widetilde{\Phi}_{n,r}(t) \geq \widetilde{\Phi}_{\Pbb,r}(t)
	\end{equation*}
	As a consequence, $m$ and $(t_i)_{i=1}^{m}$ defined as in Lemma \ref{lem: continuous_conductance_lb}, we have that
	\begin{equation}
	\label{eqn: consistency_of_stepapprox_conductance_function}
	\liminf_{n \to \infty} \widetilde{\Phi}_{n,r} \geq \overline{\Phi}_{\Pbb,r}
	\end{equation}
\end{lemma}
Lemma \ref{lem: consistency_of_conductance_function} stems from \textcolor{red}{Garcia Trillos 16}, with a pair of notable distinctions: here, we do not allow the radius $r$ to go to zero, but rather set it to be constant, and in the minimization we have an additional constraint on the measure $\pi(\cdot)$, in both the discrete and continuous functionals. As such, we will need to re-prove a number of the statements of \textcolor{red}{Garcia Trillos 16} to work in this context. We defer this work to Section \ref{subsection: proof_of_pointwise consitency_of_conductance_function}, and here will only show that  \eqref{eqn: consistency_of_stepapprox_conductance_function} is implied immediately by the pointwise result. 

\begin{proof}[Proof of \eqref{eqn: consistency_of_stepapprox_conductance_function}]
We take as given that for any $0 < t < 1/2$,
\begin{equation*}
\lim_{n \to \infty} \widetilde{\Phi}_{n,r}(t) \geq \widetilde{\Phi}_{\Pbb,r}(t).
\end{equation*}
In particular, this will occur for $t_0, t_1, \ldots, t_m$ and therefore
\begin{equation*}
\lim_{n \to \infty} \overline{\Phi}_{n,r} \geq \overline{\Phi}_{\Pbb,r}
\end{equation*}
uniformly over $[1/m,1/2]$. But by Lemma \ref{lem: stepwise_approximation}, $\widetilde{\Phi}_{n,r} \geq \overline{\Phi}_{n,r}$, so we have shown \eqref{eqn: consistency_of_stepapprox_conductance_function}.
\end{proof}

Lemma \ref{lem: consistency_of_conductance_function} coupled with Lemma \ref{lem: montenegro} yields a bound on the total variation mixing time.
\begin{lemma}
	
\end{lemma}

\subsection{Proof of pointwise consistency of conductance function.}
\label{subsection: proof_of_pointwise consitency_of_conductance_function}









\section{OTHER STUFF}

	\begin{lemma}[Bernstein's inequality for $U$-statistics]
		\label{lem: bernstein}
		Additionally, assume $\sigma^2 = \var\left(k(X_1, \ldots, X_m) \right) < \infty$. Then for any $\delta > 0$, 
		\begin{align*}
		\mathbb{P}(U - \mathbb{E}U \geq t) \leq \exp\left\{-\frac{n}{2m}\frac{t^2}{\sigma^2 + t/3}\right\},
		\end{align*}
		
		Moreover if $\sigma^2 \leq \mu/n$, 
		\begin{align*}
		U & \leq \mathbb{E}U \cdot \left(1 + \max\left\{ \sqrt{\frac{2m\log(1/\Delta)}{\mu}}, \frac{2m \log(1/\Delta)}{3\mu} \right\}\right), \\
		U & \geq \mathbb{E}U \cdot \left(1 - \max\left\{ \sqrt{\frac{2m\log(1/\Delta)}{\mu}}, \frac{2m \log(1/\Delta)}{3\mu} \right\}\right)
		\end{align*}
		each with probability at least $1 - \Delta$.
	\end{lemma}

\textcolor{red}{\textbf{Multiplicative bound}: As $\tilde{k}(x_1,x_2)$ is the sum of two Bernoulli random variables with negative covariance (since $\1(x_i \in \Asigr) \1(x_j \in B(x_i,r) \cap \Asig) = 1$ implies $\1(x_j \in \Asigr) \1(x_i \in B(x_j,r) \cap \Asig) = 0$ and vice versa), we can upper bound $\var\left(\tilde{k}(x_1, x_2)\right) \leq  \widetilde{p}$, where we recall 
	\begin{equation*}
	\widetilde{p} = 2\cdot \mathbb{P}\left(\1(x_1 \in \Asigr) \1(x_2 \in B(x_1,r) \cap \Asig)\right)
	\end{equation*}
	From Lemma \ref{lem: bernstein}, we therefore have
	\begin{equation*}
	\frac{\widetilde{\mathcal{E}}}{{n \choose 2}} \leq \widetilde{p} + \max\left\{ \sqrt{\frac{4\log(1/\Delta)\widetilde{p}}{n}}, \frac{4 \log(1/\Delta)}{3n} \right\}
	\end{equation*}
	with probability at least $1 - \Delta$.}

\textcolor{red}{Multiplicative bound: The two terms on the right hand side are both distributed $\mathrm{Bernoulli}(p/2)$. Moreover, since $\1(x_i \in A_\sigma) = 1$ implies $\1(x_j \in A_{\sigma}) = 0$, they have negative covariance. We can therefore upper bound $\var(k'(x_i,x_j)) \leq p$, and so from Lemma \ref{lem: bernstein}, we have
	\begin{equation*}
	\frac{\mathcal{V}}{{n \choose 2}} \geq p - \max\left\{ \sqrt{\frac{4\log(1/\Delta)p}{n}}, \frac{4 \log(1/\Delta)}{3n} \right\}
	\end{equation*}
	with probability at least $1 - \Delta$. }
\end{document}