%%%%%%%% ICML 2019 submission %%%%%%%%%%%%%%%%%

\documentclass{article}

% \usepackage{icml2019}
% \usepackage[accepted]{icml2019}

\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\theequation}{A.\arabic{equation}}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
%\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[parfill]{parskip}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{xr-hyper}
\usepackage{bm}
\usepackage[colorlinks=true,citecolor=blue,urlcolor=blue,linkcolor=blue]{hyperref}

\externaldocument{icml_local_density_clustering}

\newcommand{\diam}{\mathrm{diam}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\defeq}{\overset{\mathrm{def}}{=}}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\1}{\mathbf{1}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\Err}{\mathrm{Err}}

%%% Graph terms
\newcommand{\cut}{\mathrm{cut}}

%%% Vectors
\newcommand{\pbf}{\mathbf{p}}
\newcommand{\ebf}[1]{\mathbf{e}_{#1}}
\newcommand{\pibf}{\bm{\pi}}

%%% Matrices
\newcommand{\Abf}{\mathbf{A}}
\newcommand{\Xbf}{\mathbf{X}}
\newcommand{\Wbf}{\mathbf{W}}
\newcommand{\Lbf}{\mathbf{L}}
\newcommand{\Dbf}{\mathbf{D}}
\newcommand{\Ibf}[1]{\mathbf{I}_{#1}}

%%% Probability distributions (and related items)
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Cbb}{\mathbb{C}}
\newcommand{\Ebb}{\mathbb{E}}

%%% Sets
\newcommand{\Cset}{\mathcal{C}}
\newcommand{\Aset}{\mathcal{A}}
\newcommand{\Asig}{\Aset_{\sigma}}
\newcommand{\Csig}{\Cset_{\sigma}}
\newcommand{\Asigr}{\Aset_{\sigma,\sigma + r}}
\newcommand{\Csigr}{\Cset_{\sigma,\sigma + r}}

%%% Operators
\DeclareMathOperator*{\argmin}{arg\,min}


%%% Algorithm notation
\newcommand{\ppr}{{\sc PPR}}
\newcommand{\pprspace}{{\sc PPR~}}


\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}


%\newcommand{\theHalgorithm}{\arabic{algorithm}}


%\icmltitlerunning{Local clustering of density upper level sets}

\begin{document}

%\twocolumn[
%\icmltitle{Supplement to ``Local clustering of density upper level sets''}

%\icmlsetsymbol{equal}{*}

%\begin{icmlauthorlist}
%\icmlauthor{Alden Green}{cmu}
%\icmlauthor{Sivaraman Balakrishnan}{cmu}
%\icmlauthor{Ryan Tibshirani}{cmu}
%\end{icmlauthorlist}

%\icmlaffiliation{cmu}{Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh PA, USA}

%\icmlcorrespondingauthor{Alden Green}{ajgreen@andrew.cmu.edu}

%\icmlkeywords{local clustering}

%\vskip 0.3in
%]

%\printAffiliationsAndNotice{} % otherwise use the standard text.

\section{Proofs}

In this supplement, we present proofs for ``Local Clustering of Density Upper Level Sets''. We begin by providing technical lemmas, before moving on to proving the main results of the paper. 

Throughout, we will fix $\Aset \subset \Rd$ to be an arbitrary set. To simplify expressions, for the $\sigma$-expansion $\Asig$, we will write the set difference between $\Asig$ and the $(\sigma + r)$-expansion $\Aset_{\sigma + r}$ as 
\begin{equation*}
\Asigr := \set{x: 0 < \rho(x, \Asig) \leq r},
\end{equation*}
where $\rho(x, \Aset) = \min_{x' \in \Aset} \norm{x - x'}$.
. 

For notational ease, we write
\begin{align*}
\cut_{n,r} = \cut(\Csig[\Xbf]; G_{n,r}), ~ \mu_K = \mathbb{E}(\cut_{n,r}), ~ p_K = \frac{\mu_K}{{n \choose 2}} \\
\vol_{n,r} = \vol(\Csig[\Xbf]; G_{n,r}), ~ \mu_V = \mathbb{E}(\vol_{n,r}), ~ p_V = \frac{\mu_V}{{n \choose 2}}
\end{align*}
for the random variable, mean, and probability of cut size and volume, respectively.

\subsection{Technical Lemmas}

We state Lemma \ref{lem: expansion_sets} without proof, as it is trivial. We formally include it mainly to comment on its (potential) suboptimality; for sets $\Aset$ with diameter much larger than $\sigma$, the volume estimate of Lemma \ref{lem: expansion_sets} will be quite poor. 

\begin{lemma}
	\label{lem: expansion_sets}
	For any $\sigma > 0$ and the $\sigma$-expansion $\Asig = \Aset + \sigma B$, 
	\begin{equation*}
	\sigma B \subset \Asig, ~~\mathrm{and~ }\nu(\Aset + \sigma B) \leq \nu((1 + \sigma)\Aset) = (1 + \sigma)^d \nu(\Aset).
	\end{equation*}
\end{lemma}

We will need to carefully control the volume of the expansion set using the above estimate; Lemma \ref{lem: Taylor_series} serves this purpose.
\begin{lemma}
	\label{lem: Taylor_series}
	For any $0 \leq x \leq 1/2d$,
	\begin{equation*}
	(1 + x)^d \leq 1 + 2dx.
	\end{equation*}
\end{lemma}
The proof of Lemma \ref{lem: Taylor_series} is based on approximation via Taylor series, and we omit it.

We will repeatedly employ Lemma \ref{lem: expansion_sets} and Lemma \ref{lem: Taylor_series} in tandem. As a first example, in Lemma \ref{lem: interior_of_expansion_sets}, we use it to bound the ratio of $\nu(\Asig)$ to $\nu(\Aset_{\sigma - r})$. This will be useful when we bound $\vol(\Csig)$.

\begin{lemma}
	\label{lem: interior_of_expansion_sets}
	For $\sigma$, $\Asig$ as in Lemma \ref{lem: expansion_sets}, let $r > 0$ satisfy $r \leq \sigma/4d$. Then,
	\begin{equation*}
	\frac{\nu(\Asig)}{\nu(\Aset_{\sigma - r})} \leq 2.
	\end{equation*}
\end{lemma}
\begin{proof}
	Fix $q = \sigma - r$. Then,
	\begin{align*}
	\nu(\Asig) & = \nu(\Aset_{q + \sigma - q}) = \nu(\Aset_q + (\sigma - q)B ) \\
	& \leq \nu(\Aset_q + \frac{(\sigma - q)}{q} \Aset_q) = \left(1 + \frac{\sigma - q}{q}\right)^d \nu(\Aset_q)
	\end{align*}
	where the inequality follows from Lemma \ref{lem: expansion_sets}. Of course, $\sigma - q = r$, and $\frac{r}{q} \leq \frac{1}{2d}$ for $r \leq \frac{1}{4d}$. The claim then follows from Lemma \ref{lem: Taylor_series}.
\end{proof}

\subsection{Cut and volume estimates}
\begin{lemma}
	\label{lem: expected_number_boundary_points}
	Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound}, and for any $r < \sigma/2d$,
	\begin{equation*}
	\Pbb(\Csigr) \leq 2 \nu(\Csig) \frac{rd}{\sigma}  \left(\lambda_{\sigma} - \frac{r^{\gamma}}{\gamma + 1}\right)
	\end{equation*}	
\end{lemma}
\begin{proof}
	Recalling that $f$ is the density function for $\Pbb$, we have
	\begin{equation}
	\label{eqn: integral_over_epsilon_neighborhood}
	\Pbb(\Csigr) = \int_{\Csigr} f(x) dx
	\end{equation}
	We partition $\Csigr$ into slices, based on distance from $\Csig$, as follows: for $k \in \N$,
	\begin{equation*}
	\mathcal{T}_{i,k} = \set{x \in \Rd: t_{i,k} < \frac{\rho(x, \Csig)}{r} \leq t_{i+1,k}}, ~~ \Csigr = \bigcup_{i = 0}^{k-1} \mathcal{T}_{i,k}
	\end{equation*}
	where $t_i = i/k$ for $i = 0, \ldots, k - 1$. As a result,
	\begin{equation*}
	\int_{\Csigr} f(x) dx = \sum_{i = 0}^{k-1} \int_{\mathcal{T}_{i,k}} f(x) dx \leq \sum_{i = 0}^{k-1} \nu(\mathcal{T}_{i,k}) \max_{x \in \mathcal{T}_{i,k}} f(x).
	\end{equation*}
	We substitute
	\begin{equation*}
	\nu(\mathcal{T}_{i,k}) = \nu(\Csig + rt_{i+1,k}B) - \nu(\Csig + rt_{i,k}B) := \nu_{i+1,k} - \nu_{i,k}. 
	\end{equation*}
	where for simplicity we've written $\nu_{i,k} = \nu(\Csig + rt_{i+1,k}B)$.
	This, in concert with the upper bound
	\begin{equation*}
	\max_{x \in \mathcal{T}_{i,k}} f(x) \leq \lambda_{\sigma} - (rt_{i,k})^{\gamma},
	\end{equation*}
	which follows from \ref{asmp: bounded_density} and \ref{asmp: low_noise_density}, yields
	\begin{align}
	\label{eqn: telescoping_sum}
	\sum_{i = 0}^{k-1} \nu(\mathcal{T}_{i,k}) \max_{x \in \mathcal{T}_{i,k}} f(x) & \leq \sum_{i = 0}^{k-1} \biggl\{ \nu_{i+1,k} - \nu_{i,k} \biggr\} \biggl( \lambda_{\sigma} - (rt_{i,k})^{\gamma} \biggr) \nonumber \\
	& = \sum_{i = 1}^{k} 
	\underbrace{\nu_{i,k} \biggl( \left[\lambda_{\sigma} - (rt_{i,k})^{\gamma}\right] -  \left[\lambda_{\sigma} - (rt_{i-1,k})^{\gamma}\right]\biggr)}_{:= \Sigma_k} + \underbrace{\biggl(\nu_{k,k}\left[\lambda_{\sigma} - r^{\gamma}\right] - \nu_{1,k}\lambda_{\sigma} \biggr)}_{:= \xi_k}
	\end{align}
	
	We first consider the term $\Sigma_k$. Here we use Lemma \ref{lem: expansion_sets} to upper bound
	\begin{equation*}
	\nu_{i,k} \leq \vol(\Csig)\left(1 + \frac{rt_{i,k}}{\sigma}\right)^d
	\end{equation*}
	and so we can in turn upper bound $\Sigma_k$:
	\begin{equation}
	\label{eqn: Sigmak_riemann_sum}
	\Sigma_k \leq \vol(\Csig) r^\gamma \sum_{i = 1}^{k} \left(1 + \frac{rt_{i,k}}{\sigma}\right)^d \biggl( (t_{i-1,k})^{\gamma} - (t_{i,k})^{\gamma}\biggr).
	\end{equation}
	This, of course, is a Riemann sum, and as the inequality holds for all values of $k$ it holds in the limit as well, which we compute to be
	\begin{align*}
	\lim_{k \to \infty} \sum_{i = 1}^{k} \left(1 + \frac{rt_{i,k}}{\sigma}\right)^d \biggl( (t_{i-1,k})^{\gamma} - (t_{i,k})^{\gamma}\biggr) & = \gamma \int_{0}^{1} \left(1 + \frac{rt}{\sigma}\right)^d t^{\gamma - 1} dt \\
	& \overset{(i)}{\leq} \gamma \int_{0}^{1} \left(1 + \frac{2drt}{\sigma}\right) t^{\gamma - 1} dt = \left(1 + \frac{\gamma 2dr}{\gamma + 1}\right).
	\end{align*}
	where $(i)$ follows from Lemma \ref{lem: Taylor_series}. 
	We plug this estimate in to \eqref{eqn: Sigmak_riemann_sum} and obtain
	\begin{equation*}
	\lim_{k \to \infty} \Sigma_k \leq \vol(\Csig) r^{\gamma} \left(1 + \frac{\gamma 2dr}{\gamma + 1}\right).
	\end{equation*}
	
	We now provide an upper bound on $\xi_k$. It will follow the same basic steps as the bound on $\Sigma_k$, but will not involve integration:
	\begin{align*}
	\xi_k & \overset{(ii)}{\leq} \nu(\Csig) \biggl\{ \left(1 + \frac{r}{\sigma}\right)^d(\lambda - r^{\gamma}) - \lambda \biggr\} \\
	& \overset{(iii)}{\leq} \nu(\Csig) \biggl\{ \left(1 + \frac{2dr}{\sigma}\right)(\lambda - r^{\gamma}) - \lambda \biggr\} = \nu(\Csig) \biggl\{ \frac{2dr}{\sigma}(\lambda - r^{\gamma}) - r^{\gamma} \biggr\}.
	\end{align*}
	where $(ii)$ follows from Lemma \ref{lem: expansion_sets} and $(iii)$ from Lemma \ref{lem: Taylor_series}. The final result comes from adding together the upper bounds on $\Sigma_k$ and $\xi_k$ and taking the limit as $k \to \infty$.
\end{proof}

\begin{lemma}
	\label{lem: expected_density_cut}
	Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound}, and for any $r < \sigma/2d$,
	\begin{equation*}
	p_K \leq \frac{4 \lambda \nu_d r^{d+1} \nu(\Csig)d}{\sigma}  \left(\lambda_{\sigma} - \frac{r^{\gamma}}{\gamma + 1}\right)
	\end{equation*}
\end{lemma}
\begin{proof}
	We can write $\cut_{n,r}$ as the sum of indicator functions,
	\begin{equation}
	\label{eqn: density_cut_expansion}
	\cut_{n,r} = \sum_{i = 1}^{n} \sum_{j = 1}^{n} \1(x_i \in \Csigr) \1(x_j \in B(x_i,r) \cap \Csig)
	\end{equation}
	and by linearity of expectation, we can obtain
	\begin{equation*}
	p_K = \frac{\mu_K}{{n \choose 2}} = 2 \cdot \Pbb(x_i \in \Csigr, x_j \in B(x_i,r) \cap \Csig)
	\end{equation*}
	Writing this with respect to the density function $f$, we have
	\begin{align*}
	p_K & = 2 \int_{\Csigr} f(x) \left\{ \int_{B(x,r) \cap \Csig} f(x') dx' \right\} dx \\
	& \leq 2 \nu_d r^d \lambda  \int_{\Csigr} f(x) dx
	\end{align*}
	where the inequality follows from Assumption \ref{asmp: cluster_separation}, which implies that the density function $f(x') \leq \lambda$ for all $x' \in \Csig \setminus \Cset$ (otherwise, $x'$ would be in some $\Cset' \in \Cbb_f(\lambda)$, which \ref{asmp: cluster_separation} forbids). Then, upper bounding the integral using Lemma \ref{lem: expected_density_cut} gives the final result.
\end{proof}

\begin{lemma}
	\label{lem: expected_density_volume}
	Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound},
	\begin{equation*}
	p_V \geq \lambda_{\sigma}^2 \nu_d r^d \nu(\Csig)
	\end{equation*}
\end{lemma}
\begin{proof}
	The proof will proceed similarly to Lemma \ref{lem: expected_density_cut}. We begin by writing $\vol_{n,r}$ as the sum of indicator functions,
	\begin{equation}
	\label{eqn: volume_expansion}
	\vol_{n,r} = \sum_{i = 1}^{n} \sum_{j = 1}^{n} \1(x_i \in \Csig) \1(x_j \in B(x_i, r))
	\end{equation}
	and by linearity of expectation we obtain
	\begin{equation*}
	p_V = \frac{\mu_V}{{n \choose 2}} = 2 \cdot \Pbb(x_i \in \Csig, x_j \in B(x_i,r)).
	\end{equation*}
	Writing this with respect to the density function $f$, we have
	\begin{align*}
	p_V & = 2 \int_{\Csig} f(x) \left\{ \int_{B(x,r)} f(x') dx' \right\} dx \\
	& \geq 2 \int_{\Cset_{\sigma - r}} f(x) \left\{ \int_{B(x,r)} f(x') dx' \right\} dx \\
	& \overset{(i)}{\geq} 2 \lambda_{\sigma}^2 \nu_d r^d \int_{\Cset_{\sigma - r}} f(x) dx
	\end{align*}
	where $(i)$ follows from the fact that $B(x,r) \subset \Csig$ for all $x \in C_{\sigma - r}$, along with the lower bound in Assumption \ref{asmp: bounded_density}. The claim then follows from Lemma \ref{lem: interior_of_expansion_sets}.
\end{proof}

We now convert from bounds on $p_K$ and $p_V$ to probabilistic bounds on $\cut_{n,r}$ and $\vol_{n,r}$ in Lemmas \ref{lem: prob_bound_cut} and \ref{lem: prob_bound_vol}. The key ingredient will be Lemma \ref{lem: bounded_difference}, Hoeffding's inequality for U-statistics; the proofs for both are nearly identical and we give only a proof for Lemma \ref{lem: prob_bound_cut}.

\begin{lemma}
	\label{lem: prob_bound_cut}
	The following statement holds for any $\delta \in (0,1]$: Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound}, 
	\begin{equation}
	\label{eqn: numerator_additive_bound}
	\frac{\cut_{n,r}}{{n \choose 2}} \leq p_K + \sqrt{\frac{\log(1/\delta)}{n}}
	\end{equation}
	with probability at least $1 - \delta$. 
\end{lemma}

\begin{lemma}
	\label{lem: prob_bound_vol}
	The following statement holds for any $\delta \in (0,1]$: Under the setup and conditions of Theorem \ref{thm: conductance_upper_bound}, 
	\begin{equation}
	\label{eqn: denominator_additive_bound}
	\frac{\vol_{n,r}}{{n \choose 2}} \geq p_V - \sqrt{\frac{\log(1/\delta)}{n}}
	\end{equation}
	with probability at least $1 - \delta$. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem: prob_bound_cut}.]
	From \eqref{eqn: density_cut_expansion}, we see that $\cut_{n,r}$, properly scaled, can be expressed as an order-$2$ $U$-statistic,
	\begin{equation*}
	\frac{\cut_{n,r}}{{n \choose 2}} = \frac{1}{{n \choose 2}} \sum_{1 \leq i < j \leq n} \phi_K(x_i, x_j)
	\end{equation*}
	where 
	\begin{equation*}
	\phi_K(x_i,x_j) = \1(x_i \in \Asigr) \1(x_j \in B(x_i,r) \cap \Asig) + \1(x_j \in \Asigr) \1(x_i \in B(x_j,r) \cap \Asig).
	\end{equation*}
	
	From Lemma \ref{lem: bounded_difference} we therefore have
	\begin{equation*}
	\frac{\cut_{n,r}}{{n \choose 2}} \leq p_k + \sqrt{\frac{\log(1/\delta)}{n}}
	\end{equation*}
	with probability at least $1 - \delta$. 
\end{proof}

\subsection{Proof of Theorem \ref{thm: conductance_upper_bound}}
The proof of Theorem \ref{thm: conductance_upper_bound} is more or less given by Lemmas \ref{lem: expected_density_cut}, \ref{lem: expected_density_volume}, \ref{lem: prob_bound_cut}, and \ref{lem: prob_bound_vol}. All that remains is some algebra, which we take care of below.

Fix $\delta \in (0,1]$ and let $\delta' = \delta/2$. Noting that $\Phi_{n,r}(\Csig[\Xbf]) = \frac{\cut_{n,r}}{\vol_{n,r}}$, some trivial algebra gives us the expression
\begin{equation}
\label{eqn: conductance_representation_1}
\Phi_{n,r}(\Csig[\Xbf]) = \frac{p_K + \left(\frac{\cut_{n,r}}{{n \choose 2}} - p_K\right)}{p_V + \left(\frac{\vol_{n,r}}{{n \choose 2}} - p_V\right)}
\end{equation}
We assume (\ref{eqn: numerator_additive_bound}) and (\ref{eqn: denominator_additive_bound}) hold with respect to $\delta'$, keeping in mind that this will happen with probability at least $1 - \delta$. Along with (\ref{eqn: conductance_representation_1}) this means
\begin{equation*}
\Phi_{n,r}(\Csig[\mathbf{X}]) \leq \frac{p_K + \Err_n}{p_V - \Err_n}
\end{equation*}
for $\Err_n = \sqrt{\frac{\log(1/\delta')}{n}}$.
Now, some straightforward algebraic manipulations yield
\begin{align*}
\frac{p_K + \Err_n}{p_V - \Err_n} & = \frac{p_K}{p_V} + \left(\frac{p_K}{p_V - \Err_n} - \frac{p_K}{p_V}\right) + \frac{\Err_n}{p_V - \Err_n} \\
& = \frac{p_k}{p_V} + \frac{\Err_n}{p_V - \Err_n}\left(\frac{p_K}{p_V} + 1\right) \\
& \leq \frac{p_K}{p_V} + 2 \frac{\Err_n}{p_V - \Err_n}.
\end{align*}
By Lemmas \ref{lem: expected_density_cut} and Lemma \ref{lem: expected_density_volume}, we have
\begin{equation*}
\frac{p_K}{p_V} \leq \frac{4rd}{\sigma} \frac{\lambda}{\lambda_{\sigma}} \frac{\left(\lambda_{\sigma} - \frac{r^{\gamma}}{\gamma + 1}\right)}{\lambda_{\sigma}}
\end{equation*}
Then, the choice of 
\begin{equation*}
n \geq \frac{9\log(2/\delta)}{\epsilon^2}\left(\frac{1}{ \lambda_{\sigma}^2 \nu(\Csig) \nu_d r^d}\right)^2 
\end{equation*}

implies $2 \frac{\Err_n}{p_V - \Err_n} \leq \epsilon$.


\subsection{Concentration inequalities}

Given a symmetric kernel function $k: \mathcal{X}^m \to \Reals$, and data $\set{x_1, \ldots, x_n}$, we define the \textit{order-$m$ $U$ statistic} to be 
\begin{equation*}
U := \frac{1}{ {n \choose m} } \sum_{1 \leq i_1 < \ldots < i_m \leq n} k(x_{i_1},\ldots,x_{i_m})
\end{equation*}

For both Lemmas \ref{lem: bounded_difference} and \ref{lem: bernstein}, let $X_1, \ldots, X_n \in \mathcal{X}$ be independent and identically distributed. We will additionally assume the order-$m$ kernel function $k$ satisfies the boundedness property $\sup_{x_1, \ldots, x_m} \abs{k(x_1, \ldots, x_m)} \leq 1$. 

\begin{lemma}[Hoeffding's inequality for $U$-statistics.]
	\label{lem: bounded_difference}
	For any $t > 0$,
	\begin{equation*}
	\mathbb{P}(\abs{U - \mathbb{E}U} \geq t) \leq 2 \exp\left\{- \frac{2nt^2}{m}\right\}
	\end{equation*}
	Further, for any $\delta > 0$, we have
	\begin{align*}
	U & \leq \mathbb{E}U + \sqrt{\frac{m \log(1 / \delta)}{2n} }, \\
	U & \geq \mathbb{E}U - \sqrt{\frac{m \log(1 / \delta)}{2n} }
	\end{align*}
	each with probability at least $1 - \delta$. 
\end{lemma}











\section{OTHER STUFF}

	\begin{lemma}[Bernstein's inequality for $U$-statistics]
		\label{lem: bernstein}
		Additionally, assume $\sigma^2 = \var\left(k(X_1, \ldots, X_m) \right) < \infty$. Then for any $\delta > 0$, 
		\begin{align*}
		\mathbb{P}(U - \mathbb{E}U \geq t) \leq \exp\left\{-\frac{n}{2m}\frac{t^2}{\sigma^2 + t/3}\right\},
		\end{align*}
		
		Moreover if $\sigma^2 \leq \mu/n$, 
		\begin{align*}
		U & \leq \mathbb{E}U \cdot \left(1 + \max\left\{ \sqrt{\frac{2m\log(1/\Delta)}{\mu}}, \frac{2m \log(1/\Delta)}{3\mu} \right\}\right), \\
		U & \geq \mathbb{E}U \cdot \left(1 - \max\left\{ \sqrt{\frac{2m\log(1/\Delta)}{\mu}}, \frac{2m \log(1/\Delta)}{3\mu} \right\}\right)
		\end{align*}
		each with probability at least $1 - \Delta$.
	\end{lemma}

\textcolor{red}{\textbf{Multiplicative bound}: As $\tilde{k}(x_1,x_2)$ is the sum of two Bernoulli random variables with negative covariance (since $\1(x_i \in \Asigr) \1(x_j \in B(x_i,r) \cap \Asig) = 1$ implies $\1(x_j \in \Asigr) \1(x_i \in B(x_j,r) \cap \Asig) = 0$ and vice versa), we can upper bound $\var\left(\tilde{k}(x_1, x_2)\right) \leq  \widetilde{p}$, where we recall 
	\begin{equation*}
	\widetilde{p} = 2\cdot \mathbb{P}\left(\1(x_1 \in \Asigr) \1(x_2 \in B(x_1,r) \cap \Asig)\right)
	\end{equation*}
	From Lemma \ref{lem: bernstein}, we therefore have
	\begin{equation*}
	\frac{\widetilde{\mathcal{E}}}{{n \choose 2}} \leq \widetilde{p} + \max\left\{ \sqrt{\frac{4\log(1/\Delta)\widetilde{p}}{n}}, \frac{4 \log(1/\Delta)}{3n} \right\}
	\end{equation*}
	with probability at least $1 - \Delta$.}

\textcolor{red}{Multiplicative bound: The two terms on the right hand side are both distributed $\mathrm{Bernoulli}(p/2)$. Moreover, since $\1(x_i \in A_\sigma) = 1$ implies $\1(x_j \in A_{\sigma}) = 0$, they have negative covariance. We can therefore upper bound $\var(k'(x_i,x_j)) \leq p$, and so from Lemma \ref{lem: bernstein}, we have
	\begin{equation*}
	\frac{\mathcal{V}}{{n \choose 2}} \geq p - \max\left\{ \sqrt{\frac{4\log(1/\Delta)p}{n}}, \frac{4 \log(1/\Delta)}{3n} \right\}
	\end{equation*}
	with probability at least $1 - \Delta$. }
\end{document}