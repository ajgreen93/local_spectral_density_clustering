%%%%%%%% ICML 2019 submission %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{icml2019}
% \usepackage[accepted]{icml2019}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts, amsthm, amssymb}
\usepackage{graphicx}
\usepackage[parfill]{parskip}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}

\newcommand{\diam}{\mathrm{diam}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\defeq}{\overset{\mathrm{def}}{=}}
\newcommand{\vol}{\text{vol}}
\newcommand{\cut}{\mathrm{cut}}
\newcommand{\abs}[1]{\left \lvert #1 \right \rvert}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\Rd}{\Reals^d}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\Asig}{A_{\sigma}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\1}{\mathbf{1}}

\newtheoremstyle{aldenthm}
{6pt} % Space above
{6pt} % Space below
{\itshape} % Body font
{} % Indent amount
{\bfseries} % Theorem head font
{.} % Punctuation after theorem head
{.5em} % Space after theorem head
{} % Theorem head spec (can be left empty, meaning `normal')

\theoremstyle{aldenthm}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\theoremstyle{remark}
\newtheorem{remark}{Remark}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\icmltitlerunning{Local clustering of density upper level sets}

\begin{document}

\twocolumn[
\icmltitle{Local Spectral Clustering of Density Upper Level Sets}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Alden Green}{cmu}
\icmlauthor{Sivaraman Balakrishnan}{cmu}
\icmlauthor{Ryan Tibshirani}{cmu}
\end{icmlauthorlist}

\icmlaffiliation{cmu}{Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh PA, USA}

\icmlcorrespondingauthor{Alden Green}{ajgreen@andrew.cmu.edu}

\icmlkeywords{local clustering}

\vskip 0.3in
]

\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
\end{abstract}

\section{Introduction}
\label{introduction}

Given data $\mathbf{x} := \set{x_1, \ldots, x_n} \subset \Rd$, our statistical learning task is clustering: splitting data into groups which satisfy some notion of within-group similarity and between-group difference. 

In particular, spectral clustering methods are a family of powerful non-parametric clustering algorithms. Given a symmetric matrix $A \in \Reals^{n \times n}$ with $(i,j)$th entry representing the similarity between data points $x_i$ and $x_j$, we form the graph Laplacian matrix \footnote{Often, either of the Laplacian matrices  $L_{sym} := D^{\frac{1}{2}}WD^{-\frac{1}{2}}$ or $L_{unn} := D - A$ are used instead.},
\begin{equation}
\label{eqn: random_walk_laplacian}
W := D^{-1}A; ~~~ L = I - W
\end{equation}
where the degree matrix $D$ is a diagonal matrix with $D_{ii} := \sum_j A_{ij}$.

Roughly speaking, spectral clustering techniques first embed the data $\x$ using the spectrum of the graph Laplacian matrix and subsequently 
use this \emph{spectral embedding} to find a clustering of the data.
When applied to large graphs (or large point clouds) classical global spectral 
methods can be computationally cumbersome and 
can be insensitive to the local geometry of the distribution of the samples
\citep{mahoney2012,leskovec2010}.
This in turn has led to the investigation of local spectral algorithms \citep{spielman2013,anderson2006,leskovec2010}
which leverage locally-biased spectra computed using random walks around 
a user-specified seed node. 

A natural model to consider when analyzing point cloud data is the following:
\begin{equation}
x_i \sim P, \text{   independently, for $i = 1,\ldots,n$},
\end{equation} 
with $f$ the density function of $P$ with respect to the uniform measure over $\Rd$. In this case, we are interested in understanding what the output of a clustering algorithm on this finite sample reveals about the unknown density $f$. For $\lambda > 0$ and $\set{x: f(x) \geq \lambda}$, it is intuitive \cite{hartigan1981,chaudhuri2010} to define clusters to be the connected components $\mathbb{C}_f(\lambda)$ of the upper level set; we call these connected regions of high density \textit{density clusters}, and study the ability of spectral methods to identify such clusters. 

\paragraph{Graph connectivity criteria.}

A somewhat more standard mode of analyzing spectral clustering methods is through approximation to a pair of graph connectivity criteria.

For $A$ as before, let the graph $G  = (V,E)$, with vertices $V = (v_1, \ldots, v_n)$ corresponding to the $n$ rows of $A$, and edges $E = \set{(v_i,v_j): 1 \leq i < j \leq n: (v_i,v_j) = A_{ij}}$ (Since $A$ is symmetric, $G$ is an undirected graph; also, by convention, we preclude self-loops). There are many \cite{yang2015,fortunato2010} graph-theoretic measures which assess the cluster quality of a subset $S \subseteq V$ (or more generally the quality of a partition $S_1 \cup \ldots \cup S_m = V$, for $m \geq 2$.)

Arguably a natural way to assess cluster quality is via a pair of criteria capturing the \textit{external} and \textit{internal connectivity} of $S$, respectively.  As the names suggest, external connectivity should relate to the number of edges between $S$ and $G / S$ (hereafter denoted $S^c$), while internal connectivity in turn measures the number of edges between subsets within $S$. The clustering task then becomes to find a subset $S$ (or, for global algorithms, a partition $S_1 \cup \ldots \cup S_m = V$), which has both small external and large internal connectivity. 

We will assess the external connectivity of a subset $S \subseteq V$ through its normalized cut. The cut of $S$ is
\begin{equation*}
\cut(S; G) := \sum_{u \in S} \sum_{v \in S^c} \1\left((u,v) \in E \right)
\end{equation*}
-- where $S^c = V / S$ is the complement of $S$ in $V$ -- and the volume of $S$ is
\begin{equation*}
\vol(S; G) := \sum_{u \in S} \sum_{v \in V} \1\left((u,v) \in E \right).
\end{equation*}
Then, the \textit{normalized cut} of $S$ is given by
\begin{equation}
\label{eqn: norm_cut}
\Phi(S; G) := \frac{\cut(S; G)}{ \min\left\{\vol(S; G), \vol(S^c; G) \right\} }
\end{equation} 
Intuitively, a set with low \textit{normalized cut} has many more edges which do not cross the cut than edges which do cross the cut. 

Given $S \subseteq V$, the subgraph induced by $S$ is given by $G[S] = (S, E_S)$, where $(u,v) \in E_S$ if both $u$ and $v$ are in $S$ and $(u,v) \in E_S$. \textcolor{red}{Introduce and define the \textbf{inverse mixing time}.} 
\begin{equation}
\label{eqn: inv_mixing_time}
\Psi(S; G) = 
\end{equation}

If $S$ has normalized cut no greater than $\Phi$, and inverse mixing time no less than $\Psi$, we will refer to it as a $(\Phi,\Psi)$-cluster. Both local \cite{zhu2013} and global \cite{kannan04} spectral algorithms have been shown to output clusters (or partitions) which provably satisfy approximations to the optimal $(\Phi, \Psi)$-cluster (or partition), where the optimization is carried out over the graph $G$. \footnote{In the case of \cite{kannan04}, the internal connectivity parameter $\phi$ is actually the conductance, i.e. the minimum normalized cut within the subgraph $G[S]$. See Theorem 3.1 for details; however, note that $\phi^2 / \log(\vol(S)) \leq O(\Psi)$, and so the lower bound on $\phi$ translates to a lower bound on $\Psi$.}

\paragraph{Personalized PageRank.}
As mentioned previously, global algorithms which find spectral cuts may be computationally infeasible for large graphs; in this setting, local algorithms may be preferred or even required. We will restrict our attention in particular to one such popular algorithm: \textit{personalized PageRank} (PPR). The personalized PageRank algorithm was first introduced by \cite{haveliwala2003} and variants of this algorithm have been studied further in several recent works \citep{spielman2011,spielman2014,zhu2013,anderson2006,mahoney2012}. 

The random walk matrix is given by $W$ defined as in (\ref{eqn: random_walk_laplacian}), and vertices $V$ and edges $E$ are as above. The PPR vector is defined with respect to the following inputs: a user-specified seed node $v_i \in V$ , and  $\alpha \in [0,1]$ a teleportation parameter. Letting $v = v_i$ for notational simplicity, and $e_{v}$ be the indicator vector for the $v$th node (meaning $e_v$ has a $1$ in the $i$th location and $0$ everywhere else), the \textit{PPR vector} is given by the recursive formulation
\begin{equation}
\label{eqn: ppr_vector}
p(v,\alpha;G) := \alpha e_v + (1 - \alpha) p(v,\alpha;G) W
\end{equation}
We note in passing that, for $\alpha > 0$ the vector $p(v,\alpha;G)$ can be well-approximated by a simple local computation (of a random walk with restarts at the node $v$.) We also point out that, from a density clustering standpoint, since density clusters are inherently local, using the PPR algorithm eases the analysis, and as we will observe in the sequel our analysis requires fewer global regularity conditions relative to more classical global spectral algorithms. 

To compute a cluster $\widehat{C}_n \subset V$ using the PPR vector, we will take sweep cuts of $p(r, \alpha; G)$. For a vector $p$, $p[j]$ be the $j$th entry of $p$, and $p_{(k)}$ be the $k$th \textit{largest} entry of $p$. Then the sweep cut $S_k$ is
\begin{equation}
\label{eqn: sweep_cuts}
S_k = \set{u_j: p(r, \alpha; G)[j] > p(r, \alpha; G)_{(k)}}
\end{equation}

We delay formal introduction of the local clustering algorithm we analyze until we have given defined a method for forming a graph over the data $\x$.

\paragraph{Large sample behavior.}

Given a kernel function $\mathbf{k}: \Rd \times \Rd \to [0,\infty)$ of the form $\mathbf{k}(x,x') = k(\norm{x - x'}/ r_n)$ with $k$ non-increasing and some $r_n > 0$, and data $\x = \set{x_1, \ldots, x_n}$ sampled from $P$ as before, form the (weighted, complete) similarity graph $G_n = (\x, E_n)$ with $E_n = \set{k(x_i,x_j): 1 \leq i < j \leq n}$. (Here, $\norm{\cdot}$ is used to denote Euclidean norm). 

It is worth pointing out that in this context, some theory has been developed regarding how graph theoretic quantities such as the normalized cut $\Phi$ (and others) relate to properties of the underlying distribution $f$ as well as the kernel function $\mathbf{k}$. Such analyses typically proceed by defining a continuous analogue to the measure of cluster quality under consideration. Then, under appropriate specification of $\mathbf{k}$ and a proper schedule of $\set{r_n}_{n \in N}$, convergence of clusters output by spectral (and other) algorithms to the corresponding minima of these continuous analogues has been shown \cite{vonluxburg2008, garciatrillos18}.

These continuous analogues, and their corresponding minimizers, are not always easily identifiable for the particular density function under consideration. Relating these partitions to the arguably more simply defined high density clusters can be also challenging in general. \textcolor{red}{Intuitively, however, under the right conditions such high-density clusters should have more edges within themselves than to the remainder of the graph.} We formalize this intuition next.

\subsection{Summary of results}

Hereafter, we consider the \textit{uniform kernel function} for a fixed $r > 0$,
\begin{equation}
\mathbf{k}(x,x') = \1(\norm{x - x'} \leq r)
\end{equation}
and the associated \textit{neighborhood graph} 
\begin{equation}
G_{n,r} = (\x, E_{n,r}), \text{  $(x_i,x_j) \in E_{n,r}$ if $\mathbf{k}(x_i,x_j) = 1$}
\end{equation}
For a given high density cluster $C \subseteq \mathbb{C}_f(\lambda)$, we call $C[\x] = C \cap \x$ the \textit{empirical density cluster}. We now introduce a notion of consistency for the task of density cluster estimation:

\begin{definition}[Consistent density cluster estimation]
	For an estimator $\widehat{C}_n$, and any $C, C' \in C_f(\lambda),$ we say $\widehat{C}_n$ is a consistent estimator of $C$ if the following statement holds: as the sample size $n \to \infty$,
	\begin{equation}
	C[\x] \subseteq \widehat{C}_n, ~\mathrm{ and }~ \widehat{C}_n \cap C'[\x] = \emptyset
	\end{equation}
	occurs with probability tending to $1$.
	
\end{definition}


Our results can now be summarized by the following two points:

\begin{enumerate}
	\item 
	Under a natural set of geometric conditions\footnote{We formally introduce the geometric conditions in \ref{sec: measures_of_cluster_quality}. They preclude clusters which are too thin and long, or those for which the gap in density between the high density area and the outside is not sufficiently large}, Theorems \ref{thm: conductance_upper_bound} and \textcolor{red}{Theorem 2} upper and lower bound, respectively, the normalized cut and inverse mixing time of an empirical density cluster $C[\x]$.
	
	\item 
	We show these bounds on the graph connectivity criteria have algorithmic consequences for the PPR algorithm. An immediate consequence of Theorems \ref{thm: conductance_upper_bound} and \textcolor{red}{Theorem 2}, along with the previous work of \cite{zhu2013}, is to yield an upper bound on the normalized cut of the set $\widehat{C}_n$ output by Algorithm \ref{alg: ppr_cluster}, as well as upper bounding the symmetric set difference between $\widehat{C}_n$ and $C[\x]$. Further, in Section 4 we show that a careful analysis of the form typical to local clustering algorithms yields \textcolor{red}{Theorem 4}, which states that Algorithm \ref{alg: ppr_cluster}, properly initialized, performs consistent density cluster estimation.
\end{enumerate}

\paragraph{Organization.}
In Section \ref{section: everything_else}, we provide some example density functions, to clarify the relevance of our results. In \textcolor{red}{Section 5}, we show empirical performance of the PPR algorithm, which demonstrates that violations of the geometric conditions we set out in Section \ref{sec: measures_of_cluster_quality} manifestly impact density cluster recovery (i.e. the conditions are not superfluous), before concluding in \textcolor{red}{Section 6}. First, however, we summarize some related work.
\subsection{Related Work}


\paragraph{Graph notation.}

Let $G = (V,E)$ be an undirected, unweighted graph, with $S,S' \subseteq V$.
The volume of $S$ is given by $\vol(S; G) = \sum_{v \in S} \deg(v; G) $
where $\deg(v; G) = \sum_{u \in V} 1\bigl((u,v) \in E\bigr)$ is the degree of $v$ in $G$. For the random walk over $G$, denote the stationary distribution by $\pi$, where $\pi(S; G) = \frac{\vol(S ; G)}{\vol(V; G)}$. 

\vspace{.05 in}

For the sample $\mathbf{X} := \set{x_1, \ldots, x_n} \subset \Rd$ and a set $A \subset \Rd$, define $A[\mathbf{X}] = A \cap \mathbf{X}$. We denote the uniform measure over $\Rd$ by $\nu(\cdot)$.

\section{Measures of cluster quality}
\label{sec: measures_of_cluster_quality}

Let $P$ be a distribution supported on a compact set $\mathcal{X} \subset \Rd$, with continuous density function $f$ (with respect to the uniform measure). The Euclidean distance is denoted by $\norm{ \cdot }$. 

\paragraph{Density upper-level set.}
For a number $\tau \geq 0$, let $C_f(\tau)$ be the collection of collected components of the density upper-level set $\set{x \in \mathcal{X}: f(x) \geq \tau}$. Define a \textit{$\tau$-density cluster} to be one such connected component $A \in C_f(\tau)$.  We will sometimes refer to $A[\bf{X}]$ as an \textit{empirical density cluster}.

\paragraph{Graph bicriteria.}
For $G = (V,E)$ an undirected, unweighted graph and $S, S' \subset V$ as before, let $\abs{E(S, S'; G)}$ denote the cut of $S$ and $S'$, given by
\begin{equation*}
\abs{E(S, S'; G)} = \sum_{v \in S} \sum_{u \in S'} 1((v,u) \in E).
\end{equation*}
Define the balance $B(S; G)$ to be
\begin{equation*}
B(S; G) = \min\{\vol(S;G), \vol(V \setminus S;G)\}.
\end{equation*}

We can now formally introduce our first criterion for assessing the quality of graph cuts: the \textbf{conductance}, $\Phi$, given by
\begin{equation}
\label{eqn: conductance}
\Phi(S; G) \defeq \frac{\abs{E(S, V \setminus S; G)}}{B(S; G)}.
\end{equation}
We typically seek a set $S^{\star} \subset V$ such that $\Phi(S^{\star}; G)$ is small. 

\paragraph{Neighborhood graph.}

Given $r \geq 0$, define the neighborhood graph to be $G_{n,r} = (\mathbf{X}, E_n)$, where for $x_i, x_j \in \mathbf{X}$, $(x_i, x_j) \in E_n$ if $\norm{x_i - x_j} \leq r$. (By convention, we do not allow loops, meaning $(x_i, x_i) \not\in E_n$). For ease of notation, for $S \subset \mathbf{X}$, let $\Phi_{n,r}(S) := \Phi(S; G_{n,r})$.

\subsection{Well-conditioned density clusters.}

In order to satisfy the \textcolor{red}{bicriteria}, we must introduce some assumptions on the density $f$. Let $B(x,r)$ be a closed ball with respect to Euclidean distance) of radius $r$ around the point $x$.  Given a set $A \subset \Rd$, and a number $\sigma > 0$, define the set $\Asig = A + B(0,\sigma) = \set{y \in \Rd: \inf_{x \in A} \norm{y - x} \leq \sigma}$. 
\begin{enumerate}[label=(A\arabic*)]
	\item 
	\label{asmp: cluster_regularity}
	\textit{Minimum density:} For numbers $\sigma, \tau_{\sigma} > 0$, a set $A \subset \mathcal{X}$ is said to be \textit{$(\sigma, \tau_{\sigma})$-regular} if
	\begin{equation}
	\inf_{x \in \Asig} f(x) = \tau_{\sigma}
	\end{equation} 
	%and 
	%\begin{equation*}
	%\frac{\diam \Asig}{\sigma} \leq \mu
	%\end{equation*}
	%where $\diam \Asig = \sup \set{d(x,y) : x,y \in \Asig}$
	
	\item 
	\label{asmp: low_noise_density}
	\textit{Low noise density:} For numbers $\gamma, \sigma > 0$, and a set $A \subset \mathcal{X}$, the density function $f$ is said to be \textit{$(\gamma, \sigma)$- low noise around $A$} if there exists a constant $C_1$ such that for all $x \in \mathcal{X}$ with $0 < \rho(x, \Asig) \leq \sigma$,
	\begin{equation*}
	\inf_{x' \in \Asig} f(x') - f(x) \geq C_1 \rho(x, \Asig)^{\gamma}.
	\end{equation*}
	where $\rho(x,\Asig) = \min_{x_0 \in \Asig} \norm{x - x_0}$.
	
	\item
	\label{asmp: cluster_separation}
	\textit{Cluster separation:}
	For $\tau,\sigma > 0$, $C_f(\tau)$ the set of connected components of the density upper-level set is said to be \textit{$\sigma$-well separated} if for all $\tau$-density clusters $A, A' \in C_f(\tau)$,
	\begin{equation*}
	\rho(A,A') > \sigma.
	\end{equation*}
	where $\rho(A,A') = \min_{x \in A, x' \in A'} \norm{x - x'}$.
\end{enumerate}

\textcolor{red}{Assumptions are standard, provide references.} We note that these assumptions are all local in nature, further motivating the study of a local algorithm. Assumptions \ref{asmp: cluster_regularity}-\ref{asmp: cluster_separation} are used to upper bound \textcolor{red}{bicriteria 1}, whereas \ref{asmp: cluster_regularity} and \textcolor{red}{(A4)} are necessary to lower bound \textcolor{red}{bicriteria 2}. 

\begin{definition}
	For $\tau > 0$ and $A \in C_f(\tau)$ a $\tau$-density cluster, we say that $A$ is a \textit{$(\lambda, \sigma, \gamma)$-well-conditioned cluster} if $A$ is $(\lambda,\sigma)$-regular, the density $f$ is $(\gamma, \sigma)$-low noise around $A^{\sigma}$, and $C_f(\tau)$ is $\sigma$-well separated.
\end{definition}
We note that $\sigma$ plays dual roles here, both in effect precluding arbitrarily narrow and long clusters in \ref{asmp: cluster_regularity} and arbitrarily flat densities in \ref{asmp: low_noise_density}. \textcolor{red}{We give some examples of densities which satisfy these assumptions in Section \ref{section: everything_else}.}

\section{Local Clustering on Density Level Sets}

In this section we show that a well-conditioned cluster $A$ satisfies the bicriteria \textcolor{red}{criteria 1} and \textcolor{red}{criteria 2}, and therefore the \textcolor{red}{PPR algorithm} outputs a low conductance set  which has small symmetric set difference with the empirical cluster. 

\begin{theorem}
	\label{thm: conductance_upper_bound}
	For some $\tau > 0$, let a $\tau$-density cluster $A \in C_f(\tau)$ satisfy Assumptions \ref{asmp: cluster_regularity}-\ref{asmp: cluster_separation} for some $\sigma, \tau_{\sigma}, \gamma > 0$. Then, for any $r < \sigma$ and $\delta > 0$, the following statements hold with probability at least $1 - \delta$: 
	\begin{itemize}
		\item 
		\textbf{Additive error bound.} Fix $\epsilon > 0$. Then, for
		\begin{equation}
		\label{eqn: conductance_sample_complexity}
		n \geq \log(2/\delta)\left(\frac{1 + \epsilon/2}{2 \tau_{\sigma}^2 \nu(\Asig) \nu_d (r/2)^d}\right)^2 
		\end{equation}
		we have
		\begin{equation}
		\label{eqn: conductance_additive_error_bound}
		\frac{\Phi_{n,r}(\Asig[\mathbf{X}])}{r} \leq C_{\sigma} \frac{\tau}{\tau_{\sigma}} \frac{(\tau_{\sigma} - \frac{r^{\gamma+1}}{\gamma+1})}{\tau_{\sigma}} + \epsilon
		\end{equation}
		\item
		\textcolor{red}{\textbf{Multiplicative error bound.}}
	\end{itemize}
	where $C_0 = 2^{2d+1}d$ and $C_{\sigma} = C_0 / \sigma$. 
\end{theorem}

A few remarks are in order. 
\begin{remark}
	\label{rem: exp_in_d}
	Note that the bound of (\ref{eqn: conductance_additive_error_bound}) depends exponentially on $d$; precisely, on $C_0 = d2^{2d +1}$. It is possible to improve this dependency to the order of $(1 + \frac{r}{\sigma})^{2d}$. However, in this setting, we think of $r$ as being a constant radius (rather than $r = r_n \to 0$ as $n \to \infty$, and therefore even this improvement results in exponential dependency on the dimension $d$. \textcolor{red}{Make references suggesting this type of exponential dependency is not uncommon in non-parametric clustering problems.}
\end{remark}
\begin{remark}
	Other than the looseness implied by Remark \ref{rem: exp_in_d}, the error bound of (\ref{eqn: conductance_additive_error_bound}) is almost tight. Specifically, choosing
	\begin{align*}
	\Asig & = B(0,\sigma), \\
	 f(x) & = 
	 \begin{cases}
	 \tau \text{ for $x \in \Asig$}, \\
	  f(x) = \tau - \rho(x,\Asig)^{\gamma} \text{ for $0 < \rho(x,\Asig) < r$}
	 \end{cases}
	\end{align*}
	we have
	\begin{equation}
	\frac{\Phi_{n,r}(\Asig[\mathbf{X}])}{r} \geq C_1 \frac{(\tau - \frac{r^{\epsilon+1}}{\epsilon+1})}{\tau}
	\end{equation}
	for some constant $C_1$, with probability at least $1 - \delta$. (Note that a factor of $1 / \sigma$ is not (\ref{eqn: conductance_additive_error_bound}) replicated in this lower bound.) \textcolor{red}{Provide justification in supplement, and cite it.}
\end{remark}

\section{Implications, extensions, and discussion}
\label{section: everything_else}
\subsection{Examples}
\subsection{Experiments}

\clearpage

\bibliography{icml_bib}
\bibliographystyle{icml2019}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
